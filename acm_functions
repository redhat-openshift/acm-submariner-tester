#!/bin/bash
# shellcheck disable=SC2153,SC2031,SC2016,SC2120,SC2005,SC1091

############ ACM and Submariner operator installation functions ############

# ------------------------------------------

function remove_multicluster_engine() {
### Removing Multi Cluster Engine from ACM hub and Managed cluster (if exists) ###
  trap_to_debug_commands;

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Removing Multi Cluster Engine from cluster: $cluster_name"

  TITLE "Deleting Discovery CRDs"

  ${OC} delete crd discoveredclusters.discovery.open-cluster-management.io --ignore-not-found || :
  ${OC} delete crd discoveryconfigs.discovery.open-cluster-management.io --ignore-not-found || :

  TITLE "Deleting '${MCE_NAMESPACE}' CSVs, Subscription and Namespace"

  ${OC} delete csv --all -n "${MCE_NAMESPACE}" --timeout=30s || :
  ${OC} delete subs --all -n "${MCE_NAMESPACE}" --timeout=30s || :
  force_delete_namespace "${MCE_NAMESPACE}"

  TITLE "Deleting all 'multicluster-engine' resources and Namespace"

  ${OC} get all -n multicluster-engine || ( echo -e "\n# MultiClusterEngine is not installed" && return )

  ${OC} delete MultiClusterEngine --all --timeout=30s || :

  force_delete_namespace "multicluster-engine"

  force_delete_namespace "${ACM_NAMESPACE}" 1m || :

}

# ------------------------------------------

function delete_acm_image_streams_and_tags() {
### Delete old image streams and tags of ACM and MCE from the HUB cluster
  trap_to_debug_commands;

  # Following steps should be run on ACM MultiClusterHub with $KUBECONF_HUB (NOT with the managed cluster kubeconfig)
  export KUBECONFIG="${KUBECONF_HUB}"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  TITLE "Delete ACM image streams and tags in cluster ${cluster_name}, namespace '${ACM_NAMESPACE}'"

  ${OC} delete imagestream --all -n "${ACM_NAMESPACE}" --wait || :
  ${OC} delete istag --all -n "${ACM_NAMESPACE}" --wait || :

  TITLE "Delete MCE image streams and tags in cluster ${cluster_name}, namespace '${MCE_NAMESPACE}'"

  ${OC} delete imagestream --all -n "${MCE_NAMESPACE}" --wait || :
  ${OC} delete istag --all -n "${MCE_NAMESPACE}" --wait || :

}

# ------------------------------------------

function remove_acm_managed_cluster_from_hub() {
### Removing Cluster-ID from ACM managed clusters on the Hub (if exists) ###
  trap_to_debug_commands;

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_id
  cluster_id="$(generate_acm_managed_cluster_id)"

  PROMPT "Removing the ACM Managed Cluster ID from the Hub: $cluster_id"

  # Following steps should be run on ACM MultiClusterHub with $KUBECONF_HUB (NOT on the managed cluster)
  export KUBECONFIG="${KUBECONF_HUB}"

  ${OC} get managedcluster -o wide || :

  if ${OC} get managedcluster "${cluster_id}" ; then

    ${OC} delete KlusterletAddonConfig "${cluster_id}" --timeout=30s --ignore-not-found || :

    ${OC} delete managedcluster "${cluster_id}" --timeout=30s || force_managedcluster_delete=TRUE

    if [[ "$force_managedcluster_delete" == TRUE && $(${OC} get managedcluster "${cluster_id}") ]]; then
      TITLE "Resetting finalizers of managed cluster '${cluster_id}' and force deleting its namespace"

      ${OC} patch managedcluster "${cluster_id}" --type json --patch='[ { "op": "remove", "path": "/metadata/finalizers" } ]' || :

      force_delete_namespace "${cluster_id}"

    fi

  else
    echo -e "\n# ACM Hub does not have a managed cluster '$cluster_id' (skipping removal)"
  fi

}

# ------------------------------------------

function remove_acm_resources_on_managed_cluster() {
### Removing ACM resources if already exists on the managed cluster (even if it's also same cluster as the Hub) ###
  trap_to_debug_commands;

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  local cluster_id
  cluster_id="$(generate_acm_managed_cluster_id)"

  PROMPT "Deleting '$cluster_id' resources from cluster: $cluster_name"

  delete_roles_by_name "open-cluster-management"

  delete_crds_by_name "open-cluster-management" || :
  delete_crds_by_name "multicluster" || :

  force_delete_namespace "open-cluster-management-agent"
  force_delete_namespace "open-cluster-management-agent-addon"
  force_delete_namespace "${cluster_id}"

}

# ------------------------------------------

function clean_acm_namespace_and_resources() {
### Uninstall ACM MultiClusterHub ###
# Ref: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/install/installing#uninstalling
  trap_to_debug_commands;

  # Should be run on the ACM MultiClusterHub cluster only
  export KUBECONFIG="${KUBECONF_HUB}"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Uninstalling previous ACM Hub instance from cluster ${cluster_name}"

  # Should be deprecated - The open-cluster-management github script is not officially supported:
  #
  # local acm_uninstaller_url="https://raw.githubusercontent.com/open-cluster-management/deploy/master/multiclusterhub/uninstall.sh"
  # local acm_uninstaller_file="./acm_cleanup.sh"
  #
  # TITLE "Uninstalling ACM with the uninstall.sh script from Github: \n
  # ${acm_uninstaller_url}"
  #
  # download_file "${acm_uninstaller_url}" "${acm_uninstaller_file}"
  # chmod +x "${acm_uninstaller_file}"
  # sed -i 's/kubectl /oc /' "${acm_uninstaller_file}" 
  #
  # # TODO: Use script from https://github.com/open-cluster-management/acm-qe/wiki/Cluster-Life-Cycle-Component
  # # https://raw.githubusercontent.com/open-cluster-management/deploy/master/hack/cleanup-managed-cluster.sh
  # # https://github.com/open-cluster-management/endpoint-operator/raw/master/hack/hub-detach.sh
  #
  # export TARGET_NAMESPACE="${ACM_NAMESPACE}"
  # ${acm_uninstaller_file} || :
  #
  # BUG "ACM uninstaller script does not delete all resources" \
  # "Delete ACM resources directly" \
  # "https://github.com/open-cluster-management/deploy/issues/218"
  # # Workaround:

  ${OC} project "$ACM_NAMESPACE"
  ${OC} delete managedcluster --all --timeout=3m || :

  ${OC} delete DiscoveryConfig --all --all-namespaces --timeout=3m || :
  ${OC} delete MultiClusterObservability --all --timeout=3m || :
  ${OC} delete MultiClusterHub --all --timeout=3m || :

  # Show ACM resources
  ${OC} get MultiClusterHub -o yaml || :
  ${OC} api-resources | grep -E "multicluster|open-cluster" || :

  # helm ls --namespace $ACM_NAMESPACE | cut -f 1 | tail -n +2 | xargs -n 1 helm delete --namespace $ACM_NAMESPACE
  ${OC} delete apiservice \
  v1beta2.webhook.certmanager.k8s.io \
  v1.admission.cluster.open-cluster-management.io \
  v1.admission.work.open-cluster-management.io --timeout=3m || :

  ${OC} delete clusterimageset --all --timeout=3m || :

  ${OC} delete clusterrole \
  multiclusterengines.multicluster.openshift.io-v1-admin \
  multiclusterengines.multicluster.openshift.io-v1-crdview \
  multiclusterengines.multicluster.openshift.io-v1-edit \
  multiclusterengines.multicluster.openshift.io-v1-view --timeout=3m || :

  ${OC} delete configmap -n $ACM_NAMESPACE \
  cert-manager-controller \
  cert-manager-cainjector-leader-election \
  cert-manager-cainjector-leader-election-core --timeout=3m || :

  ${OC} delete consolelink acm-console-link --timeout=3m || :

  ${OC} delete crd \
  klusterletaddonconfigs.agent.open-cluster-management.io \
  placementbindings.policy.open-cluster-management.io \
  policies.policy.open-cluster-management.io \
  userpreferences.console.open-cluster-management.io \
  searchservices.search.acm.com \
  discoveredclusters.discovery.open-cluster-management.io \
  discoveryconfigs.discovery.open-cluster-management.io --timeout=3m || :

  ${OC} delete mutatingwebhookconfiguration \
  cert-manager-webhook cert-manager-webhook-v1alpha1 ocm-mutating-webhook \
  managedclustermutators.admission.cluster.open-cluster-management.io \
  multicluster-observability-operator --timeout=3m || :

  ${OC} delete oauthclient multicloudingress --timeout=3m || :

  ${OC} delete rolebinding -n kube-system \
  cert-manager-webhook-webhook-authentication-reader --timeout=3m || :

  ${OC} delete scc kui-proxy-scc --timeout=3m || :

  ${OC} delete validatingwebhookconfiguration \
  cert-manager-webhook \
  cert-manager-webhook-v1alpha1 \
  channels.apps.open.cluster.management.webhook.validator \
  application-webhook-validator \
  multiclusterhub-operator-validating-webhook \
  ocm-validating-webhook \
  multicluster-observability-operator \
  multiclusterengines.multicluster.openshift.io --timeout=3m || :


  TITLE "Delete global CRDs, Managed Clusters, and Validation Webhooks of ACM in cluster ${cluster_name}"

  delete_crds_by_name "open-cluster-management" || :
  delete_crds_by_name "multicluster" || :

  ${OC} delete validatingwebhookconfiguration --all --timeout=30s || :
  ${OC} delete manifestwork --all --timeout=30s || :

  TITLE "Delete all MCE and ACM resources in cluster ${cluster_name}"

  ${OC} delete subs --all -n "${MCE_NAMESPACE}" --timeout=30s || :
  ${OC} delete subs --all -n "${ACM_NAMESPACE}" --timeout=30s || :
  ${OC} delete catalogsource --all -n "${ACM_NAMESPACE}" --timeout=30s || :
  ${OC} delete is --all -n "${ACM_NAMESPACE}" --timeout=30s || :
  ${OC} delete "${ACM_INSTANCE}" --all -n "${ACM_NAMESPACE}" --timeout=30s || :
  ${OC} delete csv --all -n "${MCE_NAMESPACE}" --timeout=30s || :
  ${OC} delete csv --all -n "${ACM_NAMESPACE}" --timeout=30s || :
  ${OC} delete cm --all -n "${ACM_NAMESPACE}" --timeout=30s || :
  ${OC} delete service --all -n "${ACM_NAMESPACE}" --timeout=30s || :

  force_delete_namespace "${MCE_NAMESPACE}" 1m || :

  # BUG "Uninstalling ACM did not remove open-cluster-management-hub resources" \
  # "Forcedly delete ${ACM_HUB_NS} and ${ACM_GS_NS} namespaces from the Hub" \
  # "https://issues.redhat.com/browse/ACM-2258"
  # # Workaround:
  # force_delete_namespace "${ACM_HUB_NS}" 1m || :
  # force_delete_namespace "${ACM_GS_NS}" 1m || :

  force_delete_namespace "${ACM_NAMESPACE}" 1m || :

}

# ------------------------------------------

function check_if_mce_is_required() {
  ### Return 1 if MCE is NOT required to be installed : Before ACM 2.5 or after ACM 2.6 ###

  TITLE "Check if MCE $MCE_VER_TAG is required to be installed before ACM $ACM_VER_TAG"

  export KUBECONFIG="${KUBECONF_HUB}"

  local acm_current_version
  acm_current_version="$(${OC} get MultiClusterHub -n "${ACM_NAMESPACE}" "${ACM_INSTANCE}" -o jsonpath='{.status.currentVersion}' 2>/dev/null)" || :

  if [[ -z "$acm_current_version" ]] ; then
    echo -e "\n# ACM is not installed"
  else
    echo -e "\n# ACM version $acm_current_version is currently installed"
  fi

  echo -e "\n# Check if ACM requested version '$ACM_VER_TAG' < 2.5, OR '$ACM_VER_TAG' > 2.6:"

  if check_version_greater_or_equal "$ACM_VER_TAG" "2.5" && \
  ! check_version_greater_or_equal "$ACM_VER_TAG" "2.6" ; then
    echo -e "\n# MCE $MCE_VER_TAG should be installed prior to ACM $ACM_VER_TAG"
  else
    echo -e "\n# MCE $MCE_VER_TAG should NOT be pre-installed before ACM $ACM_VER_TAG"
    return 1
  fi

}

# ------------------------------------------

function install_mce_operator_on_hub() {
  ### Install MCE operator - It should be run only on the Hub cluster ###
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"

  local mce_version="${2:-$MCE_VER_TAG}"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Install MCE $mce_version bundle on the Hub cluster ${cluster_name}"

  # Get MCE version from ACM downstream, if MCE version not specified
  if [[ -z "$mce_version" ]] ; then
    find_acm_component_version_in_downstream "${MCE_BUNDLE_IMAGE}" "${ACM_VER_TAG}" "${MCE_VERSION_FILE}" "${ACM_BUILD_DATE}"
    mce_version="$(< "${MCE_VERSION_FILE}")"
  fi

  local mce_current_version
  mce_current_version="$(${OC} get MultiClusterEngine -n "${MCE_NAMESPACE}" "${MCE_INSTANCE}" -o jsonpath='{.status.currentVersion}' 2>/dev/null)" || :

  # Install MCE if it's not installed, or if it's already installed, but with a different version than requested
  if [[ ! ${mce_current_version} =~ ^${mce_version} ]] ; then

    if [[ -z "$mce_current_version" ]] ; then
      echo -e "\n# MCE is not installed on current cluster ${cluster_name} - Installing MCE $mce_version from scratch"
    else
      echo -e "\n# MCE $mce_current_version currently installed on cluster ${cluster_name} is different then requested - Re-installing MCE $mce_version"
    fi

    local mce_channel
    mce_channel="$(generate_channel_name "${MCE_CHANNEL_PREFIX}" "$mce_version")"

    local mce_catalog
    mce_catalog="$(generate_catalog_name "${MCE_OPERATOR}" "${mce_channel}")"

    TITLE "Install MCE bundle $mce_version in namespace '${MCE_NAMESPACE}' on the Hub ${cluster_name}
    Catalog: ${mce_catalog}
    Channel: ${mce_channel}
    "

    # Deploy MCE operator as an OCP bundle
    mce_version="v${mce_version}" # e.g. v2.2.0-231

    deploy_ocp_bundle "${MCE_BUNDLE}" "${mce_version}" "${MCE_OPERATOR}" "${MCE_NAMESPACE}" "${mce_catalog}" "${mce_channel}"
    
    echo -e "\n# MCE $mce_version installation completed"

  else
    TITLE "Requested MCE version $mce_version is already installed on current cluster ${cluster_name} - Skipping MCE installation"
  fi

}

# ------------------------------------------

function install_acm_operator_on_hub() {
  ### Install ACM operator - It should be run only on the Hub cluster ###
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"

  local acm_version="${1:-$ACM_VER_TAG}"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Install ACM $acm_version bundle on the Hub cluster ${cluster_name}"

  local acm_current_version
  acm_current_version="$(${OC} get MultiClusterHub -n "${ACM_NAMESPACE}" "${ACM_INSTANCE}" -o jsonpath='{.status.currentVersion}' 2>/dev/null)" || :

  # Install ACM if it's not installed, or if it's already installed, but with a different version than requested.
  # For example, will skip install if current ACM version (e.g. 2.7.1) starts with requested version regex (e.g. ^2.7)  
  if [[ ! ${acm_current_version} =~ ^${acm_version} ]] ; then

    find_acm_component_version_in_downstream "${ACM_BUNDLE_IMAGE}" "${acm_version}" "${ACM_VERSION_FILE}" "${ACM_BUILD_DATE}"
    acm_version="$(< "${ACM_VERSION_FILE}")"

    if [[ -z "$acm_current_version" ]] ; then
      echo -e "\n# ACM is not installed on current cluster ${cluster_name} - Installing ACM $acm_version from scratch"
    else
      echo -e "\n# ACM $acm_current_version currently installed on cluster ${cluster_name} is different then requested - Re-installing ACM $acm_version"
    fi

    # Deploy ACM operator as an OCP bundle
    acm_version="v${acm_version}" # e.g. v2.7.0-220 

    local acm_channel
    acm_channel="$(generate_channel_name "${ACM_CHANNEL_PREFIX}" "$acm_version")"

    local acm_catalog
    acm_catalog="$(generate_catalog_name "${ACM_OPERATOR}" "${acm_channel}")"

    TITLE "Install ACM bundle $acm_version in namespace '${ACM_NAMESPACE}' on the Hub ${cluster_name}
    Catalog: ${acm_catalog}
    Channel: ${acm_channel}
    "

    deploy_ocp_bundle "${ACM_BUNDLE}" "${acm_version}" "${ACM_OPERATOR}" "${ACM_NAMESPACE}" "${acm_catalog}" "${acm_channel}"

    echo -e "\n# ACM $acm_version installation completed"

  else
    TITLE "Requested ACM version $acm_version is already installed on current cluster ${cluster_name} - Skipping ACM installation"
  fi

}

# ------------------------------------------

function find_acm_component_version_in_downstream() {
  ### Get full image number for an ACM image found in https://github.com/stolostron/deploy ###
  trap_to_debug_commands;

  # Input args:
  local acm_component="$1" # e.g. mce-operator-bundle
  local acm_version="$2" # e.g. 2.7.0
  local component_version_file="$3" # Where to save the component version. e.g. mce-operator-bundle.ver

  # Optional args:
  local stolostron_commit_date="$4" # downstream date to filter by (otherwise using the latest date found)

  local stolostron_commit_title
  local stolostron_commit_hash
  local stolostron_downstream_file

  # ACM Stolstron recent commits: https://github.com/stolostron/deploy/commits/master/mirror"
  local acm_builds_info_url="https://github.com/stolostron/deploy/search?o=desc&q=%22${acm_version}%22+%22downstream+mirror%22&s=committer-date&type=commits"

  TITLE "Find build number of '${acm_component}' in the latest commit of ACM version '$acm_version'
  ACM builds information: $acm_builds_info_url
  ACM build date (optional): $stolostron_commit_date
  "
  
  # Filter all href links which includes the required version (escaping dots) and commit date. For example:
  # Added 2.7.0 downstream mirror mapping for 2023-01-26-20-15-10 : /stolostron/deploy/commit/2eb2756a18614865c50651f57e2ffeb1663e2f4c

  curl -L "${acm_builds_info_url}" | grep -Eo "href=\".*>${acm_version//./\\.}.*<.*${stolostron_commit_date}.*" | \
  sed -E 's#</?em>##g' | awk -F '"|=|>|<' '{print $7 $5 " : " $3}' | sort -k2,7 > "${component_version_file}"

  cat "${component_version_file}"

  # Get latest ACM commit title
  stolostron_commit_title="$(tail -1 "${component_version_file}")" || :

  # Get full ACM commit date, e.g. 2023-01-26-20-15-10
  stolostron_commit_date="$(echo "$stolostron_commit_title" | awk '{print $7}')" || :

  # Get ACM commit hash, e.g. /stolostron/deploy/commit/2eb2756a18614865c50651f57e2ffeb1663e2f4c
  stolostron_commit_hash="$(echo "$stolostron_commit_title" | awk -F '/' '{print $5}')" || :

  # Get and export ACM version (global variable)
  ACM_VER_TAG="$(echo "$stolostron_commit_title" | awk '{print $2}')"
  export ACM_VER_TAG

  stolostron_downstream_file="mirror/${ACM_VER_TAG}-DOWNSTREAM-${stolostron_commit_date}.txt" 

  TITLE "Download downstream file from https://github.com/stolostron, of the latest commit for ACM version '$ACM_VER_TAG':
  Stolostron commit title: \n${stolostron_commit_title}
  Stolostron commit hash: \n${stolostron_commit_hash}
  Stolostron commit date: \n${stolostron_commit_date}
  Stolostron downstream file: \n${stolostron_downstream_file}
  "

  download_github_file_or_dir "stolostron" "deploy" "${stolostron_commit_hash}" "${stolostron_downstream_file}"

  TITLE "Find and save the full version number for ACM component '$acm_component'"

  grep -Po "${acm_component}:v\K.*" "${stolostron_downstream_file}" > "$component_version_file"

  echo -e "\n# $acm_component version stored in ${component_version_file}: $(< "${component_version_file}")"

}

# ------------------------------------------

function create_mce_subscription() {
  ### Create MCE subscription - It should be run only on the Hub cluster ###
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"

  local mce_version="${1:-$MCE_VER_TAG}" # e.g. v2.2.0

  # Get MCE version from ACM downstream, if MCE version not specified
  if [[ -z "$mce_version" ]] ; then
    find_acm_component_version_in_downstream "${MCE_BUNDLE_IMAGE}" "${ACM_VER_TAG}" "${MCE_VERSION_FILE}" "${ACM_BUILD_DATE}"
    mce_version="$(< "${MCE_VERSION_FILE}")"
  fi
  
  mce_version="v${mce_version}" # e.g. v2.2.0

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Create new Subscription for ${MCE_OPERATOR} in cluster ${cluster_name}"

  local mce_channel
  mce_channel="$(generate_channel_name "${MCE_CHANNEL_PREFIX}" "$mce_version")"

  local mce_catalog
  mce_catalog="$(generate_catalog_name "${MCE_OPERATOR}" "${mce_channel}")"

  # Skip creating Automatic Subscription (channel without a specific version) for MCE operator
  # create_subscription "${MCE_OPERATOR}" "${mce_channel}" "${mce_catalog}" "" "${MCE_NAMESPACE}"

  # Create Manual Subscription with a specific version for MCE operator
  create_subscription "${MCE_OPERATOR}" "${mce_channel}" "${mce_catalog}" "${mce_version}" "${MCE_NAMESPACE}"

  echo -e "\n# MCE Subscription for ${MCE_OPERATOR} is ready"

}

# ------------------------------------------

function create_acm_subscription() {
  ### Create ACM subscription - It should be run only on the Hub cluster ###
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"

  local acm_version="v${1:-$ACM_VER_TAG}" # e.g. v2.7.0

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  local acm_channel
  acm_channel="$(generate_channel_name "${ACM_CHANNEL_PREFIX}" "$acm_version")" || :

  local acm_catalog
  acm_catalog="$(generate_catalog_name "${ACM_OPERATOR}" "${acm_channel}")" || :

  PROMPT "Create Subscription for ${ACM_OPERATOR} in ${ACM_NAMESPACE} (catalog ${acm_catalog}) on cluster ${cluster_name}"

  local acm_current_version
  acm_current_version="$(${OC} get MultiClusterHub -n "${ACM_NAMESPACE}" "${ACM_INSTANCE}" -o jsonpath='{.status.currentVersion}' 2>/dev/null)" || :

  # Create Subscription (with Automatic or Manual install plan)
  if [[ "$acm_version" != "$acm_current_version" ]] ; then

    if [[ -z "$acm_current_version" ]] ; then
      TITLE "ACM is not installed on current cluster ${cluster_name}" # Skip Creating Automatic Subscription for ACM $acm_version"
      # # Create Automatic Subscription (channel without a specific version) for ACM operator
      # create_subscription "${ACM_OPERATOR}" "${acm_channel}" "${acm_catalog}" "" "${ACM_NAMESPACE}"

    else
      TITLE "ACM $acm_current_version is already installed on current cluster ${cluster_name}" # Changing to Manual Subscription for ACM $acm_version"
      # # Create Manual Subscription with a specific version for ACM operator
      # create_subscription "${ACM_OPERATOR}" "${acm_channel}" "${acm_catalog}" "${acm_version}" "${ACM_NAMESPACE}"
    fi

    # Create Manual Subscription with a specific version for ACM operator
    create_subscription "${ACM_OPERATOR}" "${acm_channel}" "${acm_catalog}" "${acm_version}" "${ACM_NAMESPACE}"

    echo -e "\n# ACM Subscription for ${ACM_OPERATOR} is ready"

  else
    TITLE "Requested ACM version $acm_version is already installed on current cluster ${cluster_name} - Skipping ACM Subscription"
  fi

}

# ------------------------------------------

function create_multicluster_engine() {
  ### Create MCE instance ###
  trap_to_debug_commands;

  local mce_version="${1:-$MCE_VER_TAG}" # e.g. 2.2.0

  export KUBECONFIG="${KUBECONF_HUB}"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Create Multi-Cluster Engine on cluster ${cluster_name}"

  echo -e "\n# Verify that the MultiClusterEngine CRD exists in cluster ${cluster_name}"

  cmd="${OC} get crd multiclusterengines.multicluster.openshift.io"
  watch_and_retry "$cmd" 3m || : # FATAL "MultiClusterEngine CRD does not exist in cluster ${cluster_name}"

  TITLE "Create the MultiClusterEngine in namespace ${MCE_NAMESPACE} on cluster ${cluster_name}"

  cat <<EOF | ${OC} apply -f -
  apiVersion: multicluster.openshift.io/v1
  kind: MultiClusterEngine
  metadata:
    name: ${MCE_INSTANCE}
    namespace: ${MCE_NAMESPACE}
  spec: {}
EOF

  TITLE "Wait for MCE instance '${MCE_INSTANCE}' to be available"
  local duration=15m
  cmd="${OC} get MultiClusterEngine ${MCE_INSTANCE} -n ${MCE_NAMESPACE}"
  watch_and_retry "$cmd" "$duration" "Available" || mce_status=FAILED

  TITLE "All MultiClusterEngine resources status:"
  ${OC} get MultiClusterEngine -A -o json | jq -r '.items[].status' || :

  [[ -n "$mce_version" ]] || mce_version="$(< "${MCE_VERSION_FILE}")"
  mce_version="${mce_version%%-*}" # Remove version build number after "-"

  TITLE "Verify requested MCE version '${mce_version}' appears in MultiClusterEngine" 

  cmd="${OC} get MultiClusterEngine -A -o json | jq -r '.items[].status.currentVersion'"
  watch_and_retry "$cmd" "$duration" "$mce_version" || acm_status=FAILED

  if [[ "$mce_status" == FAILED ]] ; then
    TITLE "Check Operator deployment logs of '${MCE_OPERATOR}' in cluster ${cluster_name}"

    ${OC} logs -n "${MCE_NAMESPACE}" deploy/multicluster-engine-operator \
    --all-containers --tail=100 --timestamps \
    |& (! highlight '^E0|"error"|level=error|Error') || :

    FATAL "MultiClusterEngine version '${mce_version}' is not ready after $duration"
  fi

  local mce_current_version
  mce_current_version="$(${OC} get MultiClusterEngine -A -o json | jq -r '.items[].status.currentVersion')" || :

  echo -e "\n# MultiClusterEngine version [${acm_current_version}] is installed" 

}

# ------------------------------------------

function create_acm_multiclusterhub() {
  ### Create ACM MultiClusterHub instance ###
  trap_to_debug_commands;

  local acm_version="${1:-$ACM_VER_TAG}" # e.g. 2.7.0

  export KUBECONFIG="${KUBECONF_HUB}"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Create ACM MultiClusterHub instance on cluster ${cluster_name}"

  echo -e "\n# Verify that the MultiClusterHub CRD exists in cluster ${cluster_name}"

  cmd="${OC} get crd multiclusterhubs.operator.open-cluster-management.io"
  watch_and_retry "$cmd" 5m || FATAL "MultiClusterHub CRD does not exist in cluster ${cluster_name}"

  echo -e "\n# Verify that all '${ACM_NAMESPACE}' operator pods are running, before creating MultiClusterHub"
  ${OC} wait --timeout=3m --for=condition=ready pod --all -n "${ACM_NAMESPACE}" --field-selector=status.phase!=Succeeded

  TITLE "Create the MultiClusterHub in namespace ${ACM_NAMESPACE} on cluster ${cluster_name}"

  local existing_mce_catalog
  existing_mce_catalog="$(${OC} get catalogsource -n "${MCE_NAMESPACE}" -o name | grep "${MCE_OPERATOR}" | awk -F '/' 'NR==1 {print $2}')" || :
  
  cat <<EOF | ${OC} apply -f - || acm_status=FAILED
  apiVersion: operator.open-cluster-management.io/v1
  kind: MultiClusterHub
  metadata:
    name: ${ACM_INSTANCE}
    namespace: ${ACM_NAMESPACE}
    annotations: 
      installer.open-cluster-management.io/mce-subscription-spec: '{"source": "${existing_mce_catalog}"}'
  spec:
    disableHubSelfManagement: true
    imagePullSecret: multiclusterhub-operator-pull-secret
EOF

  echo -e "\n# Verify that all '${ACM_NAMESPACE}' operator pods are running, before creating MultiClusterHub"
  ${OC} wait --timeout=3m --for=condition=ready pod --all -n "${ACM_NAMESPACE}" --field-selector=status.phase!=Succeeded
  
  BUG "${ACM_OPERATOR} deployment error: Found more than one catalogSource with expected channel" \
  "Annotate MultiClusterHub to point to MCE catalog-source" \
  "https://issues.redhat.com/browse/ACM-2347"
  # Workaround:
  local mce_annotation="{\"source\": \"${existing_mce_catalog}\", \"sourceNamespace\": \"${MCE_NAMESPACE}\"}"
  ${OC} annotate MultiClusterHub "${ACM_INSTANCE}" -n "${ACM_NAMESPACE}" \
  installer.open-cluster-management.io/mce-subscription-spec="${mce_annotation}" || :


  TITLE "Wait for ACM Instance '${ACM_INSTANCE}' to be running"
  local duration=15m
  # cmd="${OC} get MultiClusterHub ${ACM_INSTANCE} -o jsonpath='{.status.phase}'"
  cmd="${OC} get MultiClusterHub ${ACM_INSTANCE} -n ${ACM_NAMESPACE}"
  watch_and_retry "$cmd" "$duration" "Running" || acm_status=FAILED

  # Since ACM 2.7 ACM console route is not available, check for console plugin instead
  if check_version_greater_or_equal "$ACM_VER_TAG" "2.7" ; then
    TITLE "Verify ACM console plugin (for ACM >= 2.7)"

    ${OC} wait --timeout=$duration consoles.operator.openshift.io cluster \
    --for=condition=DeploymentAvailable || acm_status=FAILED

    ${OC} get consoleplugin || :

  else
    TITLE "Verify ACM console url (for ACM < 2.7)"
    
    cmd="${OC} get routes -n ${ACM_NAMESPACE} multicloud-console --no-headers -o custom-columns='URL:spec.host'"
    watch_and_retry "$cmd" "$duration" || acm_status=FAILED
  fi

  TITLE "All MultiClusterHub resources status:"

  ${OC} get MultiClusterHub -A -o json | jq -r '.items[].status' || :

  [[ -n "$acm_version" ]] || acm_version="$(< "${ACM_VERSION_FILE}")"
  acm_version="${acm_version%%-*}" # Remove version build number after "-"

  TITLE "Verify requested ACM version '${acm_version}' appears in MultiClusterHub" 

  cmd="${OC} get MultiClusterHub -n '${ACM_NAMESPACE}' '${ACM_INSTANCE}' -o jsonpath='{.status.currentVersion}'"
  watch_and_retry "$cmd" "$duration" "$acm_version" || acm_status=FAILED

  if [[ "$acm_status" == FAILED ]] ; then
    TITLE "Check Operator deployment logs of '${ACM_OPERATOR}' in cluster ${cluster_name}"

    ${OC} logs -n "${ACM_NAMESPACE}" deploy/multiclusterhub-operator \
    --all-containers --tail=100 --timestamps \
    |& (! highlight '^E0|"error"|level=error|Error') || :

    FATAL "ACM MultiClusterHub is not ready after $duration"
  fi

  local acm_current_version
  acm_current_version="$(${OC} get MultiClusterHub -n "${ACM_NAMESPACE}" "${ACM_INSTANCE}" -o jsonpath='{.status.currentVersion}')" || :

  echo -e "\n# ACM version [${acm_current_version}] hub installed" 
  # console url: $(${OC} get routes -n "${ACM_NAMESPACE}" multicloud-console --no-headers -o custom-columns='URL:spec.host')"

}

# ------------------------------------------

function create_clusterset_for_submariner_in_acm_hub() {
  ### Create ACM cluster-set ###
  PROMPT "Create cluster-set '${ACM_CLUSTER_SET}' on ACM hub"
  trap_to_debug_commands;

  local acm_resource
  acm_resource="$(mktemp)_acm_resource"
  local duration=5m

  # Run on ACM MultiClusterHub cluster
  export KUBECONFIG="${KUBECONF_HUB}"

  cmd="${OC} api-resources | grep ManagedClusterSet"
  watch_and_retry "$cmd" "$duration" || acm_status=FAILED

  if [[ "$acm_status" == FAILED ]] ; then
    ${OC} api-resources
    FATAL "ManagedClusterSet resource type is missing"
  fi 

  TITLE "Creating ACM 'ManagedClusterSet' object: ${ACM_CLUSTER_SET}"

  create_namespace "${ACM_CLUSTER_SET}"

  create_namespace "${SUBM_NAMESPACE}"

  # Create the cluster-set
  cat <<EOF | ${OC} apply -f -
  apiVersion: cluster.open-cluster-management.io/v1beta1
  kind: ManagedClusterSet
  metadata:
    name: ${ACM_CLUSTER_SET}
EOF

  local cmd="${OC} describe ManagedClusterSet &> '$acm_resource'"
  # local regex="Reason:\s*ClustersSelected" # Only later it includes "ManagedClusterSet"
  local regex="Name:\s*${ACM_CLUSTER_SET}"

  watch_and_retry "$cmd ; grep -E '$regex' $acm_resource" "$duration" || :
  highlight "$regex" "$acm_resource" || acm_status=FAILED

  if [[ "$acm_status" == FAILED ]] ; then
    FATAL "'${ACM_CLUSTER_SET}' was not added to ManagedClusterSet after $duration"
  fi

  TITLE "Creating 'ManagedClusterSetBinding' for the ClusterSet '${ACM_CLUSTER_SET}' in namespace '${SUBM_NAMESPACE}'"

  # Bind the managed cluster set to a namespace
  # The ManagedClusterSetBinding must have the same name as the target ManagedClusterSet

  # API in ACM 2.7 for ManagedClusterSetBinding was upgraded from v1beta1 to v1beta2:
  local api_version="cluster.open-cluster-management.io/v1beta1"
  if check_version_greater_or_equal "$ACM_VER_TAG" "2.7" ; then
    api_version="cluster.open-cluster-management.io/v1beta2"
  fi

  cat <<EOF | ${OC} apply -f -
  apiVersion: ${api_version}
  kind: ManagedClusterSetBinding
  metadata:
    name: ${ACM_CLUSTER_SET}
    namespace: ${SUBM_NAMESPACE}
  spec:
    clusterSet: ${ACM_CLUSTER_SET}
EOF

  local cmd="${OC} describe ManagedClusterSetBinding -n ${SUBM_NAMESPACE} &> '$acm_resource'"
  local regex="Cluster Set:\s*${ACM_CLUSTER_SET}"

  watch_and_retry "$cmd ; grep -E '$regex' $acm_resource" "$duration" || :
  highlight "$regex" "$acm_resource" || acm_status=FAILED

  if [[ "$acm_status" == FAILED ]] ; then
    FATAL "ManagedClusterSetBinding '${ACM_CLUSTER_SET}' was not created in ${SUBM_NAMESPACE} after $duration"
  fi

}

# ------------------------------------------

function create_and_import_managed_cluster() {
  trap_to_debug_commands;

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_id
  cluster_id="$(generate_acm_managed_cluster_id)"

  PROMPT "Create and import a managed cluster in ACM: $cluster_id"

  local ocp_cloud
  ocp_cloud="$(print_current_cluster_cloud)"

  create_new_managed_cluster_in_acm_hub "$cluster_id" "$ocp_cloud"

  export KUBECONFIG="$kubeconfig_file"

  import_managed_cluster "$cluster_id"
}


# ------------------------------------------

function generate_acm_managed_cluster_id() {
  trap '' DEBUG # DONT trap_to_debug_commands

  export KUBECONFIG="$KUBECONFIG"

  local cluster_name
  local cluster_id
  
  # local ocp_cloud
  # ocp_cloud="$(print_current_cluster_cloud)"
  # if [[ "$ocp_cloud" != "Amazon" ]] ; then
  #   cluster_name="$(print_current_cluster_name)"
  #   cluster_id="acm-${cluster_name}"
  # else
  #   # Workaround for https://issues.redhat.com/browse/ACM-2380
  #   cluster_id="local-cluster"
  # fi

  cluster_name="$(print_current_cluster_name)"
  cluster_id="acm-${cluster_name}"
  echo "$cluster_id"

}

# ------------------------------------------

function create_new_managed_cluster_in_acm_hub() {
  ### Create ACM managed cluster by cluster ID ###
  # References: 
  # https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html-single/clusters/index#adding-clusters-to-a-managedclusterset-cli
  # https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html-single/clusters/index#modifying-the-klusterlet-add-ons-settings-of-your-cluster
  # https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html-single/add-ons/index#deploying-submariner-mcaddon-api

  trap_to_debug_commands;

  local cluster_id="${1}"
  local cluster_type="${2}"

  # Run on ACM MultiClusterHub cluster (Manager)
  export KUBECONFIG="${KUBECONF_HUB}"

  TITLE "Create the namespace for the managed cluster: ${cluster_id}"

  create_namespace "${cluster_id}"

  ${OC} label namespace "${cluster_id}" cluster.open-cluster-management.io/managedCluster="${cluster_id}" --overwrite

  TITLE "Add new ManagedCluster of type '$cluster_type' to '${ACM_CLUSTER_SET}' ClusterSet: $cluster_id"

  cat <<EOF | ${OC} apply -f -
  apiVersion: cluster.open-cluster-management.io/v1
  kind: ManagedCluster
  metadata:
    name: ${cluster_id}
    labels:
      cloud: ${cluster_type}
      name: ${cluster_id}
      vendor: OpenShift
      cluster.open-cluster-management.io/clusterset: ${ACM_CLUSTER_SET}
  spec:
    hubAcceptsClient: true
    leaseDurationSeconds: 60
EOF


  TITLE "Create ACM ${ACM_VER_TAG} Klusterlet Addon Config for managed cluster '${cluster_id}', in the ClusterSet '${ACM_CLUSTER_SET}'"

  local Klusterlet_conf="Klusterlet_${cluster_id}.yaml"

  ### Create the klusterlet Addon config
  cat <<-EOF > "$Klusterlet_conf"
  apiVersion: agent.open-cluster-management.io/v1
  kind: KlusterletAddonConfig
  metadata:
    name: ${cluster_id}
    namespace: ${cluster_id}
    labels:
      cluster.open-cluster-management.io/${SUBM_AGENT}: "true"
  spec:
    clusterName: ${cluster_id}
    clusterNamespace: ${cluster_id}
    clusterLabels:
      cloud: auto-detect
      cluster.open-cluster-management.io/clusterset: ${ACM_CLUSTER_SET}
      name: ${cluster_id}
      vendor: auto-detect
    applicationManager:
      enabled: true
    certPolicyController:
      enabled: true
    iamPolicyController:
      enabled: true
    policyController:
      enabled: true
    searchCollector:
      enabled: true
    version: "${ACM_VER_TAG}"
EOF

  echo -e "\n# Apply Klusterlet (and wait until created)"

  # ${OC} apply --dry-run='server' -f "$Klusterlet_conf" | highlight "unchanged" \
  # || ${OC} apply -f "$Klusterlet_conf" --wait || ${OC} apply -f "$Klusterlet_conf" || :

  local duration=5m
  local klusterlet_output
  klusterlet_output="$(mktemp)_klusterlet_output"

  local cmd="${OC} apply -f '$Klusterlet_conf' --wait &> '$klusterlet_output'"
  local regex="${cluster_id} (created|unchanged)"

  watch_and_retry "$cmd ; grep -E '$regex' $klusterlet_output" "$duration" || :

  TITLE "Wait for KlusterletAddonConfig '${cluster_id}' to be created in the namespace '${cluster_id}'"

  # Wait for the new ManagedCluster $cluster_id
  ${OC} wait --timeout=$duration managedcluster "${cluster_id}" -n "${cluster_id}" --for=condition=HubAcceptedManagedCluster || :

  ${OC} describe KlusterletAddonConfig -n "${cluster_id}" "${cluster_id}"

  TITLE "Wait for ManagedCluster Opaque secret '${cluster_id}-import' to be created"

  # Wait for the Opaque secret
  local cmd="${OC} get secrets -n ${cluster_id}"
  local regex="${cluster_id}-import"

  watch_and_retry "$cmd" "$duration" "$regex" || \
  FATAL "Opaque secret '${cluster_id}-import' was not created after $duration"

  local kluster_crd="./${cluster_id}-klusterlet-crd.yaml"
  local kluster_import="./${cluster_id}-import.yaml"

  TITLE "Save the yamls to be applied on the managed clusters: '${kluster_crd}' and '${kluster_import}'"

  local json_data

  json_data="$(${OC} get secret "${cluster_id}"-import -n "${cluster_id}" -o jsonpath="{.data.crds\\.yaml}")"
  echo "$json_data" | base64 --decode > "${kluster_crd}"

  json_data="$(${OC} get secret "${cluster_id}"-import -n "${cluster_id}" -o jsonpath="{.data.import\\.yaml}")"
  echo "$json_data" | base64 --decode > "${kluster_import}"

}

# ------------------------------------------

### Function to import the clusters to the clusterSet
function import_managed_cluster() {
  trap_to_debug_commands;

  local cluster_id="${1}"

  local kluster_crd="./${cluster_id}-klusterlet-crd.yaml"
  local kluster_import="./${cluster_id}-import.yaml"

  ocp_login "${OCP_USR}" "$(< "${WORKDIR}/${OCP_USR}.sec")"

  TITLE "Install klusterlet (Addon) on the managed clusters"
  # Import the managed clusters
  ${OC} apply -f "${kluster_crd}"

  ${OC} apply -f "${kluster_import}"

  TITLE "Verify klusterlets for the managed cluster id: ${cluster_id}"

  local ocm_deployments
  ocm_deployments="$(oc get deployments -n open-cluster-management-agent -o name)"

  for deploy_name in $ocm_deployments ; do
    ${OC} rollout status -n open-cluster-management-agent --watch --timeout 1m $deploy_name || klusterlet_status=FAILED
  done

  ${OC} get all -n open-cluster-management-agent

  if [[ "$klusterlet_status" == FAILED ]] ; then
    FAILURE "Klusterlet installation failure occurred for managed cluster id: $cluster_id"
  fi

}

# ------------------------------------------

function install_submariner_operator_on_cluster() {
  trap_to_debug_commands;

  local kubeconfig_file="$1"
  local submariner_version="${2:-$SUBM_VER_TAG}"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Install Submariner bundle $submariner_version (without Subscription) on cluster $cluster_name"

  # # Fix the $submariner_version value for custom images (the function is defined in main setup_subm.sh)
  # set_subm_version_tag_var "submariner_version"

  echo -e "\n# Since Submariner 0.12 the channel has changed from 'alpha' to 'stable'"
  local subscription_channel
  if check_version_greater_or_equal "$SUBM_VER_TAG" "0.12" ; then
    subscription_channel="$(generate_channel_name "${SUBM_CHANNEL_PREFIX}" "$submariner_version")"
  else
    subscription_channel="$(generate_channel_name "${SUBM_CHANNEL_PREFIX_TECH_PREVIEW}" "$submariner_version")"
  fi

  local submariner_catalog
  submariner_catalog="$(generate_catalog_name "${SUBM_OPERATOR}" "${subscription_channel}")"

  ocp_login "${OCP_USR}" "$(< "${WORKDIR}/${OCP_USR}.sec")"

  # Deploy Submariner operator as an OCP bundle
  deploy_ocp_bundle "${SUBM_BUNDLE}" "${submariner_version}" "${SUBM_OPERATOR}" "${SUBM_NAMESPACE}" "${submariner_catalog}" "${subscription_channel}"
  # Note: No need to create Subscription for Submariner bundle, as it is done later within: create_submariner_config_in_acm_managed_cluster()

}

# ------------------------------------------

function configure_submariner_addon_for_acm_managed_cluster() {
  # TODO: This funtion should be split and executed as several junit tests

  trap_to_debug_commands;

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  echo -e "\n# Generate new Managed cluster ID according to the current cluster name of kubeconfig: $kubeconfig_file"
  local cluster_id

  BUG "Submariner Gateway on OVN: unable to parse node L3 gw annotation" \
  "Rename the managed cluster id on the Hub to 'local-cluster" \
  "https://issues.redhat.com/browse/ACM-2380"
  # Workaround in:
  cluster_id="$(generate_acm_managed_cluster_id)"

  echo -e "\n# Get OCP Infra ID for the new Managed cluster" # Required for Azure Resource group identification
  local ocp_infra_id
  ocp_infra_id="$(${OC} get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster)"

  PROMPT "Configure Submariner $SUBM_VER_TAG Addon for ACM managed cluster: $cluster_id"

  TITLE "Apply the 'scc' policy for Submariner Gateway, Router-agent, Globalnet and Lighthouse on cluster $cluster_name"
  ${OC} adm policy add-scc-to-user privileged "system:serviceaccount:${SUBM_NAMESPACE}:${SUBM_GATEWAY}"
  ${OC} adm policy add-scc-to-user privileged "system:serviceaccount:${SUBM_NAMESPACE}:${SUBM_ROUTE_AGENT}"
  ${OC} adm policy add-scc-to-user privileged "system:serviceaccount:${SUBM_NAMESPACE}:${SUBM_GLOBALNET}"
  ${OC} adm policy add-scc-to-user privileged "system:serviceaccount:${SUBM_NAMESPACE}:${SUBM_LH_COREDNS}"

  # TODO: Wait for acm agent installation on the managed clusters
  local cmd="${OC} get clusterrolebindings --no-headers -o custom-columns='USER:subjects[].*' | grep '${SUBM_LH_COREDNS}'"
  watch_and_retry "$cmd" 5m || BUG "WARNING: Submariner users may not be scc privileged"

  ${OC} get clusterrolebindings

  # Following steps should be run on ACM MultiClusterHub to configure Submariner addon with $KUBECONF_HUB (NOT with the managed cluster kubeconfig)
  export KUBECONFIG="${KUBECONF_HUB}"

  ocp_login "${OCP_USR}" "$(< "${WORKDIR}/${OCP_USR}.sec")"

  ${OC} get managedcluster -o wide

  local managed_cluster_cloud
  managed_cluster_cloud=$(${OC} get managedcluster -o jsonpath="{.items[?(@.metadata.name=='${cluster_id}')].metadata.labels.cloud}")

  if [[ "$managed_cluster_cloud" == "Unknown" ]] ; then
    BUG "Azure Managed cluster '${cluster_id}' Cloud label is Unknown" \
    "Retrieve Platform name from clusterClaims instead" \
    "https://issues.redhat.com/browse/ACM-1918"
    
    managed_cluster_cloud="$(${OC} get managedcluster "${cluster_id}" -o json \
    | jq -r '.status.clusterClaims[] | select(.name == "platform.open-cluster-management.io").value')"
  fi

  TITLE "Configure ${cluster_id} credentials for the Submariner Gateway on cloud: $managed_cluster_cloud"

  local cluster_secret_name

  if [[ "$managed_cluster_cloud" == "Amazon" ]] ; then
    cluster_secret_name="${cluster_id}-aws-creds"
    configure_submariner_addon_for_amazon "$cluster_id" "$cluster_secret_name"
  elif [[ "$managed_cluster_cloud" == "Google" ]] ; then
    cluster_secret_name="${cluster_id}-gcp-creds"
    configure_submariner_addon_for_google "$cluster_id" "$cluster_secret_name"
  elif [[ "$managed_cluster_cloud" == "Azure" ]] ; then
    cluster_secret_name="${cluster_id}-azure-creds"
    configure_submariner_addon_for_azure "$cluster_id" "$cluster_secret_name" "$ocp_infra_id"
  elif [[ "$managed_cluster_cloud" == "Openstack" ]] ; then
    cluster_secret_name="${cluster_id}-osp-creds"
    configure_submariner_addon_for_openstack "$cluster_id" "$cluster_secret_name"
  else
    FATAL "Could not determine Cloud type '$managed_cluster_cloud' for Managed cluster '${cluster_id}'"
  fi

  # After creating the cloud credentials for the managed cluster - use it in the SubmarinerConfig
  create_submariner_config_in_acm_managed_cluster "$cluster_id" "$cluster_secret_name" "$SUBM_VER_TAG"

  # Validate manifestwork
  validate_submariner_manifestwork_in_acm_managed_cluster "$cluster_id" || sumariner_addon_status=FAILED

  # Validate managedclusteraddons
  validate_submariner_addon_status_in_acm_managed_cluster "$cluster_id" || sumariner_addon_status=FAILED

  # Validate submarinerconfig
  validate_submariner_config_in_acm_managed_cluster "$cluster_id" || sumariner_addon_status=FAILED

  # Validate submariner connection
  validate_submariner_agent_connected_in_acm_managed_cluster "$cluster_id" || sumariner_addon_status=FAILED

  if [[ "$sumariner_addon_status" == FAILED ]] ; then
    FAILURE "Submariner Addon had installation failures on managed cluster '$cluster_id'"
  fi

}

# ------------------------------------------

function configure_submariner_addon_for_amazon() {
  ### Configure submariner addon credentials for AWS cloud ###

  trap_to_debug_commands;

  local cluster_id="${1}"
  local cluster_secret_name="${2}"

  TITLE "Using '${cluster_secret_name}' for Submariner on Amazon"

  ( # subshell to hide commands
    { [[ -n "$AWS_KEY" ]] && [[ -n "$AWS_SECRET" ]] ; } \
    || FATAL "AWS credentials are required to configure Submariner in the managed cluster '${cluster_id}'"

    cat <<EOF | ${OC} apply -f -
    apiVersion: v1
    kind: Secret
    metadata:
      name: ${cluster_secret_name}
      namespace: ${cluster_id}
    type: Opaque
    data:
      aws_access_key_id: $(echo -n "${AWS_KEY}" | base64 -w0)
      aws_secret_access_key: $(echo -n "${AWS_SECRET}" | base64 -w0)
EOF
  )

}

# ------------------------------------------

function configure_submariner_addon_for_google() {
  ### Configure submariner addon credentials for GCP cloud ###

  trap_to_debug_commands;

  local cluster_id="${1}"
  local cluster_secret_name="${2}"

  TITLE "Using '${cluster_secret_name}' for Submariner on Google"

  ( # subshell to hide commands
    [[ -s "$GCP_CRED_JSON" ]] || FATAL "GCP credentials file (json) is required to configure Submariner in the managed cluster '${cluster_id}'"

    cat <<EOF | ${OC} apply -f -
    apiVersion: v1
    kind: Secret
    metadata:
        name: ${cluster_secret_name}
        namespace: ${cluster_id}
    type: Opaque
    data:
        osServiceAccount.json: $(base64 -w0 "${GCP_CRED_JSON}")
EOF
  )

}

# ------------------------------------------

function configure_submariner_addon_for_azure() {
  ### Configure submariner addon credentials for Azure cloud ###

  trap_to_debug_commands;

  local cluster_id="${1}"
  local cluster_secret_name="${2}"

  local ocp_infra_id="${3}"
  local azure_resource_group_name="${ocp_infra_id}-rg"

  TITLE "Azure Resource group is required, and it should be named as OCP infra ID of Azure Cluster + '-rg'"

  az group show -n "$azure_resource_group_name"

  TITLE "Using '${cluster_secret_name}' for Submariner on Azure"

  ( # subshell to hide commands
    [[ -s "$AZURE_CRED_JSON" ]] || FATAL "Azure credentials file (json) is required to configure Submariner in the managed cluster '${cluster_id}'"

    cat <<EOF | ${OC} apply -f -
    apiVersion: v1
    kind: Secret
    metadata:
      name: ${cluster_secret_name}
      namespace: ${cluster_id}
    type: Opaque
    data:
      baseDomainResourceGroupName: $(echo -n "${azure_resource_group_name}" | base64 -w0)
      osServicePrincipal.json: $(base64 -w0 "${AZURE_CRED_JSON}")
EOF
  )

}

# ------------------------------------------

function configure_submariner_addon_for_openstack() {
  ### Configure submariner addon credentials for OSP cloud ###

  trap_to_debug_commands;

  local cluster_id="${1}"
  local cluster_secret_name="${2}"

  TITLE "Using '${cluster_secret_name}' for Submariner on Openstack"

  # Since ACM 2.5 Openstack cloud prepare is supported
  if check_version_greater_or_equal "$ACM_VER_TAG" "2.5" ; then

    ( # subshell to hide commands

      if [[ -s "$OSP_CRED_YAML" ]] ; then
        echo -e "\n# Using \$OSP_CRED_YAML file"

        cat <<EOF | ${OC} apply -f -
            apiVersion: v1
            kind: Secret
            metadata:
                name: ${cluster_secret_name}
                namespace: ${cluster_id}
            type: Opaque
            data:
                clouds.yaml: $(base64 -w0 "${OSP_CRED_YAML}")
                cloud: $(echo -n "openstack" | base64 -w0)
EOF
      elif [[ -n "$OS_PROJECT_ID" ]] ; then
        echo -e "\n# Using direct credentials"

        cat <<EOF | ${OC} apply -f -
        apiVersion: v1
        kind: Secret
        metadata:
            name: ${cluster_secret_name}
            namespace: ${cluster_id}
        type: Opaque
        data:
            clouds.yaml: $(echo -n "
            clouds:
              openstack:
                auth:
                  auth_url: ${OS_AUTH_URL}
                  username: '${OS_USERNAME}'
                  project_id: ${OS_PROJECT_ID}
                  project_name: '${OS_PROJECT_NAME}'
                  user_domain_name: '${OS_USER_DOMAIN_NAME}'
                region_name: '${OS_REGION_NAME}'
                interface: 'public'
                identity_api_version: 3
            " | base64 -w0)
            cloud: $(echo -n "openstack" | base64 -w0)
EOF
      else
        FATAL "OSP credentials are required to configure Submariner in the managed cluster '${cluster_id}'"
      fi
    )

  else
    BUG "Openstack Gateway creation is not yet supported with Submariner Addon" \
    "The Gateway should be configured externally with 'configure_osp.sh'"
  fi

}

# ------------------------------------------


function create_submariner_config_in_acm_managed_cluster() {
  ### Create the SubmarinerConfig on the managed cluster_id with specified submariner version ###

  trap_to_debug_commands;

  local cluster_id="${1}"
  local cluster_secret_name="${2}"
  local submariner_version="${3:-$SUBM_VER_TAG}"

  # # Fix the $submariner_version value for custom images (the function is defined in main setup_subm.sh)
  # set_subm_version_tag_var "submariner_version"

  echo -e "\n# Since Submariner 0.12 the channel has changed from 'alpha' to 'stable'"
  local subscription_channel
  if check_version_greater_or_equal "$SUBM_VER_TAG" "0.12" ; then
    subscription_channel="$(generate_channel_name "${SUBM_CHANNEL_PREFIX}" "$submariner_version")"
  else
    subscription_channel="$(generate_channel_name "${SUBM_CHANNEL_PREFIX_TECH_PREVIEW}" "$submariner_version")"
  fi

  local submariner_catalog
  submariner_catalog="$(generate_catalog_name "${SUBM_OPERATOR}" "${subscription_channel}")"

  local starting_csv
  starting_csv="$(get_csv_for_operator "$SUBM_OPERATOR" "$submariner_version" "$SUBM_NAMESPACE")"

  TITLE "Create the SubmarinerConfig for ACM managed cluster '${cluster_id}'
  Submariner version: ${submariner_version}
  Subscription channel ${subscription_channel}
  Subscription version ${starting_csv}
  "

  local submariner_conf="SubmarinerConfig_${cluster_id}.yaml"

  cat <<-EOF > "$submariner_conf"
  apiVersion: submarineraddon.open-cluster-management.io/v1alpha1
  kind: SubmarinerConfig
  metadata:
    name: ${SUBM_CONFIG}
    namespace: ${cluster_id}
  spec:
    IPSecIKEPort: ${IPSEC_IKE_PORT}
    IPSecNATTPort: ${IPSEC_NATT_PORT}
    cableDriver: ${SUBM_CABLE_DRIVER}
    credentialsSecret:
      name: ${cluster_secret_name}
    gatewayConfig:
      aws:
        instanceType: c5d.large
      gateways: 1
    imagePullSpecs:
      lighthouseAgentImagePullSpec: ''
      lighthouseCoreDNSImagePullSpec: ''
      submarinerImagePullSpec: ''
      submarinerRouteAgentImagePullSpec: ''
    subscriptionConfig:
      channel: ${subscription_channel}
      source: ${submariner_catalog}
      sourceNamespace: ${SUBM_NAMESPACE}
      startingCSV: ${starting_csv}
EOF

  echo -e "\n# Apply SubmarinerConfig (if failed once - apply again)"

  ${OC} apply --dry-run='server' -f "$submariner_conf" | highlight "unchanged" \
  || ${OC} apply -f "$submariner_conf" || ${OC} apply -f "$submariner_conf"

  TITLE "Display the SubmarinerConfig CRD"

  ${OC} describe crd submarinerconfigs.submarineraddon.open-cluster-management.io || :

  TITLE "Create the Submariner Addon to start the deployment"

  cat <<EOF | ${OC} apply -f -
  apiVersion: addon.open-cluster-management.io/v1alpha1
  kind: ManagedClusterAddOn
  metadata:
    name: ${SUBM_ADDON}
    namespace: ${cluster_id}
  spec:
    installNamespace: ${SUBM_NAMESPACE}
EOF

  ${OC} describe managedclusteraddons -n "${cluster_id}" "${SUBM_ADDON}"

  TITLE "Label the managed clusters and klusterletaddonconfigs to deploy submariner"

  # ${OC} label managedclusters.cluster.open-cluster-management.io ${cluster_id} "cluster.open-cluster-management.io/${SUBM_AGENT}=true" --overwrite
  ${OC} label managedcluster "${cluster_id}" "cluster.open-cluster-management.io/clusterset=${ACM_CLUSTER_SET}" --overwrite

}

# ------------------------------------------

function validate_submariner_manifestwork_in_acm_managed_cluster() {
  ### Validate that Submariner manifestwork created in ACM, for the managed cluster_id ###

  trap_to_debug_commands;

  local cluster_id="${1}"
  local regex
  local cmd

  # Check Submariner manifests
  regex="submariner"
  TITLE "Wait for ManifestWork of '${regex}' to be ready in the ACM MultiClusterHub under namespace ${cluster_id}"
  cmd="${OC} get manifestwork -n ${cluster_id} --ignore-not-found"
  watch_and_retry "$cmd | grep -E '$regex'" "5m" || :
  $cmd |& highlight "$regex" || FATAL "Submariner Manifestworks were not created in ACM MultiClusterHub for the cluster id: $cluster_id"


  # Check Klusterlet manifests

  # In ACM 2.5 the addon manifest was renamed to "addon-application-manager-deploy"
  if check_version_greater_or_equal "$ACM_VER_TAG" "2.5" ; then
    regex="addon-application-manager-deploy"
  else
    regex="klusterlet-addon-appmgr"
  fi

  TITLE "Wait for ManifestWork of '${regex}' to be ready in the ACM MultiClusterHub under namespace ${cluster_id}"
  cmd="${OC} get manifestwork -n ${cluster_id} --ignore-not-found"
  watch_and_retry "$cmd | grep -E '$regex'" "15m" || :
  $cmd |& highlight "$regex" || FAILURE "Klusterlet Manifestworks were not created in ACM MultiClusterHub for the cluster id: $cluster_id"

}

# ------------------------------------------

function validate_submariner_addon_status_in_acm_managed_cluster() {
  ### Validate that Submariner Addon has gateway labels and running agent for the managed cluster_id ###

  trap_to_debug_commands;

  local cluster_id="${1}"

  local managed_cluster_status

  TITLE "Verify ManagedClusterAddons '${SUBM_ADDON}' in ACM MultiClusterHub under namespace ${cluster_id}"

  ${OC} get managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}" || :

  ### Test ManagedClusterAddons ###
  # All checks should print: managedclusteraddon.addon.open-cluster-management.io/submariner condition met
  # "Degraded" conditions should be false

  ${OC} wait --timeout=20m managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}" --for=condition=RegistrationApplied \
  && ${OC} wait --timeout=20m managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}" --for=condition=ManifestApplied \
  && ${OC} wait --timeout=20m managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}" --for=condition=SubmarinerGatewayNodesLabeled \
  && ${OC} wait --timeout=20m managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}" --for=condition=Available \
  || managed_cluster_status=FAILED

  BUG "Submariner Addon condition shows wrong status" \
  "Skip verifying managedclusteraddons '${SUBM_ADDON}' -n '${cluster_id}' --for=condition=SubmarinerAgentDegraded=false" \
  "TODO: Need to report bug"
  # Workaround - Remove this step:
  # ${OC} wait --timeout=20m managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}" --for=condition=SubmarinerAgentDegraded=false


  ${OC} describe managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}" || managed_cluster_status=FAILED

  if [[ "$managed_cluster_status" == FAILED ]] ; then
    ${OC} logs deploy/submariner-addon -n "${SUBM_NAMESPACE}" \
    --all-containers --tail=100 |& (! highlight '^E0|"error"|level=error|Error') || :

    ${OC} describe submarinerconfig "${SUBM_CONFIG}" -n "${cluster_id}" || :

    FATAL "Submariner ManagedClusterAddon has unhealthy conditions in ACM cluster id: $cluster_id"
  fi

}

# ------------------------------------------

function validate_submariner_config_in_acm_managed_cluster() {
  ### Validate that SubmarinerConfig is ready in ACM, for the cluster_id ###

  trap_to_debug_commands;

  local cluster_id="${1}"

  local config_status

  TITLE "Verify SubmarinerConfig '${SUBM_CONFIG}' in the ACM MultiClusterHub under namespace ${cluster_id}"

  ${OC} get submarinerconfig "${SUBM_CONFIG}" -n "${cluster_id}" || :

  ### Test SubmarinerConfig ###
  # All checks should print: submarinerconfig.submarineraddon.open-cluster-management.io/submariner condition met

  ${OC} wait --timeout=5m submarinerconfig "${SUBM_CONFIG}" -n "${cluster_id}" --for=condition=SubmarinerClusterEnvironmentPrepared \
  && ${OC} wait --timeout=5m submarinerconfig "${SUBM_CONFIG}" -n "${cluster_id}" --for=condition=SubmarinerGatewaysLabeled \
  || config_status=FAILED

  BUG "Submariner Config condition shows wrong status" \
  "Skip verifying submarinerconfig '${SUBM_CONFIG}' -n '${cluster_id}' --for=condition=SubmarinerConfigApplied" \
  "TODO: Need to report bug"
  # Workaround - Remove this step:
  # ${OC} wait --timeout=5m submarinerconfig "${SUBM_CONFIG}" -n "${cluster_id}" --for=condition=SubmarinerConfigApplied

  ${OC} describe submarinerconfig "${SUBM_CONFIG}" -n "${cluster_id}" || config_status=FAILED

  if [[ "$config_status" == FAILED ]] ; then
    FATAL "SubmarinerConfig '${SUBM_CONFIG}' resource has unhealthy conditions in ACM cluster id: $cluster_id"
  fi

}

# ------------------------------------------

function validate_submariner_agent_connected_in_acm_managed_cluster() {
  ### Validate that Submariner Addon has gateway labels and running agent for the managed cluster_id ###

  trap_to_debug_commands;

  local cluster_id="${1}"

  local managed_cluster_status

  TITLE "Verify Submariner connection established in the '${ACM_CLUSTER_SET}' cluster-set of ACM namespace ${cluster_id}"

  ${OC} get managedcluster -o wide

  # The ManagedCluster-Broker connection is a management-plane connection only (via the Kube API), and not a data-plane connection.
  # "SubmarinerConnectionDegraded" condition is relevant only when more than one ManagedClusters are configured.
  # i.e. To establish actual inter-cluster data-plane connections between ManagedClusters (which are part of the same ManagedClusterSet).

  local clusterset_count
  clusterset_count="$(${OC} get managedclusteraddons -A -o jsonpath="{.items[?(@.metadata.name=='${SUBM_ADDON}')].metadata.name}" | wc -w)"

  if (( clusterset_count > 1 )) ; then
    ${OC} wait --timeout=15m managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}" --for=condition=SubmarinerConnectionDegraded=false || managed_cluster_status=FAILED

    if [[ "$managed_cluster_status" == FAILED ]] ; then
      ${OC} describe managedclusteraddons "${SUBM_ADDON}" -n "${cluster_id}"
      FAILURE "Submariner connection could not be established in ACM cluster id: $cluster_id"
    fi

  else
    echo -e "\n# Ignoring 'SubmarinerConnectionDegraded' condition, as it requires at least 2 ManagedClusterSet with Submariner Addon '${SUBM_ADDON}'"
  fi

}
