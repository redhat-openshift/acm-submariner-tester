#!/bin/bash

### General Helpers Function ###

# Constants #

export RED='\x1b[0;31m'
export GREEN='\x1b[38;5;22m'
export CYAN='\x1b[36m'
export YELLOW='\x1b[33m'
export NO_COLOR='\x1b[0m'
export HAT="${RED}ðŸŽ©ï¸Ž${NO_COLOR}"

export CONDA_ENV_NAME="conda-test"

# Set SCRIPT_DIR as current absolute path where this script runs in
export SCRIPT_DIR="$(dirname "$(realpath -s $0)")"

# include sh2ju library
source "$SCRIPT_DIR/lib/sh2ju.sh"

shopt -s nocasematch # Case-insensitive match for string evaluations

set -o allexport # To export each function when it is declared, to be used on external bash processes

### Functions ###

# ------------------------------------------

# Function to search and highlight rexexp in file (or pipe content)
function highlight()
{
  #trap_to_debug_commands
  pattern="$1"
  file_input="$2"

  exit_code=0
  # 0 = Match found
  # 1 = SED execution error
  # 3 = Match not found
  # 4 = Empty string or file

  local tmp_file="`mktemp`_highlight"
  # Save all output to temp file
  #grep -E "$pattern|$" "$@" > $tmp_file
  grep -Ei "$pattern|$" ${file_input:+"$file_input"} > $tmp_file

  # Exit if file is empty or contains spaces only
  grep -q '[^[:space:]]' $tmp_file || return 4

  # Print with match colored
  # cat $tmp_file | grep --color -E "$pattern|$"

  # Search line for sed command. Highlighting pattern, if found
  search_line='/'"$pattern"'/I,${s//'$YELLOW'&'$NO_COLOR'/g;b};$q3'

  # SED with -r : Basic regex
  sed -r "$search_line" $tmp_file || exit_code=$?

  # If SED had an error, try to search with -e : Extended regex"
  if [[ "$exit_code" -eq 1 ]] ; then
    exit_code=0
    sed -e "$search_line" $tmp_file || exit_code=$?
  fi

  # Return if match found
  # cat $tmp_file | grep -q -E "$pattern"
  return $exit_code
}

# ------------------------------------------

function print_current_dir_and_cmd() {
  # Get command to print, but skip printing if it's empty
  local cmd="$(echo -e "$1" | xargs)"
  [[ -n "$cmd" ]] || return

  # Print PS1 prompt (and trim trailing and leading spaces)
  local new_location="[$0] $PS1 [${PWD##*/}]" && \
  if [[ "$new_location" != "$current_location" ]]; then
    export current_location="$new_location"
    echo -e "\n${YELLOW}$current_location${NO_COLOR}\n"
  fi

  # Save all command variables' values inside $all_vars_output
  local all_vars_output="\n"

  # List simple variables ($v or ${v}) first
  local var_list=( $(echo "$cmd" | grep -ohP "\\\$\w+|\\\$\{\w+\}" | sort -u || :) )

  # List complex variables (with parameter expansion) next
  var_list+=( $(echo "$cmd" | grep -ohP "\\\$\{\w+[:/][^\}]+\}" | sort -u || :) )

  for variable in "${var_list[@]}" ; do
     local var_name=$(echo ${variable%%[:/]*} | tr -d "\${}")
     local var_value="${!var_name}"
     if [[ "$variable" =~ [:/] ]] ; then
       # For complex variables - add it to $all_vars_output
       all_vars_output+="# ${var_name}=${var_value} \n"
     else
       # For simple variables - replace them with their values within $cmd
       cmd="${cmd//$variable/$var_value}"
     fi
   done

  # Print $all_vars_output (after # sign), and $cmd (after $ sign)
  echo -e "${CYAN}${all_vars_output}\$ ${cmd}${NO_COLOR}"
}

# ------------------------------------------

# Function to print each bash command before it is executed (using with "set -T" might have issues)
function trap_to_debug_commands() {
  trap '
   if [[ ! "$BASH_COMMAND" =~ ^(echo|read|PROMPT|BUG|FATAL) ]] ; then
      print_current_dir_and_cmd "$BASH_COMMAND" 2>/dev/null || :
   fi
  ' DEBUG
}

# export -f trap_to_debug_commands
# trap_to_debug_commands

# ------------------------------------------

# Function to trap test exit failure - to print more info before exiting main flow
function trap_function_on_error() {
  trap '' DEBUG # DONT trap_to_debug_commands

  local func_on_script_error="$@"

  # If other traps already set - make sure to execute it after $func_on_script_error
  current_traps="$(trap -p EXIT | tr '\n' ' :; ' | cut -f2 -d \')"

  trap_cmd='rc=$?; echo "# $FUNCNAME[$@] returned EXIT $rc:
  ${BASH_SOURCE[$i+1]}:$BASH_LINENO > ${BASH_SOURCE[$i]}:${BASH_LINENO[$i-1]}" ;
  [[ "$rc" = "0" ]] || ('${func_on_script_error}') || : ;
  [[ -z "'${current_traps}'" ]] || ('${current_traps}') || : ;'

  echo -e "\n# Trapping on script EXIT: \n${trap_cmd} \n"
  trap '(exit 130)' INT # When SIGINT is received call $trap_cmd
  trap '(exit 143)' TERM # When SIGTERM is received call $trap_cmd
  trap "$trap_cmd" EXIT

}

# ------------------------------------------

# function to watch for expected regex of command output, with retry attempts or timeout
function watch_and_retry() {
  # Do not trap_to_debug_commands

  # Param 1: Command to execute and watch
  CMD="$1"
  # Param 2 (Optional): Number of retry attempts (default is 10)
  RETRY="${2:-10}" # If adding s/m to the number, it will be timeout in seconds/minutes
  # Param 3 (Optional): Pattern to search, otherwise expect $CMD exit 0
  REGEX="$3"

  # Max timeout for the whole watch process (default is 10 minutes)
  TIMEOUT=10m

  # echo -e "# Watching output of command: $(eval echo \"$CMD\") \
  # echo -e "# Watching output of command: $CMD"

  # Set timeout for the whole process, if RETRY is number of seconds/minutes (e.g. 20s or 3m)
  if [[ "$RETRY" =~ [0-9]+[sm] ]] ; then
    TIMEOUT=$RETRY
    RETRY=-1
  fi

  echo -e "# Watching output of command #
  \n# Retry attempts: $RETRY
  \n# Timeout limit: $TIMEOUT"

  # If REGEX was specified - will wait for REGEX to be found in CMD output
  if [[ -n "$REGEX" ]] ; then
    echo "# Search pattern: $REGEX"
    CMD="( $CMD ) | highlight '$REGEX'"
  fi

  # To use timeout - must run command as an external script
  timeout $TIMEOUT bash <<EOF \
  || ( [[ $? -eq 124 ]] && echo "# Timeout of $TIMEOUT has exceeded." && return 1 )
    COUNT=0 ;
    until $CMD || [[ \$COUNT -eq $RETRY ]] ; do
      printf "\$(( COUNT++ ))... " ;
      sleep 1 # A second interval ;
    done
    if [[ \$COUNT -eq $RETRY ]]; then
      echo "# Limit of $RETRY attempts has exceeded." ;
      exit 1 ;
    fi ;
    exit 0 ;
EOF
}

# ------------------------------------------

# function to run command with timeout, and tail log in parallel
function run_and_tail() {
  trap_to_debug_commands

  # Param 1: Command to execute
  local cmd="$1"
  # Param 2: Log file to tail
  local log_to_tail="$2"
  # Param 3: timeout duration (default 1 hour)
  local duration="${3:-1h}"
  # Param 4: RegEx to find in log (if not given, return process exit code)
  local regex="$4"

  echo -e "# Running command as background process: $cmd
  \n# Timeout duration: $duration
  \n# Tailing log file: $log_to_tail"

  # Running the process with timeout - in the background, and store its PID
  timeout --foreground $duration $cmd &
  local pid=$!
  local process_rc=0

  # Tail the log, until command has ended or timed-out
  mkdir -p "$(dirname "$log_to_tail")"
  touch "$log_to_tail"
  tail --pid=$pid -n0 -F "$log_to_tail"

  # Check the exit code of the command PID
  wait $pid || process_rc="$?"

  if [[ "$process_rc" = 124 ]] ; then
    echo -e "\n# Timeout of $duration has exceeded while running [$cmd]"
  elif [[ ! "$process_rc" = 0 ]] ; then
    echo -e "\n# Command [$cmd] has failed (Error $process_rc)"
  fi

  if [[ -n "$regex" ]] ; then
    # When searching for regex in log, it will ignore cmd exit code
    process_rc=0
    grep "$regex" "$log_to_tail" || process_rc="$?"
    [[ "$process_rc" = 0 ]] || \
    echo -e "\n# The expected search pattern '$regex' was not written to the log file [$log_to_tail]"
  fi

  return $process_rc
}

# ------------------------------------------

# Function to print current time and a message
function PROMPT() {
  message="$1"
  color="${2:-${YELLOW}}"
  cur_time="$HAT[$(date '+%Y-%m-%d %H:%M:%S' || :)]"
  echo -e "\n${cur_time}\n"\
  "${color}### $message ###${NO_COLOR}\n"\
  "[${PWD}]\n"
}

# ------------------------------------------

# Function to print FAILURE message and return 5 for the current test
# sh2ju treats exit code 5 as non-critical failure (to continue tests)
function FAILURE() {
  local failure_msg="${1}"
  echo -e "\n${RED}*** Test failure occurred *** \n $failure_msg ${NO_COLOR}" >&2
  return 5
}

# ------------------------------------------

# Function to print FATAL message and exit the whole test suite
function FATAL() {
  local failure_msg="${1}"
  echo -e "\n${RED}*** Fatal error occurred *** \n $failure_msg ${NO_COLOR}" >&2
  # kill -s TERM $$
  # kill -s TERM $BASHPID
  # kill -s EXIT $PPID
  return 127
}

# ------------------------------------------

# Function to print message about a potential bug/workaround, and an optional bug link
function BUG() {
  failed_action="${1}"
  workaround="${2}" # optional
  bug_ref="${3}" # optional

  echo -e "${RED}****** BUG ******\n Failure:${NO_COLOR} $failed_action"
  [[ -z "$workaround" ]] || echo -e "${RED} Workaround:${NO_COLOR} $workaround"
  [[ -z "$bug_ref" ]] || echo -e "${RED} Reference:${NO_COLOR} $bug_ref"
}


# ------------------------------------------

# Function to run the command and records its output/error messages in junit format
# Ref: https://github.com/kubernetes/kubernetes/blob/master/test/cmd/legacy-script.sh
function record_junit() {
  # it expects the first to be the name of the command
  # Example:
  # record_junit function_name arg1 arg2
  #
  # WARNING: Variable changes in the command will NOT be effective after record_junit returns.
  #          This is because the command runs in subshell.

  # $1 => $output_file and $output_dir for junit results
  local output_file=$(basename "$1")
  local output_dir=$(dirname "$1")

  # Set junit_suite as the base directory name of $output_dir
  local junit_suite="$(basename "$output_dir")"

  # Shift arguments to the left ($1 gets lost), to save rest of args as array $@
  shift

  # Set junit test name as the called function
  local test_name="$1"
  local func_and_args="$@"

  echo "+++ Recording junit test \"$test_name\" ${func_and_args:+[$func_and_args]}
  +++ into xml output file \"${output_dir}/${output_file}\""

  juLog -output="$output_dir" -file="$output_file" -index -class="$junit_suite" -name="$test_name" "$func_and_args"
}

# ------------------------------------------

# Function to installing Anaconda (https://github.com/conda-forge)
function install_anaconda() {
  trap_to_debug_commands
  work_dir="$(echo "${1:-.}" | tr -s /)" # Remove redundant slashes
  install_dir="${work_dir}/miniconda"
  miniconda_bin_dir="${install_dir}/bin"
  [[ ":$PATH:" != *"/miniconda/bin:"* ]] && export PATH=${miniconda_bin_dir}:$PATH

  if conda info ; then
    # Clean previous conda packages
    conda clean -y --all
    # rm -rf "${install_dir}" || :
  fi

  echo "# Installing Anaconda"
  # wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh
  # miniconda_installer="Miniconda3-py38_4.9.2-Linux-x86_64.sh"
  miniconda_installer="Miniconda3-latest-Linux-x86_64.sh"
  download_file "https://repo.anaconda.com/miniconda/${miniconda_installer}" "${work_dir}/${miniconda_installer}"
  chmod +x "${work_dir}/${miniconda_installer}"
  ${work_dir}/${miniconda_installer} -b -u -p ${install_dir}
  conda update -y -n base -c defaults conda

  # Remove old conda environment dir, if it is not configured
  env_dir=$(conda env list | awk -v name="$CONDA_ENV_NAME" '$0 ~ name { print $2 }')
  env_base=$(conda env list | awk '/base/ { print $NF }')

  if [[ -z "$env_dir" ]] ; then
    echo "# Removing old Conda environment \"$CONDA_ENV_NAME\" directory (since it is not configured):"
    conda env remove -y -n $CONDA_ENV_NAME || :
    conda env remove -y -p "$env_base/envs/$CONDA_ENV_NAME" || :
    rm -rf "$env_base/envs/$CONDA_ENV_NAME" || :
  fi

  echo "# Creating Conda environment \"$CONDA_ENV_NAME\" (if it does not exist already)"
  [[ -d "$env_base/envs/$CONDA_ENV_NAME" ]] || conda create -y -n $CONDA_ENV_NAME
}
# ------------------------------------------

# Function to verify Anaconda installation
function verify_anaconda() {
  trap_to_debug_commands
  install_dir="$(echo "${1:-.}/miniconda" | tr -s /)" # Remove redundant slashes
  miniconda_bin_dir="${install_dir}/bin"
  [[ ":$PATH:" != *"/miniconda/bin:"* ]] && export PATH=${miniconda_bin_dir}:$PATH
  conda info
  conda env list | grep "${install_dir}/envs/${CONDA_ENV_NAME}"
}

# ------------------------------------------

# Function to install local tool/package with Anaconda (https://github.com/conda-forge/{package name})
function install_conda_package() {
  trap_to_debug_commands

  local pkg_name="$1"
  local install_dir="${2:-.}/"
  local pkg_version="$3"

  echo "# Installing a local tool [${pkg_name}${pkg_version:+=$pkg_version}] with Anaconda, in \"$(whoami)\" home directory."

  verify_anaconda "${install_dir}"

  # conda config --add channels defaults
  conda config --add channels conda-forge
  source activate $CONDA_ENV_NAME
  conda install -y --update-deps -c conda-forge ${pkg_name}${pkg_version:+=$pkg_version}
  conda list
}

# ------------------------------------------

# Function to install local Golang with Anaconda (https://github.com/conda-forge/go-nocgo_linux-64)
function install_local_golang() {
  trap_to_debug_commands

  local install_dir="${1:-.}/"
  local pkg_version="$2"

  install_conda_package "go-nocgo_linux-64" "${install_dir}" "${pkg_version}"
}

# ------------------------------------------

# Function to verify that Golang is installed
function verify_golang() {
  echo "# Verifying Golang installation, and setting GOROOT, GOPATH, GOBIN, and GO111MODULE=on"
  go version

  # export GOROOT=/usr/local/go
  export GOROOT=$(go env GOROOT)

  # export GOPATH=~/go
  export GOPATH=$(go env GOPATH)

  # Optional: exporting $1 as GOBIN path
  # export GOBIN="${1:-$GOROOT/bin}"

  # export PATH=$PATH:$GOROOT/bin
  [[ ":$PATH:" != *":$GOBIN:"* ]] && export PATH=$GOBIN:$PATH

  export GO111MODULE=on

  # Print all GO env variables
  go env
}

# ------------------------------------------

# Function to install local Terraform with Anaconda (https://anaconda.org/conda-forge/terraform)
function install_local_terraform() {
  trap_to_debug_commands

  local install_dir="${1:-.}/"
  local pkg_version="$2"

  install_conda_package "terraform" "${install_dir}" "${pkg_version}"
}

# ------------------------------------------

# Docker with Anaconda is not supported in Linux yet - use Podman instead
# Function to install local Docker with Anaconda (https://anaconda.org/conda-forge/docker)
function install_local_docker() {
  trap_to_debug_commands
  # curl -o /etc/yum.repos.d/vbatts-shadow-utils-newxidmap-epel-7.repo \
  # https://copr.fedorainfracloud.org/coprs/vbatts/shadow-utils-newxidmap/repo/epel-7/vbatts-shadow-utils-newxidmap-epel-7.repo
  # yum install -y shadow-utils46-newxidmap
  # SKIP_IPTABLES=1 curl -fsSL https://get.docker.com/rootless | sh -s -- --experimental
  # # curl -fsSL https://get.docker.com/rootless | sh -s -- --iptables=false

  local install_dir="${1:-.}/"
  local pkg_version="$2"

  install_conda_package "docker" "${install_dir}" "${pkg_version}"
}

# ------------------------------------------

# Function to install local Podman with Anaconda (https://anaconda.org/conda-forge/podman)
function install_local_podman() {
  trap_to_debug_commands

  local install_dir="${1:-.}/"
  local pkg_version="$2"

  install_conda_package "podman" "${install_dir}" "${pkg_version}"
}

# ------------------------------------------

# Function to install local JQ with Anaconda (https://anaconda.org/conda-forge/jq)
function install_local_jq() {
  trap_to_debug_commands

  local install_dir="${1:-.}/"
  local pkg_version="$2"

  install_conda_package "jq" "${install_dir}" "${pkg_version}"
}

# ------------------------------------------

# Function to convert raw text (e.g. yaml) to encoded url format
function raw_to_url_encode() {

  local string="$(cat < /dev/stdin)"
  local strlen=${#string}
  local encoded=""
  local pos c o

  for (( pos=0 ; pos<strlen ; pos++ )); do
     c=${string:$pos:1}
     case "$c" in
        [-_.~a-zA-Z0-9] ) o="${c}" ;;
        * )               printf -v o '%%%02x' "'$c"
     esac
     encoded+="${o}"
  done

  echo "${encoded}"

}

# ------------------------------------------

# Function to download particular file or directory from particular Github commit or branch
function download_github_file_or_dir() {
  trap_to_debug_commands

  local git_user="$1"
  local git_project="$2"
  local commit_or_branch="${3:-master}" # default is master branch
  local dir_or_file="$4" # default is whole project tree

  archive_url="https://github.com/${git_user}/${git_project}/archive/${commit_or_branch}.tar.gz"

  echo -e "\n# Downloading from Github project: ${git_user}/${git_project} \
  ${commit_or_branch:+\n# Commit ID: $commit_or_branch} \
  \n# Directory: ${dir_or_file:-Whole project tree} \n"

  wget -O - ${archive_url} | tar xz --strip=1 "${git_project}-${commit_or_branch}/${dir_or_file}"

}

# ------------------------------------------

# Function to download file from URL, if local file doesn't exists, or has a different size on URL
function download_file() {
  trap_to_debug_commands
  # $1 => $source_file (download link)
  source_file="$1"

  # Optional: $2 => $target_file (where to save)
  target_file=${2:-$(basename "$1")}

  echo "# Downloading [$source_file] into $target_file"
  ls -ld

  local_file_size=$([[ -f ${target_file} ]] && wc -c < ${target_file} || echo "0")
  echo "local_file_size = $local_file_size"
  remote_file_size=$(curl -sI ${source_file} | awk '/Content-Length/ { print $2 }' | tr -d '\r' )
  echo "remote_file_size = $remote_file_size"
  [[ -n "$remote_file_size" ]] || remote_file_size=-1
  if [[ "$local_file_size" -ne "$remote_file_size" ]]; then
      echo "# Requested file was not downloaded yet, or remote file size is different. Downloading..."
      rm -rf "$target_file"
      if [[ -x "$(command -v wget)" ]] ; then
        target_dir=$(dirname -- "$target_file")
        wget "$source_file" --no-verbose --no-check-certificate --directory-prefix="$target_dir"
        source_file=$(basename -- "$source_file")
        mv "$target_dir/$source_file" "$target_file" || :
      else
        echo "# Wget is not installed, using Curl instead."
        curl -L "$source_file" -o "$target_file" --create-dirs
      fi
  else
    echo "# $target_file already downloaded, and equals to remote file $source_file"
  fi
}

# ------------------------------------------

# Function to delete files/directories which were not accessed N days (no recursive search in sub directories)
function delete_old_files_or_dirs() {
  trap_to_debug_commands
  # Input $1 : $wildcard_name
  wildcard_name="$1"

  # Input $2 (Optional) : $dir_or_file. f=file, d=directory (default)
  dir_or_file="${2:-f}"

  # Input $3 (Optional) : $access_days (default=5)
  access_days="${3:-5}"

  # Set $search_path as parent directory of $wildcard_name, if it includes slashes (current path by default)
  search_path=$(dirname -- "$wildcard_name")

  # Set $search_name as base directory of $wildcard_name, if it includes slashes
  search_name=$(basename -- "$wildcard_name")

  # find "$search_path"/* -maxdepth 0 -type $dir_or_file -name "$search_name" -atime +${access_days} -print -delete
  find "$search_path"/* -maxdepth 0 -type $dir_or_file -name "$search_name" -atime +${access_days} | xargs -n1 --verbose rm -rf
}

# ------------------------------------------

# Function to move non-empty directory to an existing directory
function backup_and_remove_dir() {
  trap_to_debug_commands
  # Input $1 : $src_dir
  src_dir="$1"

  # Input $2 (Optional) : $dest_dir
  dest_dir="${2:-_$1}"

  if [[ ! -d ${src_dir} ]] || [[ -z `ls -A "$src_dir"` ]] ; then
    echo "# Source directory [${src_dir}] doesn't exist (or empty). Skipping backup"
  else
    echo "# Backup directory [${src_dir}] into [${dest_dir}]"
    # Make a new directory (skip if exists)
    mkdir -p "$dest_dir"

    # Copy * from src_dir to dest_dir (-afr : Keep attributes, Force overwrite, Recursive copy)
    cp -afr "${src_dir}"/. "${dest_dir}"/

    # Force remove old directory
    lsof +D "$src_dir" | awk '{print $2}' | tail -n +2 | xargs --no-run-if-empty kill -9 || :
    rm -rf "$src_dir"
  fi
}

# ------------------------------------------

# Function to change value of a key, in a YAML file:
function change_yaml_key_value() {
  local yaml_file="$1"
  local key_name="$2"
  local new_value="$3"
  local section="$4" # optional section that the key is underneath

  echo -e "\n# Modifying YAML file [$yaml_file]: Key [$key_name] => Value [$new_value] ${section:+, under [$section].}"

  local section="${section:-.*}" # section default value is any string

  # sed -r "s/^(\s*${key}\s*:\s*).*/\1${new_value}/" -i "$yaml_file"

   echo "$(awk \
     -v key="(^[ \t]*(- )?${key_name}[ \t]*:?)(.*)$" \
     -v value="${new_value}" \
     -v section="${section}" \
     '
      BEGIN {
       done=0
       }
      {
         if (!done && $0 ~ section) {
           ++section_found
           key_found=0
         }
         replace = gensub(key, "\\1 "value, "g")
         if ($0 ~ key && replace) {
           ++key_found
         }
         if (!done && section_found>0 && key_found>0) {
           $0=replace
           done=1
         }
         print $0
       } ' "$yaml_file" \
    )" > "$yaml_file"
}

# ------------------------------------------

# Function to install aws-cli, and to configure AWS access:
function configure_aws_access() {
  AWS_PROFILE_NAME="$1"
  AWS_REGION="$2"
  AWS_KEY="$3"
  AWS_SECRET="$4"
  install_dir="${5:-./}"
  aws_bin_dir="${6:-$HOME/.local/bin}"

  # pip install awscli --upgrade --user
  aws_cli_installer="awscli-exe-linux-x86_64.zip"
  download_file "https://awscli.amazonaws.com/${aws_cli_installer}" "${install_dir}/${aws_cli_installer}"
  unzip -n "${install_dir}/${aws_cli_installer}" -d "${install_dir}"

  # export PATH=$HOME/.local/bin:$PATH
  # ${install_dir}/aws/install --update --install-dir ~/.local --bin-dir ~/.local/bin
  [[ ":$PATH:" != *":${aws_bin_dir}:"* ]] && export PATH="${PATH}:${aws_bin_dir}"
  ${install_dir}/aws/install --update --install-dir "${install_dir}/aws" --bin-dir "${aws_bin_dir}"

  aws --version

  (
    # subshell to hide commands
    aws configure set profile $AWS_PROFILE_NAME
    aws configure set aws_access_key_id $AWS_KEY
    aws configure set aws_secret_access_key $AWS_SECRET

    aws configure set default.region $AWS_REGION
    aws configure set output text
    aws configure set color on
  )

  # aws --profile "$AWS_PROFILE_NAME" configure  --region "$AWS_REGION" --output text --color on
  aws sts get-caller-identity
}

# ------------------------------------------

# Function to create json file for a DNS record deletion on AWS
function create_json_for_dns_delete() {
  # Input $1 : File to write the json output to
  (
  cat <<EOF
  {
      "Comment": "Delete single record set",
      "Changes": [
          {
              "Action": "DELETE",
              "ResourceRecordSet": {
                  "Name": "$(echo ${Name/\\052/*})",
                  "Type": "$Type",
                  "AliasTarget": {
                    "HostedZoneId": "$HostedZoneId",
                    "DNSName": "$DNSName",
                    "EvaluateTargetHealth": $EvaluateTargetHealth
                    }}
                }]
    }
EOF
  ) > $1
}

# ------------------------------------------

# Function to export variables from json file as environment variables
function export_vars_from_json() {
  # Input $1 : Json file to read variables from
  for s in $(grep -E '": [^\{]' "$1" | sed -e 's/: /=/' -e 's/^\s*//' -e "s/\(\,\)$//"); do
    echo "# export $s"
    eval export $s
  done
}

# ------------------------------------------

# Function to delete an DNS record set in AWS Hosted Zone
function delete_aws_dns_records() {
  trap_to_debug_commands;
  local hosted_zone_id="$1"
  local dns_record_set="$2"
  local tmp_file="`mktemp`_aws_records"

  echo -e "\n# Searching in Hosted Zone [$hosted_zone_id] - for a DNS record set: [$dns_record_set]"
  aws route53 list-resource-record-sets --hosted-zone-id $hosted_zone_id --query "ResourceRecordSets[?Name == '$dns_record_set']" --out json > "$tmp_file"

  cat $tmp_file

  if [[ $(< $tmp_file) == '[]' ]]; then
    echo -e "The DNS record set does not exist in AWS cluster [${CLUSTER_A_NAME}]"
    return
  fi

  echo -e "\n# Exporting DNS record set variables:"
  export_vars_from_json "$tmp_file"

  echo -e "\n# Creating json file for the DNS record set delete command:"
  create_json_for_dns_delete "$tmp_file"
  cat "$tmp_file"

  echo -e "\n# Deleting the DNS record set:"
  aws route53 change-resource-record-sets --hosted-zone-id $hosted_zone_id --change-batch file://$tmp_file

}

# ------------------------------------------

# Function to delete selcted namespace and CRDs on the current KUBECONFIG cluster
function delete_namespace_and_crds() {
  trap_to_debug_commands;
  # Input $1 : Namespace to delete
  # Input $2 (Optional) : All CRDs to delete, by searching for *crd_name*
  local ns_name=$1
  local crd_name=$2

  local delete_crds=FALSE

  if [[ -n $crd_name ]] ; then
    echo "# Searching for existing CRDs of previous ${crd_name} installation on the cluster:"
    ${OC} get crds |& highlight "${crd_name}" && delete_crds=TRUE || :

    # If there are existing CRDs on the cluster - remove them all:
    # if [[ $? = 0 ]]; then
    if [[ "$delete_crds" = TRUE ]]; then
      echo "# Deleting ${crd_name} CRDs in the cluster"
      crd_list=$(${OC} get crds -o name | grep -Po "\/\K.*${crd_name}.*")
      # Ignoring hanged CRDs on delete, due to K8s bug: https://github.com/kubernetes/kubernetes/issues/60538
      ${OC} delete --timeout=10s crds $crd_list --ignore-not-found || : # || : to ignore none-zero exit code
    fi
  fi

    echo "# Searching for existing resources (and OLMs) in namespace ${ns_name} :"
    # ${OC} get all,olm --all-namespaces | grep --color "${crd_name}" || delete_namespace=FALSE
    ${OC} get all,olm -n ${ns_name} |& highlight "No resources found" || :

  # Removing whole namespace (and it's resources):
  if [[ ${ns_name} = "default" ]] ; then
    echo "# Skipping namespace deletion, since it's the \"default\" namespace."
    return
  else
    echo "# Deleting whole namespace ${ns_name} from the cluster"
    force_delete_namespace "${ns_name}"
  fi
}


# ------------------------------------------

# Function to force delete namespace
function force_delete_namespace() {
  trap_to_debug_commands;
  # Input $1 : Namespace to delete
  ns_name=$1

  ${OC} delete --timeout=30s namespace ${ns_name} --ignore-not-found || force_ns_delete=TRUE

  if [[ "$force_ns_delete" = TRUE && $(${OC} get namespace ${ns_name}) ]]; then
    echo "# Warning: Force delete of namespace via API"
    ${OC} proxy &
    ${OC} get namespace ${ns_name} -o json | jq '.spec = {"finalizers":[]}' > temp.json
    curl -k -H "Content-Type: application/json" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/${ns_name}/finalize
    kill -9 %% || :
    ${OC} delete namespace ${ns_name} --ignore-not-found --grace-period=0 --force --wait || :
  fi

}

# ------------------------------------------

# Function to install and expose Nginx service on the current KUBECONFIG cluster
function install_nginx_service() {
  trap_to_debug_commands;

  ngnix_name="${1:-NginX}"
  nginx_image="${2:-quay.io/bitnami/nginx:latest}"

  target_namespace="$3" # The Namespace for Ngnix
  service_params="$4" # e.g. to expose Nginx as headless service on port 8080 pass: --port=8080 --cluster-ip=None

  if [[ -n "$target_namespace" ]] ; then
    echo "# Create Namespace for Nginx: $target_namespace"
    # delete_namespace_and_crds "${target_namespace}"
    create_namespace "${target_namespace}"
  fi

  echo "# Delete and create Nginx Deployment: $target_namespace (using Nginx image \"$nginx_image\")"
  ${OC} delete deployment ${ngnix_name} ${target_namespace:+-n $target_namespace} --ignore-not-found
  ${OC} create deployment ${ngnix_name} ${target_namespace:+-n $target_namespace} --image=$nginx_image

  echo "# Delete and expose Ngnix service:"
  ${OC} delete service ${ngnix_name} ${target_namespace:+-n $target_namespace} --ignore-not-found
  ${OC} expose deployment ${ngnix_name} --name=${ngnix_name} ${service_params} ${target_namespace:+-n $target_namespace}

  echo "# Wait 5 minutes for Ngnix service to be ready:"
  ${OC} rollout status --timeout=5m deployment ${ngnix_name} ${target_namespace:+-n $target_namespace}
  ${OC} describe pod ${ngnix_name} ${target_namespace:+-n $target_namespace}

}

# ------------------------------------------

# Function to create a namespace, if it does not exist
function create_namespace() {
  trap_to_debug_commands;

  local ns=$1
  # Create the namespace with the API directly
  echo -e "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: ${ns}" | ${OC} apply -f -

}

# ------------------------------------------

# Function to get the cluster name from kubeconfig current-context
function print_current_cluster_name() {
  # Do not trap_to_debug_commands

  local current_context=$(${OC} config current-context)
  ${OC} config view -o jsonpath="{.contexts[?(@.name == '$current_context')].context.cluster}"

}

# ------------------------------------------

# Function to print details of all used images in the running pods of a namespace
function print_images_info_of_namespace_pods() {
  # Do not trap_to_debug_commands

  local namespace=$1 # Default is current context namespace

  echo -e "\n### Images of Pods ${namespace:+(in namespace $namespace) }###"

  for img_id in $(${OC} get pods ${namespace:+-n $namespace} -o jsonpath="{..imageID}" \
  | tr -s '[[:space:]]' '\n' | sort | uniq -c | awk '{print $2}') ; do
  # for img in $(${OC} get images | awk '$0 ~ ENVIRON["REGISTRY_MIRROR"] { print $1 }') ; do
    print_image_info "$img_id" || continue
  done

}

# ------------------------------------------

# Function to print container details of an image
function print_image_info() {
  # Do not trap_to_debug_commands

  local img_id=$1
  local regex_container_info="\s+\K(name=|url=|version=|release=).*"

  echo -e "\n### $(echo $img_id | sed -r 's|.*/([^@]+).*|\1|') Image ###"
  echo "id=${img_id}"

  # Get info from image if the original image id is accessible by url
  ${OC} image info $img_id 2>/dev/null | grep -Po "$regex_container_info" || \
  {
    # Otherwise, get the info from local copy of the image
    img_id=$(echo $img_id | awk -F '@' '{print $2}')
    ${OC} describe image $img_id 2>/dev/null | grep -Po "$regex_container_info"
  } || :

}

# ------------------------------------------

# Function to print details of all image tags info in namespace
function print_image_tags_info() {
  # Do not trap_to_debug_commands

  local namespace=$1 # Default is current context namespace
  local regex_container_info="\s+\K(name=|url=|version=|release=).*"

  echo -e "\n### ImageStream Tags ${namespace:+(in namespace $namespace) }###"

  # Show image-stream tags
  ${OC} get istag ${namespace:+-n $namespace} | awk 'NR>1 {print $1}' | \
  while read -r img_tag ; do
    echo -e "\n### $(echo $img_tag | sed -r 's|.*/([^@]+).*|\1|') Image-Stream tag ###"
    ${OC} describe istag $img_tag ${namespace:+-n $namespace} 2>/dev/null \
    | grep -Po "$regex_container_info" || continue
  done

}

# ------------------------------------------

# Function to return (print) the first running pod id, by a label. Exit with FATAL if nothing found.
function get_running_pod_by_label() {
  # Do not trap_to_debug_commands, so it will not be printed as return value
  local pod_label=$1
  local namespace=$2 # Optional: Namespace for the pod
  local tmp_file="`mktemp`_pod_status"

  # Wait up to 3 minutes for the pod to be ready
  ${OC} wait --timeout=3m --for=condition=ready pod ${namespace:+-n $namespace} -l $pod_label > $tmp_file || :
  pod_status="$(head -1 $tmp_file | awk -F '/| ' '{print $2}' | xargs)"

  # pod_id="$(${OC} get pod ${namespace:+-n $namespace} -l $pod_label --field-selector status.phase=Running | awk 'FNR == 2 {print $1}')"
  # Can also run with: -o jsonpath="{.items[0].metadata.name}"

  #if [[ -n "$(echo $pod_status)" && -n "$(echo $pod_id)" ]] ; then
  if [[ -n "$pod_status" ]]; then
    # echo "$pod_id"
    printf '%s' "$pod_status"
    return 0
  else
    # exit 1
    FAILURE "Pod '$pod_label' is not running in '${namespace:-default}' namespace. \n $pod_status"
  fi

}

# ------------------------------------------

function wait_for_all_nodes_ready() {
### function to wait for all nodes to be ready, with optional timeout duration
  trap_to_debug_commands;

  local duration=${1:-3m} # Default is 3 minutes

  ${OC} wait --timeout=$duration --for=condition=ready nodes --all || nodes_status=DOWN
  ${OC} get nodes -o wide

  if [[ "$nodes_status" = DOWN ]] ; then
    FAILURE "Timeout (${duration}) exceeded while waiting for all nodes to be ready"
  fi

}

# ------------------------------------------

function wait_for_all_machines_ready() {
### function to wait for all Machine Configuration to be ready, with optional timeout duration
  trap_to_debug_commands

  local duration=${1:-20m} # Default is 20 minutes

  echo "# Wait up to $duration for Machine Config Daemon to be rolled out by openshift-machine-config-operator:"
  local cmd="${OC} rollout status --timeout=$duration ds -n openshift-machine-config-operator machine-config-daemon"
  watch_and_retry "$cmd" $duration || machines_status=DOWN

  echo "# Wait up to $duration for all Machines Config Pool to be updated:"
  cmd="${OC} wait --timeout=$duration --for condition=updated machineconfigpool --all"
  watch_and_retry "$cmd" $duration || machines_status=DOWN

  echo "# Status of Machine Config Pool and all Daemon-Sets:"
  ${OC} get machineconfigpool -o wide || machines_status=DOWN
  ${OC} get daemonsets -A -o wide || machines_status=DOWN

  if [[ "$machines_status" = DOWN ]] ; then
    FAILURE "Timeout (${duration}) exceeded while waiting for all Machine Configuration to be ready"
  fi

}

# ------------------------------------------

# Function to return (print) the all worker node ids, that have an external ip (nothing if none)
function get_worker_nodes_with_external_ip() {
  # Do not trap_to_debug_commands, so it will not be printed as return value
  ${OC} get nodes -l node-role.kubernetes.io/worker -o wide | awk '$7!="<none>" && NR>1 {print $1}'
}

# ------------------------------------------

# Function to return (print) all external ips of worker nodes (nothing if none)
function get_external_ips_of_worker_nodes() {
  # Do not trap_to_debug_commands, so it will not be printed as return value
  ${OC} get nodes -l node-role.kubernetes.io/worker -o wide | awk '$7!="<none>" && NR>1 {print $7}'
}

# ------------------------------------------

# Function to tail pod logs for N times, after each interval
function watch_pod_logs() {
  trap_to_debug_commands;

  local pod_to_watch="$1"
  local namespace="$2"
  local regex="$3"

  # Optional params:
  local retries=${4:-5}
  local interval=${5:-20s} # time between each retry attempt
  local tmp_file="`mktemp`_pod_log"
  local search_exit_code=0

  echo -e "# Tailing logs of Pod [$pod_to_watch] in Namespace [$namespace]: \n"

  cmd="${OC} logs --tail -1 --timestamps $pod_to_watch -n $namespace \
  > $tmp_file && grep -Ei '$regex' -m 1 -C 5 $tmp_file || sleep $interval"

  watch_and_retry "$cmd" "$retries" "$regex" || search_exit_code=$?

  if [[ "$search_exit_code" -ne 0 ]] ; then
    # print the log if pattern not found
    num_of_lines=$(wc -l < "$tmp_file")
    if (( num_of_lines > 100 )) ; then
      # crop 50 lines from head and tail, if num_of_lines larger than 100
      head -n 50 "$tmp_file"
      echo -e "\n...\n"
      tail -n 50 "$tmp_file"
    else
      cat "$tmp_file"
    fi
  fi

  return $search_exit_code
}

# ------------------------------------------

function print_pod_logs_in_namespace() {
  local cluster_name="$1"
  local namespace="$2"
  local pods_label="${3:+ -l $3}"

  echo -e "\n##################### Pods descriptions and logs${3:+ by label: "$3",} in cluster: $cluster_name, namespace: $namespace #####################\n"

  for pod in $(${OC} get pods $pods_label -n $namespace -o jsonpath='{.items[*].metadata.name}') ; do
    echo -e "\n### Pod $pod in cluster: $cluster_name, namespace: $namespace ###\n"
    ${OC} -n $namespace describe pod $pod || :
    ${OC} -n $namespace get pod $pod -o yaml || :

    if [[ "$(${OC} get pods -n $namespace $pod -o jsonpath='{.status.containerStatuses[*].ready}')" == true ]] ; then
      echo -e "\n### Active pod $pod logs ###\n"
      ${OC} -n $namespace logs $pod --timestamps --tail=500 || :
    else
      echo -e "\n### Terminated (previous) pod $pod logs ###\n"
      ${OC} -n $namespace logs -p $pod --timestamps --tail=500 || :
    fi
  done
}

# ------------------------------------------

function build_go_repo() {
### Pull and build a GO project from URL ###
  trap_to_debug_commands;

  local repo_url="${1#*://}" # For go get we need to trim the "https://"

  # Optional: branch or tag to pull (the default is latest branch)
  # local branch_or_tag=${2:+@$2}
  local branch_or_tag=${2}

  # local go_repo_and_branch="${repo_url}/...${branch_or_tag}"
  local go_repo_and_branch="${repo_url}/..."

  echo "# Downloading the repo [${go_repo_and_branch}] with GO: "
  # GO111MODULE="on" go get -v ${go_repo_and_branch} || echo "# 'go get ${go_repo_and_branch} might have had errors"
  GO111MODULE="off" go get -v ${go_repo_and_branch} || echo "# GO get '${go_repo_and_branch}' might have had errors"

  echo "# Pull latest changes with Git: "
  cd $GOPATH/src/$repo_url

  git_reset_local_repo ${branch_or_tag}

  echo "# Build the project with GO: "
  # make build # May fail if Docker is not pre-installed
  export GO111MODULE=on
  go mod vendor
  # go install -mod vendor # Compile binary and moves it to $GOBIN
  go build -mod vendor || echo "# No binary to compile in $repo_url"

  echo "# Project main directory content: "
  ls -la
}

# ------------------------------------------

# Function to fetch, reset and pull latest git repository (to be run inside local directory repo)
function git_reset_local_repo() {
  trap_to_debug_commands;

  # Optional param 2: another remote repository to pull from (e.g. fork)
  local forked_repo=${2}

  local remote_name="origin"

  if [[ -n "$forked_repo" ]] ; then
    remote_name="upstream"
    echo "# Add the a new remote '${remote_name}' to pull from: $forked_repo"
    git remote -v | grep -w ${remote_name} || git remote add ${remote_name} ${forked_repo}
    git remote set-url ${remote_name} ${forked_repo}
  fi

  # Optional param 1: branch or tag to pull (default is latest branch)
  local branch_or_tag=${1:-$(git remote show ${remote_name} | awk '/HEAD branch/ { print $3 }')}

  git config user.name 'Anonymous'
  git config user.email '<>'
  git config --get-all remote.${remote_name}.fetch
  git config --unset-all remote.${remote_name}.fetch
  git fetch --prune --all
  git branch -m ${branch_or_tag}
  # git branch -u ${remote_name} ${branch_or_tag}
  git remote set-branches --add ${remote_name} ${branch_or_tag}
  # git fetch ${remote_name} ${branch_or_tag}
  # git reset --hard ${remote_name}/${branch_or_tag}
  # git checkout --theirs .
  git reset --hard ${branch_or_tag}
  git pull -f ${remote_name} ${branch_or_tag}

  echo -e "\n# Latest commits in the repository: \n"
  git log --date=short -10 --pretty="%C(Yellow)%h %x09 %C(reset)%ad %x09 %C(Cyan)%an: %x09 %C(reset)%s"

}

# ------------------------------------------

# Function to create test cases document in Polarion from Junit xml
function create_polarion_testcases_doc_from_junit() {
    trap_to_debug_commands;

    # Input (5 params) :

    local polarion_url="$1"
    local polarion_auth_file="$2" # Includes Polarion 'user:password' | base64 --wrap 0
    local junit_xml="$3"
    local polarion_project_id="$4"
    local polarion_team_name="$5"
    local polarion_user_name="$6"
    local polarion_component_id="$7"
    local polarion_testcases_doc="$8"

    local tc_list="`mktemp`_testcases"

    echo -e "### Creating Polarion test-cases document for Junit results: $junit_xml
    \n# Polarion project: ${polarion_project_id}
    \n# Document path: ${polarion_team_name}/${polarion_testcases_doc}"

    # Verify junit file exists
    [[ -s "$junit_xml" ]] || { echo "Failed to create Polarion test-cases file: '$junit_xml' is missing." ; exit 1 ; }

    echo "# Set output xml for Polarion test-cases (without 'junit' substring in filename)"
    local polarion_testcases_file="$(basename ${junit_xml%.*})" # Get file name without path and extension
    polarion_testcases_file="${polarion_testcases_file//junit}polarion_testcases.xml" # Remove all "junit" from file name, and append "polarion_testcases.xml"
    polarion_testcases_file="$(dirname $junit_xml)/$(basename $polarion_testcases_file)" # Append full path as the original junit_xml file

    echo "# Getting list of Test-Cases in format 'class-name.test-name' (without sh2ju test index): "
    grep '<testcase ' "$junit_xml" | \
    sed -r 's/.*name="([^"]+).*classname="([^"]+).*/\2.\1/ ; s/[0-9]+ : //' > "$tc_list"
    #sed -r 's/.*name="([^"]+).*classname="([^"]+).*/\2.\1/ ; s/ /_/g ; s/[0-9]+_:_//' > "$tc_list"

    cat "$tc_list"

    echo -e "<?xml version=\"1.0\" encoding=\"UTF-8\"?>  \n\
    <testcases project-id=\"${polarion_project_id}\" document-relative-path=\"${polarion_team_name}/${polarion_testcases_doc}\">  \n\
      <properties>  \n\
          <property name=\"lookup-method\" value=\"name\"/>  \n\
      </properties>" > "$polarion_testcases_file"
      # Maybe required: <response-property name="${polarion_team_name}" value="testcase_importer"/>

    echo "# Injecting Polarion custom-fields for each test-case, into $polarion_testcases_file"
    # Todo: This should rather be defined on external properties file:
    while read tc_name ; do
      echo "<testcase approver-ids=\"${polarion_user_name}:approved\" status-id=\"approved\" assignee-id=\"${polarion_user_name}\">
          <title>${tc_name}</title>
            <description>${tc_name}</description>
            <custom-fields>

              <custom-field id=\"casecomponent\" content=\"${polarion_component_id}\" />
              <custom-field id=\"testtype\" content=\"functional\" />
              <custom-field id=\"caseimportance\" content=\"medium\" />
              <custom-field id=\"caselevel\" content=\"system\" />
              <custom-field id=\"caseposneg\" content=\"positive\" />
              <custom-field id=\"legacytest\" content=\"true\" />
              <custom-field id=\"caseautomation\" content=\"automated\" />
              <custom-field id=\"automation_script\" content=\"$(hostname) : $(realpath -s ${BASH_SOURCE[$i+1]})\" />
            </custom-fields>
        </testcase>
        " >> "$polarion_testcases_file"
      done < "$tc_list"

      # More options:
      #
      # <custom-field id=\"products\" content=\"ocp\" />
      # <custom-field id=\"subteam\" content=\"${polarion_team_name}\" />
      #
      # <custom-field id=\"legacytest\" content=\"true\" /> # Test needs at least one linked work item of type Requirement or be marked as a Legacy Test Case
      # <linked-work-items>
      #   <linked-work-item workitem-id=\"${tc_name}\" lookup-method=\"name\" role-id=\"verifies\" />
      #   <linked-work-item workitem-id=\"${tc_name}\" lookup-method=\"name\" role-id=\"derived_from\" />
      #   <linked-work-item workitem-id=\"${tc_name}\" lookup-method=\"name\" role-id=\"refines\" />
      # </linked-work-items>
      echo "</testcases>" >> "$polarion_testcases_file"

      # Watch testcase-log web page
      watch_polarion_job_status "$polarion_url" "$polarion_auth_file" "testcase" "$polarion_testcases_file"
}


# ------------------------------------------

# Function to create Polarion test run (xml) from Junit
function create_polarion_testrun_result_from_junit() {
    trap_to_debug_commands;

    # Input (6 params) :

    local polarion_url="$1"
    local polarion_auth_file="$2" # Includes Polarion 'user:password' | base64 --wrap 0
    local junit_xml="$3"
    local polarion_project_id="$4"
    local polarion_team_name="$5"
    local polarion_testrun_template="$6"

    local sed_expression

    echo -e "### Creating Polarion test-run (xml) for Junit results: $junit_xml
    \n# Polarion project: [${polarion_project_id}], team name: [${polarion_team_name}]"

    # Verify junit file exists
    [[ -s "$junit_xml" ]] || { echo "Failed to create Polarion test-run file: '$junit_xml' is missing." ; exit 1 ; }

    echo "# Set output xml for Polarion test-run results (without 'junit' substring in filename)"
    local polarion_testrun_file="$(basename ${junit_xml%.*})" # Get file name without path and extension
    polarion_testrun_file="${polarion_testrun_file//junit}polarion_testrun.xml" # Remove all "junit" from file name, and append "polarion_testrun.xml"
    polarion_testrun_file="$(dirname $junit_xml)/$(basename $polarion_testrun_file)" # Append full path as the original junit_xml file

    # Set Polarion test run title
    local polarion_testrun_title="$polarion_team_name Test Run - $(basename ${polarion_testrun_file%%.*})"
    echo -e "### Polarion Test-run Title: [${polarion_testrun_title}],
    \n# Polarion Test-run file [${polarion_testrun_file}] \n"

    cp "$junit_xml" "$polarion_testrun_file"

    echo "# Remove Test-Cases Junit index (if added within sh2ju): "
    sed -r 's/(<testcase.* name=")([0-9]+ : )(.*)/\1\3/' -i "$polarion_testrun_file"

    echo "# Add <testsuites> with Polarion project id: [${polarion_project_id}], team name: [${polarion_team_name}]"

    # Remove first <testsuiteS> tag (if exists):
    sed -e '0,/<testsuites>/s/<testsuites>//' -i "$polarion_testrun_file"
    # sed -r "s:<testsuites>::" -i "$polarion_testrun_file"

    # Define Polarion <testsuites> properties (it is passed into SED, make sure new lines ends with " \n\ ")
    polarion_suite_content="<testsuites> \n\
        <properties> \n\
            <property name=\"polarion-project-id\" value=\"$polarion_project_id\" /> \n\
            <property name=\"polarion-response-myteamsname\" value=\"$polarion_team_name\" /> \n\
            <property name=\"polarion-lookup-method\" value=\"name\" /> \n\
            <property name=\"polarion-testrun-title\" value=\"$polarion_testrun_title\" /> \n\
            <property name=\"polarion-testrun-template-id\" value=\"$polarion_testrun_template\" /> \n\
            <property name=\"polarion-testrun-status-id\" value=\"inprogress\" /> \n\
        </properties> \n\
    <testsuite "

    # Removed: <property name=\"polarion-testrun-id\" value=\"$polarion_testrun_id\" /> \n\

    sed_expression="0,/<testsuite /s,<testsuite ,$polarion_suite_content,"

    # Insert this <testsuiteS> properties instead of the first <testsuite> tag:
    sed -r "$sed_expression" -i "$polarion_testrun_file"

    # Remove last </testsuiteS> tag (if exists):
    sed -r -z "s:(.*)</testsuites>(.*):\1\2:" -i "$polarion_testrun_file"

    # Add </testsuiteS> tag, after the last </testsuite> tag:
    sed -r -z 's:(.*</testsuite>):\1\n</testsuites>:' -i "$polarion_testrun_file"

    # ------------------------------------------------------------------------ #

    echo "# Add <testcase> with Polarion test-run title: $polarion_testrun_title"

    # Define Polarion <testcase> property  (it is passed into SED, make sure new lines ends with " \n\ ")
    polarion_testcase_content="\n\
        <properties> \n\
            <property name=\"polarion-testcase-comment\" value=\"$polarion_testrun_file\" /> \n\
        </properties> \n\
    </testcase>"

    sed_expression="s:</testcase>:$polarion_testcase_content:g"

    # Insert the property instead of EACH testcase tag
    sed -r "$sed_expression" -i "$polarion_testrun_file"

    # Watch testcase-log web page
    watch_polarion_job_status "$polarion_url" "$polarion_auth_file" "xunit" "$polarion_testrun_file"

    local cmd="curl --config $polarion_auth_file -k ${polarion_url}/import/xunit-log?jobId=${xunit_jobid}"
    local testrun_url="$($cmd | awk -F'&#034;' '/testrun-url/ { print $4 }')"

    # Set link for all templates:
    all_templates_link="${polarion_url}/#/project/${polarion_project_id}/testruns?query=template.id%3A\"${polarion_testrun_template// /%20}\""

    # Set links for all titles:
    all_titles_link="${polarion_url}/#/project/${polarion_project_id}/testruns?query=title%3A\"${polarion_testrun_title// /%20}\""

    echo -e "\n### The new test-run was successfully added to Polarion ###
    \n# $testrun_url \n
    \n# All test-runs by template-id '${polarion_testrun_template}':
    \n# ${all_templates_link}
    \n# All test-runs by title '${polarion_testrun_title}':
    \n# ${all_titles_link}"

}

# ------------------------------------------

# Function to watch testcase-log or xunit-log web page, for "Message sent" (replace quotes special HTML characters to ASCII)
function watch_polarion_job_status() {
  trap_to_debug_commands;

  # Input params:
  local polarion_url="$1"
  local polarion_auth_file="$2" # Includes Polarion 'user:password' | base64 --wrap 0
  local polarion_import_type="$3" # "xunit" or "testcase"
  local polarion_import_file="$4" # filepath to import

  echo -e "\n# Import $polarion_import_type file [$polarion_import_file] into ${polarion_url}/import/${polarion_import_type} :\n"

  local polarion_log="`mktemp`_polarion_log"
  curl --config "$polarion_auth_file" -k -X POST -F file=@${polarion_import_file} ${polarion_url}/import/${polarion_import_type} |& tee $polarion_log

  local polarion_job_id=$(awk '/job-ids/ { print $4 }' $polarion_log)

  if [[ -z "$polarion_job_id" ]] || [[ "$polarion_job_id" = 0 ]] ; then
    echo "Error in the file or data to import to Polarion: $polarion_import_file" 1>&2
    exit 1
  fi

  local polarion_job_url="${polarion_url}/import/${polarion_import_type}-log?jobId=${polarion_job_id}"
  echo -e "\n# Checking Polarion ${polarion_import_type} import job status at: $polarion_job_url"

  local cmd="curl --config $polarion_auth_file -k $polarion_job_url | sed -r 's/&#034;|&#039;/\"/g' |& tee $polarion_log"
  local regex='Import.+Message sent'
  watch_and_retry "$cmd" 1m "$regex"

  if tail -n 10 "$polarion_log" | grep "failed" ; then
    echo "Importing ${polarion_import_type} to Polarion did not complete successfully" 1>&2
    exit 1
  fi
}

# ------------------------------------------

function log_to_html() {
  trap_to_debug_commands;

  # Input params:

  local log_file="$1"
  local title="${2:-"${log_file%%.*}"}" # html title (default is log filename)
  local html_output="${3:-${title//[ \/]/_}.html}" # html filename to create (default is title with underscores)
  local description="$4"

  echo "# Generating HTML report from: $log_file"
  [[ -f $log_file ]] || { echo "# Error - The file does not exists, or is not accessible" ; exit 1 ; }

  # Set description div
  [[ -z $description ]] || description="<h3><div style=\"white-space: pre-line\"> ${description}
  </div></h3>"

  local repo_url=https://github.com/theZiz/aha.git
  local repo_branch=0.5.1
  local repo_dir=$(basename $repo_url .git)

  # Get AHA binary
  if [[ ! -f ./$repo_dir/aha ]]; then
      rm -rf $repo_dir
      # git clone --quiet --branch "$repo_branch" $repo_url > /dev/null
      git clone $repo_url --single-branch # --depth 1
      git --work-tree=$repo_dir  --git-dir=$repo_dir/.git checkout tags/$repo_branch
      make -C $repo_dir &> /dev/null
  fi
  ./$repo_dir/aha --version

  # Create HTML Header
  cat > "$html_output" <<EOF
  <?xml version="1.0" encoding="UTF-8" ?>
  <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
  <html xmlns="http://www.w3.org/1999/xhtml">
  <head>
  <meta http-equiv="Content-Type" content="application/xml+xhtml; charset=UTF-8; width=device-width; initial-scale=1" />
  <title>${title}</title>
  <style type="text/css">
  pre {white-space: pre-wrap; white-space: -moz-pre-wrap !important;
  white-space: -pre-wrap; white-space: -o-pre-wrap; word-wrap: break-word;}
  .collapsible {
    background-color: LightSkyBlue;
    color: black;
    cursor: pointer;
    padding: 10px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 20px;
    -webkit-touch-callout: text;
    -webkit-user-select: text;
    -khtml-user-select: text;
    -moz-user-select: text;
    -ms-user-select: text;
    user-select: text;
  }
  .active {
    background-color: RoyalBlue;
    color: white;
  }
  .collapsible:hover {
    background-color: RoyalBlue;
    color: white;
    text-decoration: underline;
  }
  .content {
    padding: 0 18px;
    display: none;
    overflow: hidden;
    background-color: LightSkyBlue;
  }
  </style>
  </head>
  <body style="background-color:LightSkyBlue">
  <h1>${title}</h1>
  ${description}
  <pre>
EOF

  # Trim empty lines (from top) and spaces (not tabs) from each line in the log file
  #sed '/\S/,$!d; s/^ \+//; s/ \+$//' -i $log_file
  sed '/\S/,$!d' -i "$log_file"

  # Create HTML content with AHA
  ./$repo_dir/aha -f $log_file --title "$title" --word-wrap --no-header >> $html_output

  # Make all urls as hyperlinks
  sed -r "s|(^[^<]*)(https?://[^> \t]+)([^>]*$)|\1<a href='\2'>\2</a>\3|" -i $html_output

  # Replace all lines of "<span...### {text...} ###</span>" - with a collapsible span ("###" was created with "PROMPT" function)
  sed -r 's:<span.+###\s+:<button type="button" class="collapsible"><span>:g' -i $html_output
  sed -r 's:\s+###</span.+$:<\/span><\/button><div class="content">:g' -i $html_output

  # Close all collapsible spans, when getting to lines of ðŸŽ©ï¸Ž (excluding first time)
  sed -r 's:(.+ðŸŽ©ï¸Ž):<\/div><br><br>\1:g' -i $html_output
  sed '0,/<\/div><br><br>/s///' -i $html_output

  # Create HTML collapsible function (must be at the bottom)
  cat >> $html_output <<EOF
  </pre>
  <br><br><br>
  <script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.display === "block") {
        content.style.display = "none";
      } else {
        content.style.display = "block";
      }
    });
  }
  </script>
  </body>
  </html>
EOF

  echo -e "HTML report created:\n$html_output"
}
