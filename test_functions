#!/bin/bash
# shellcheck disable=SC2153,SC2031,SC2016,SC2120,SC2005,SC1091 source=/dev/null

### Import Submariner setup variables ###
source "$SCRIPT_DIR/subm_variables"

####################################################################################
#                             System Test Functions                                #
####################################################################################

# ------------------------------------------

function export_all_env_variables() {
### Helper function to export all environment and test variables ###
  PROMPT "Exporting environment and test variables"

  ### Set script in debug/verbose mode, if used CLI option: --debug / -d ###
  if [[ "$SCRIPT_DEBUG_MODE" =~ ^(yes|y)$ ]]; then

    trap_to_debug_commands;

    # Extra verbosity for oc commands:
    # https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-output-verbosity-and-debugging
    # export VERBOSE_FLAG="--v=2"
    # export OC="$OC $VERBOSE_FLAG"

    # Debug flag for aws commands
    export DEBUG_FLAG="--debug"

  else
    # Clear trap_to_debug_commands function
    trap_to_debug_commands() { :; }
  fi

  export -f trap_to_debug_commands

  if [[ -s "$GLOBAL_VARS" ]]; then
    TITLE "Sourcing a local variables file (overrides global variables) from:
    --import-vars $GLOBAL_VARS"

    source "$GLOBAL_VARS"
  fi

  export_global_script_variable

  set_missing_cli_variables

  if [[ "$CREATE_JUNIT_XML" =~ ^(y|yes)$ ]] ; then
    export JUNIT_CMD="record_junit $SHELL_JUNIT_XML $TEST_STATUS_FILE"
    export JUNIT_CMD_IGNORE_STATUS="record_junit $SHELL_JUNIT_XML"
  fi

  echo -e "\n# Exporting Clusters variables based on \$WORKDIR and \$CLUSTER_X_NAMEs from 'subm_variables' or the sourced file (imported with '--import-vars)"

  # Exporting variables for Managed Cluster A
  export_cluster_variables "${CLUSTER_A_NAME}" "${CLUSTER_A_YAML}" "A"

  # Currently the Hub is also installed on Cluster A
  export KUBECONF_HUB="${KUBECONF_CLUSTER_A}"

  # Exporting variables for Managed Cluster B
  export_cluster_variables "${CLUSTER_B_NAME}" "${CLUSTER_B_YAML}" "B"

  # Exporting variables for Managed Cluster C
  export_cluster_variables "${CLUSTER_C_NAME}" "${CLUSTER_C_YAML}" "C"

  if [[ ! -s "$CLUSTER_B_YAML" ]] && [[ ! -s "$CLUSTER_C_YAML" ]] ; then

    FATAL "No OCP YAML were set for either cluster B or cluster C. \
    Multi-Cluster tests require at least one more cluster (beside cluster A)"
      
  fi

  # Set $SUBM_VER_TAG and $ACM_VER_TAG variables with the correct version (vX.Y.Z), branch name, or tag
  set_versions_variables

  # List all environment variables
  print_all_env_variables

}

# ------------------------------------------

function export_global_script_variable() {
### Helper function to export global script variables ###
  trap_to_debug_commands;

  TITLE "Exporting global script variables"

  #####################################################################################################
  #         Constant variables and files (overrides previous variables from sourced files)            #
  #####################################################################################################

  # Set WORKDIR as an absolute path to the directory
  mkdir -p "$WORKDIR"
  export_absolute_path 'WORKDIR'

  # Date-time signature for log and report files
  DATE_TIME="$(date +%d%m%Y_%H%M)"
  export DATE_TIME

  # Create OUTPUT_DIR + DATE_TIME
  OUTPUT_DIR+="_${DATE_TIME}"
  mkdir -p "$OUTPUT_DIR"
  export_absolute_path 'OUTPUT_DIR'

  # Trim trailing and leading spaces from $TEST_NS
  TEST_NS="$(echo "$TEST_NS" | xargs)"
  export TEST_NS

  # Export cred files absolute paths
  export_absolute_path 'GCP_CRED_JSON'
  export_absolute_path 'OSP_CRED_YAML'

  # Global temp file
  TEMP_FILE="$(mktemp)_temp"
  export TEMP_FILE

  # JOB_NAME is a prefix for files, which is the name of current script directory
  JOB_NAME="$(basename "$SCRIPT_DIR")"
  export JOB_NAME
  export SHELL_JUNIT_XML="$OUTPUT_DIR/${JOB_NAME}_sys_junit.xml"
  export PKG_JUNIT_XML="$OUTPUT_DIR/${JOB_NAME}_pkg_junit.xml"
  export E2E_JUNIT_XML="$OUTPUT_DIR/${JOB_NAME}_e2e_junit.xml"
  export LIGHTHOUSE_JUNIT_XML="$OUTPUT_DIR/${JOB_NAME}_lighthouse_junit.xml"

  export E2E_LOG="$OUTPUT_DIR/${JOB_NAME}_e2e_output.log"
  : > "$E2E_LOG"

  # Set SYS_LOG name according to REPORT_NAME (from subm_variables)
  export REPORT_NAME="${REPORT_NAME:-Submariner Tests}"
  # SYS_LOG="${REPORT_NAME// /_}" # replace all spaces with _
  # SYS_LOG="${SYS_LOG}_${DATE_TIME}.log" # can also consider adding timestamps with: ts '%H:%M:%.S' -s
  SYS_LOG="${OUTPUT_DIR}/${JOB_NAME}_${DATE_TIME}.log" # can also consider adding timestamps with: ts '%H:%M:%.S' -s
  : > "$SYS_LOG"

  # Common test variables
  export NEW_NETSHOOT_CLUSTER_A="${NETSHOOT_CLUSTER_A}-new" # A NEW Netshoot pod on cluster A
  export HEADLESS_TEST_NS="${TEST_NS}-headless" # Namespace for the HEADLESS $NGINX_CLUSTER_BC service

  # ACM cluster-set and binding name for the managed clusters to be connected with Submariner
  export ACM_CLUSTER_SET="${TEST_NS}-set${CLUSTER_SET_ID}"

  BUG "Submariner Broker created but Gateway was not" \
  "Trim the length of ACM cluster set" \
  "https://issues.redhat.com/browse/ACM-2076"
  # Workaround:
  export ACM_CLUSTER_SET="submariner-set${CLUSTER_SET_ID}"


  #################################################################################
  #               Saving important test properties in local files                 #
  #################################################################################

  # File and variable to store test status. Resetting to empty - before running tests (i.e. don't publish to Polarion yet)
  export TEST_STATUS_FILE="$OUTPUT_DIR/test_status.rc"
  export EXIT_STATUS
  : > "$TEST_STATUS_FILE"

  # Filename (relative to current directory) to store OCP version 
  export OCP_VERSION_FILE="installer.ver"

  # File to store SubCtl version
  export SUBCTL_VERSION_FILE="$OUTPUT_DIR/subctl.ver"
  : > "$SUBCTL_VERSION_FILE"

  # File to store Submariner components images versions
  export SUBM_IMAGES_VERSION_FILE="$OUTPUT_DIR/submariners.ver"
  : > "$SUBM_IMAGES_VERSION_FILE"

  # File to store SubCtl JOIN command for cluster A
  export SUBCTL_JOIN_CLUSTER_A_FILE="$OUTPUT_DIR/subctl_join_cluster_a.cmd"
  : > "$SUBCTL_JOIN_CLUSTER_A_FILE"

  # File to store SubCtl JOIN command for cluster B
  export SUBCTL_JOIN_CLUSTER_B_FILE="$OUTPUT_DIR/subctl_join_cluster_b.cmd"
  : > "$SUBCTL_JOIN_CLUSTER_B_FILE"

  # File to store SubCtl JOIN command for cluster C
  export SUBCTL_JOIN_CLUSTER_C_FILE="$OUTPUT_DIR/subctl_join_cluster_c.cmd"
  : > "$SUBCTL_JOIN_CLUSTER_C_FILE"

  # File to store Polarion auth
  export POLARION_AUTH="$OUTPUT_DIR/polarion.auth"
  : > "$POLARION_AUTH"

  # File to store Polarion test-run report link
  export POLARION_RESULTS="$OUTPUT_DIR/polarion_${DATE_TIME}.results"
  : > "$POLARION_RESULTS"

  # File to store Submariner images version details
  export PRODUCT_IMAGES="$OUTPUT_DIR/product_images.ver"
  : > "$PRODUCT_IMAGES"

}

# ------------------------------------------

function export_absolute_path() {
### Helper function to set and export absolute path of file/dir variable ###

  local file_var_name="$1" # Make sure to pass the var name and not $var value

  # Set original_path as the actual value of file_var_name
  local original_path="${!file_var_name}" || :

  # Get the absolute path
  original_path="$(realpath -s "$original_path")"

  # Export the variable
  export "${file_var_name}=${original_path}"
}

# ------------------------------------------

function set_missing_cli_variables() {
### Helper function to set missing cli/user variable ###
  trap_to_debug_commands;

  TITLE "Set CLI/User inputs if missing (Default is 'NO' for any unset variable)"

  # export BUILD_OPERATOR=${BUILD_OPERATOR:-NO} # [DEPRECATED]
  export BUILD_GO_TESTS=${BUILD_GO_TESTS:-NO}
  export INSTALL_ACM=${INSTALL_ACM:-NO}
  export INSTALL_MCE=${INSTALL_MCE:-NO}
  export INSTALL_SUBMARINER=${INSTALL_SUBMARINER:-NO}
  export INSTALL_WITH_SUBCTL=${INSTALL_WITH_SUBCTL:-NO}
  export REGISTRY_IMAGES=${REGISTRY_IMAGES:-NO}
  export DESTROY_CLUSTER_A=${DESTROY_CLUSTER_A:-NO}
  export CREATE_CLUSTER_A=${CREATE_CLUSTER_A:-NO}
  export RESET_CLUSTER_A=${RESET_CLUSTER_A:-NO}
  export CLEAN_CLUSTER_A=${CLEAN_CLUSTER_A:-NO}
  export DESTROY_CLUSTER_B=${DESTROY_CLUSTER_B:-NO}
  export CREATE_CLUSTER_B=${CREATE_CLUSTER_B:-NO}
  export RESET_CLUSTER_B=${RESET_CLUSTER_B:-NO}
  export CLEAN_CLUSTER_B=${CLEAN_CLUSTER_B:-NO}
  export DESTROY_CLUSTER_C=${DESTROY_CLUSTER_C:-NO}
  export CREATE_CLUSTER_C=${CREATE_CLUSTER_C:-NO}
  export RESET_CLUSTER_C=${RESET_CLUSTER_C:-NO}
  export CLEAN_CLUSTER_C=${CLEAN_CLUSTER_C:-NO}
  export JOIN_CLUSTER_A=${JOIN_CLUSTER_A:-NO}
  export JOIN_CLUSTER_B=${JOIN_CLUSTER_B:-NO}
  export JOIN_CLUSTER_C=${JOIN_CLUSTER_C:-NO}
  export GLOBALNET=${GLOBALNET:-NO}
  export SUBM_CABLE_DRIVER=${SUBM_CABLE_DRIVER:-libreswan}
  export CONFIG_GOLANG=${CONFIG_GOLANG:-NO}
  export CONFIG_CLOUDS_CLI=${CONFIG_CLOUDS_CLI:-NO}
  export SKIP_OCP_SETUP=${SKIP_OCP_SETUP:-NO}
  export SKIP_TESTS=${SKIP_TESTS:-NO}
  export PRINT_LOGS=${PRINT_LOGS:-NO}
  export CREATE_JUNIT_XML=${CREATE_JUNIT_XML:-NO}
  export UPLOAD_TO_POLARION=${UPLOAD_TO_POLARION:-NO}
  export SCRIPT_DEBUG_MODE=${SCRIPT_DEBUG_MODE:-NO}

}

# ------------------------------------------

function export_cluster_variables() {
### Helper function to set cluster name and install directory (based on defaults from global variables file)
  trap_to_debug_commands;

  local cluster_name="$1"
  local cluster_install_yaml="$2"
  local cluster_index="$3" # A / B / C

  TITLE "Exporting Cluster ${cluster_index} variables and kubeconfig for Cluster-Set '${CLUSTER_SET_ID}'"

  if [[ -z "${cluster_name}" ]] ||
     [[ -z "${cluster_install_yaml}" ]] ||
     [[ -z "${cluster_index}" ]] ; then
       BUG "Required parameters for cluster installation are missing:
       Cluster Name: ${cluster_name}
       Cluster Installer YAML: ${cluster_install_yaml}
       Cluster Index (A/B/C): ${cluster_index}
       "
  fi

  # If deploying a new cluster, check that the given installer yaml is not corrupted
  verify_yaml_for_cluster_deploy "${cluster_index}"

  local cluster_install_platform
  local cluster_install_dir

  # Set $cluster_install_yaml as an absolute path
  cluster_install_yaml="$(realpath -s "$cluster_install_yaml")" 2>/dev/null || :

  # Check if the given installer yaml contains cluster domain
  if grep 'Domain:' "$cluster_install_yaml" ; then
    echo -e "\n# Reading Cluster Installer YAML: ${cluster_install_yaml}"

    # Get cluster platform type from OCP installer yaml
    cluster_install_platform="$(print_ocp_install_platform "${cluster_install_yaml}")"

    echo -e "\n# Prefix platform type '${cluster_install_platform}' to the cluster name '${cluster_name}'"
    cluster_name="${cluster_install_platform:+${cluster_install_platform}-}${cluster_name}"

    echo -e "\n# Exporting \$KUBECONF_CLUSTER_${cluster_index} for $cluster_name"

    cluster_install_dir="${WORKDIR}/${cluster_name}"

    export "CLUSTER_${cluster_index}_NAME=${cluster_name}${CLUSTER_SET_ID}"
    export "CLUSTER_${cluster_index}_DIR=${cluster_install_dir}${CLUSTER_SET_ID}"
    export "KUBECONF_CLUSTER_${cluster_index}=${cluster_install_dir}${CLUSTER_SET_ID}/auth/kubeconfig"
    export "CLUSTER_${cluster_index}_YAML=${cluster_install_yaml}"

  else
    TITLE "Unset Cluster ${cluster_index} global variables (name, kubeconfig and yaml), since this cluster is not required for testing"

    unset "CLUSTER_${cluster_index}_NAME"
    unset "CLUSTER_${cluster_index}_DIR"
    unset "KUBECONF_CLUSTER_${cluster_index}"
    unset "CLUSTER_${cluster_index}_YAML"
  fi

}

# ------------------------------------------

function show_test_plan() {
  PROMPT "Input parameters and Test Plan steps"

  if [[ "$SKIP_OCP_SETUP" =~ ^(y|yes)$ ]]; then
    echo -e "\n# Skipping OCP clusters setup (destroy / create / clean): $SKIP_OCP_SETUP \n"
  else
    echo "### Execution plan: Openshift clusters creation/cleanup before Submariner deployment:

    AWS cluster A (public):
    - DESTROY_CLUSTER_A: $DESTROY_CLUSTER_A
    - CREATE_CLUSTER_A: $CREATE_CLUSTER_A
    - RESET_CLUSTER_A: $RESET_CLUSTER_A $TARGET_VERION_CLUSTER_A
    - CLEAN_CLUSTER_A: $CLEAN_CLUSTER_A

    OSP cluster B (on-prem):
    - DESTROY_CLUSTER_B: $DESTROY_CLUSTER_B
    - CREATE_CLUSTER_B: $CREATE_CLUSTER_B
    - RESET_CLUSTER_B: $RESET_CLUSTER_B $TARGET_VERION_CLUSTER_B
    - CLEAN_CLUSTER_B: $CLEAN_CLUSTER_B

    AWS/GCP cluster C:
    - DESTROY_CLUSTER_C: $DESTROY_CLUSTER_C
    - CREATE_CLUSTER_C: $CREATE_CLUSTER_C
    - RESET_CLUSTER_C: $RESET_CLUSTER_C $TARGET_VERION_CLUSTER_C
    - CLEAN_CLUSTER_C: $CLEAN_CLUSTER_C
    "
  fi

  TITLE "Execution plan: Submariner deployment and environment preparations"

  echo -e "\n# OCP and Submariner setup and test tools:
  - CONFIG_GOLANG: $CONFIG_GOLANG
  - CONFIG_CLOUDS_CLI: $CONFIG_CLOUDS_CLI
  - build_operator_latest: \$build_operator # [DEPRECATED]
  - build_submariner_repos: $BUILD_GO_TESTS
  "

  echo -e "\n# Submariner deployment and environment setup for the tests:

  - update_kubeconfig_context_cluster_a
  - update_kubeconfig_context_cluster_b / c
  - test_kubeconfig_cluster_a
  - test_kubeconfig_cluster_b / c
  - uninstall_submariner_cluster_a
  - delete_old_submariner_images_from_cluster_a
  - delete_all_evicted_pods_in_current_cluster_a
  - uninstall_submariner_cluster_b / c
  - delete_old_submariner_images_from_cluster_b / c
  - delete_all_evicted_pods_in_current_cluster_b / c
  - add_elevated_user_to_cluster_a
  - add_elevated_user_to_cluster_b / c
  - open_firewall_ports_on_cluster_a
  - configure_images_prune_cluster_a
  - open_firewall_ports_on_openstack_cluster_b
  - label_gateway_on_broker_nodes_with_external_ip
  - open_firewall_ports_on_cluster_c
  - label_first_gateway_cluster_b / c
  - configure_custom_registry_in_cluster: $REGISTRY_IMAGES
  - upload_submariner_images_to_cluster_registry: $REGISTRY_IMAGES
  - configure_namespace_for_submariner_tests_on_cluster_a
  - configure_namespace_for_submariner_tests_on_managed_cluster
  - test_kubeconfig_cluster_a
  - test_kubeconfig_cluster_b / c
  - install_netshoot_app_on_cluster_a
  - install_nginx_svc_on_managed_cluster
  - test_basic_cluster_connectivity_before_submariner
  - test_clusters_disconnected_before_submariner
  - install_mce_operator_on_hub $MCE_VER_TAG
  - install_acm_operator_on_hub $ACM_VER_TAG
  - download_and_install_subctl $SUBM_VER_TAG
  "

  if [[ "$INSTALL_WITH_SUBCTL" =~ ^(y|yes)$ ]]; then
    TITLE "Installing Submariner with SubCtl tool"

    echo -e "
    - test_subctl_command
    - set_join_parameters_for_cluster_a
    - set_join_parameters_for_cluster_b / c
    - append_custom_images_to_join_cmd_cluster_a
    - append_custom_images_to_join_cmd_cluster_b / c
    - install_broker_via_subctl_on_cluster_a
    - test_broker_before_join
    - run_subctl_join_on_cluster_a
    - run_subctl_join_on_cluster_b / c
    $([[ ! "$GLOBALNET" =~ ^(y|yes)$ ]] || echo "- test globalnet") \
    "
  fi

  echo -e "\n# TODO: Should add function to manipulate opetshift clusters yamls, to have overlapping CIDRs"

  if [[ "$SKIP_TESTS" =~ ((sys|all)(,|$))+ ]]; then
    echo -e "\n# Skipping high-level (system) tests: $SKIP_TESTS \n"
  else
  TITLE "Execution plan: High-level (System) tests of Submariner"
  echo -e "
    - test_submariner_resources_cluster_a
    - test_submariner_resources_cluster_b / c
    - test_public_ip_on_gateway_node
    - test_disaster_recovery_of_gateway_nodes
    - test_renewal_of_gateway_and_public_ip
    - test_cable_driver_cluster_a
    - test_cable_driver_cluster_b / c
    - test_subctl_show_on_merged_kubeconfigs
    - test_ha_status_cluster_a
    - test_ha_status_cluster_b / c
    - test_submariner_connection_cluster_a
    - test_submariner_connection_cluster_b / c
    - test_globalnet_status_cluster_a: $GLOBALNET
    - test_globalnet_status_cluster_b / c: $GLOBALNET
    - export_nginx_default_namespace_managed_cluster
    - export_nginx_headless_namespace_managed_cluster
    - test_lighthouse_status_cluster_a
    - test_lighthouse_status_cluster_b / c
    - test_clusters_connected_by_service_ip
    - install_new_netshoot_cluster_a
    - install_nginx_headless_namespace_managed_cluster
    - test_clusters_connected_overlapping_cidrs_globalnet_v1: $GLOBALNET
    - test_new_netshoot_ip_cluster_a_globalnet_v1: $GLOBALNET
    - test_nginx_headless_ip_globalnet_v1: $GLOBALNET
    - test_clusters_connected_full_domain_name
    - test_clusters_cannot_connect_short_service_name
    - test_clusters_connected_headless_service_on_new_namespace
    - test_clusters_cannot_connect_headless_short_service_name
    "
  fi

  if [[ "$SKIP_TESTS" =~ ((pkg|all)(,|$))+ ]]; then
    echo -e "\n# Skipping Submariner unit-tests: $SKIP_TESTS \n"
  else
    echo -e "\n### Execution plan: Unit-tests (Ginkgo Packages) of Submariner:

    - test_submariner_packages
    "
  fi

  if [[ "$SKIP_TESTS" =~ ((e2e|all)(,|$))+ ]]; then
    echo -e "\n# Skipping Submariner E2E tests: $SKIP_TESTS \n"
  else
    echo -e "\n### Execution plan: End-to-End (Ginkgo E2E) tests of Submariner:

    - test_submariner_e2e_with_go: $([[ "$BUILD_GO_TESTS" =~ ^(y|yes)$ ]] && echo 'YES' || echo 'NO' )
    - test_submariner_e2e_with_subctl: $([[ ! "$BUILD_GO_TESTS" =~ ^(y|yes)$ ]] && echo 'YES' || echo 'NO' )
    "
  fi

}

# ------------------------------------------

function setup_workspace() {
  PROMPT "Configuring workspace (Golang, AWS-CLI, GCP-CLI, Terraform, Polarion) in: ${WORKDIR}"
  trap '' DEBUG # DONT trap_to_debug_commands

  # Create WORKDIR and local BIN dir (if not yet exists)
  mkdir -p "${WORKDIR}"
  mkdir -p "$HOME/.local/bin"

  # Add local BIN dir to PATH
  [[ ":$PATH:" == *":$HOME/.local/bin:"* ]] || export PATH="$HOME/.local/bin:$PATH"

  # CD to main working directory
  cd "${WORKDIR}" || return 1

  # Installing GoLang with Anaconda, if $CONFIG_GOLANG = yes/y
  if [[ "$CONFIG_GOLANG" =~ ^(y|yes)$ ]] ; then
    TITLE "Installing Anaconda (virtual environment)"
    install_anaconda "${WORKDIR}"

    TITLE "Installing GoLang with Anaconda"
    install_local_golang "${WORKDIR}"

    # Set GOBIN to local directory in ${WORKDIR}
    export GOBIN="${WORKDIR}/GOBIN"
    mkdir -p "$GOBIN"

    # Verify GO installation
    verify_golang

    if [[ -e ${GOBIN} ]] ; then
      echo -e "\n# Re-exporting global variables"
      export OC="${GOBIN}/oc $VERBOSE_FLAG"
    fi

    TITLE "Installing JQ (JSON parser) with Anaconda"
    install_local_jq "${WORKDIR}"

  else
    # Verify GO installation
    verify_golang
  fi

  # Set Polarion access if $UPLOAD_TO_POLARION = yes/y
  if [[ "$UPLOAD_TO_POLARION" =~ ^(y|yes)$ ]] ; then
    TITLE "Set Polarion access for the user [$POLARION_USR]"
    ( # subshell to hide commands
      local polauth
      polauth=$(echo "${POLARION_USR}:${POLARION_PWD}" | base64 --wrap 0)
      echo "--header \"Authorization: Basic ${polauth}\"" > "$POLARION_AUTH"
    )
  fi

  # Installing Clouds CLIs and Configuring access, if $CONFIG_CLOUDS_CLI = yes/y
  if [[ "$CONFIG_CLOUDS_CLI" =~ ^(y|yes)$ ]] ; then
    TITLE "Installing and configuring AWS CLI for profile [$AWS_PROFILE_NAME] and region [$AWS_REGION]"
    ( # subshell to hide commands
    configure_aws_access \
    "${AWS_PROFILE_NAME}" "${AWS_REGION}" "${AWS_KEY}" "${AWS_SECRET}" "${WORKDIR}" "${GOBIN}"
    ) || FATAL "Missing AWS access details. Did you set AWS variables in the 'subm_variables' file ?"

    # For GCP
    if [[ -s "$GCP_CRED_JSON" ]] ; then
      TITLE "Installing and configuring GCP CLI"
      configure_gcp_access "${GCP_CRED_JSON}" "${WORKDIR}" \
      || FATAL "Missing GCP access details. Did you set GCP_CRED_JSON path in the 'subm_variables' file ?"
    fi

    # For Azure
    if [[ -s "$AZURE_CRED_JSON" ]] ; then
      TITLE "Installing and configuring Azure CLI"
      configure_azure_access "${AZURE_SUBSCRIPTION}" "${AZURE_CRED_JSON}" "${WORKDIR}" \
      || FATAL "Missing Azure access details. Did you set AZURE variables in the 'subm_variables' file ?"
    fi
  fi

}

# ------------------------------------------

function set_trap_functions() {
  PROMPT "Configuring trap functions on script exit"

  TITLE "Will run env_teardown() when exiting the script"
  trap 'env_teardown' EXIT

  if [[ "$PRINT_LOGS" =~ ^(y|yes)$ ]]; then
    TITLE "Will collect Submariner information on test failure (CLI option --print-logs)"
    trap_function_on_error "gather_submariner_info" "$TEST_STATUS_FILE"
  fi

}

# ------------------------------------------

function download_ocp_installer() {
### Download OCP installer ###
  trap_to_debug_commands;

  # Target OCP version to be installed (could also be "latest")
  local ocp_installer_version="$1"

  # Target OCP directory, where to keep the installation files
  local ocp_install_dir="$2"
  
  PROMPT "Downloading OCP Installer version $ocp_installer_version"

  local ocp_major_version
  ocp_major_version="$(echo "$ocp_installer_version" | cut -s -d '.' -f 1)" # Get the major digit of OCP version

  # If no numerical version was requested (e.g. "latest"), the default OCP major version is 4
  local ocp_major_version="${ocp_major_version:-4}"
  local oc_version_path="ocp/${ocp_installer_version}"
  local oc_major_url="https://mirror.openshift.com/pub/openshift-v${ocp_major_version}/clients"

  # Get latest "ocp-dev-preview Engineering Candidate" build, if the OCP version requested was "ec" or "nightly".
  # These builds are also available at: 
  # https://openshift-release-artifacts.svc.ci.openshift.org/ 
  # https://amd64.ocp.releases.ci.openshift.org/
  if [[ "$ocp_installer_version" =~ ^(ec|nightly)$  ]] ; then
    oc_version_path="ocp-dev-preview/"
    
    # Append latest EC version, for example: 4.12.0-ec.5
    oc_version_path+="$(curl "${oc_major_url}/${oc_version_path}" | \
    grep -Eoh ".*-ec\..*" | awk '{ gsub(/>|</, " " ); print $3; }' | tail -1)"
  fi

  local oc_installer_url="${oc_major_url}/${oc_version_path}/"

  cd "${WORKDIR}" || return 1

  local ocp_install_gz
  ocp_install_gz=$(curl "$oc_installer_url" | grep -Eoh "openshift-install-linux-.+\.tar\.gz" | cut -d '"' -f 1 | head -1)

  local oc_client_gz
  oc_client_gz=$(curl "$oc_installer_url" | grep -Eoh "openshift-client-linux-.+\.tar\.gz" | cut -d '"' -f 1 | head -1)

  [[ -n "$ocp_install_gz" && -n "$oc_client_gz" ]] || FATAL "Failed to retrieve OCP installer [${ocp_installer_version}] from $oc_installer_url"

  TITLE "Deleting previous OCP installer and client, and downloading: \n# $ocp_install_gz \n# $oc_client_gz"
  # find -type f -maxdepth 1 -name "openshift-*.tar.gz" -mtime +1 -exec rm -rf {} \;
  delete_old_files_or_dirs "openshift-*.tar.gz"

  download_file "${oc_installer_url}${ocp_install_gz}"
  download_file "${oc_installer_url}${oc_client_gz}"

  TITLE "Extracting OCP installer and client into ${ocp_install_dir} : \n $ocp_install_gz \n $oc_client_gz"

  ocp_install_gz="${ocp_install_gz%%$'\n'*}"
  oc_client_gz="${oc_client_gz%%$'\n'*}"

  # Create the OCP cluster directory and extract installation files
  echo -e "\n# Get actual OCP version from '$ocp_install_gz', and create installer directory for this version:"
  ocp_installer_version="$(find -name "$ocp_install_gz" | grep -Eo '([0-9]+)(\.?[0-9]+)*' | head -1 )"

  # ocp_install_dir="${WORKDIR}/${ocp_installer_version}" # Using cluster directory instead of WORKDIR
  mkdir -p "${ocp_install_dir}"
  echo "${ocp_installer_version}" > "${ocp_install_dir}/${OCP_VERSION_FILE}"

  tar -xvf "$ocp_install_gz" -C "${ocp_install_dir}"
  tar -xvf "$oc_client_gz" -C "${ocp_install_dir}"

  TITLE "Install OC (Openshift Client tool) version ${ocp_installer_version} into ${GOBIN}:"
  mkdir -p "$GOBIN"
  /usr/bin/install "${ocp_install_dir}/oc" "$GOBIN/oc" || :

  echo -e "\n# Install OC into user HOME bin:"
  /usr/bin/install "${ocp_install_dir}/oc" ~/.local/bin/oc

  echo -e "\n# Add user HOME bin to system PATH:"
  export PATH="$HOME/.local/bin:$PATH"

  ${OC} -h
}

# ------------------------------------------

function destroy_cluster() {
### Destroy your previous OCP cluster ###
  trap_to_debug_commands;
  local ocp_install_dir="$1"
  local cluster_name="$2"

  PROMPT "Destroying previous OCP cluster: $cluster_name"

  cd "${WORKDIR}" || return 1

  # Only if your OCP cluster still exists - run destroy command:
  echo -e "\n# TODO: should first check if it was not already purged, because it can save a lot of time."
  if [[ -d "${ocp_install_dir}" ]] ; then

    echo -e "\n# Previous OCP Installation found: ${ocp_install_dir}"
    
    if [[ -f "${ocp_install_dir}/metadata.json" ]] ; then

      echo -e "\n# Verifying 'openshift-install' command to destroy cluster $cluster_name"
      local oc_install_cmd
      oc_install_cmd="${ocp_install_dir}/openshift-install"
      
      [[ -f "${oc_install_cmd}" ]] || oc_install_cmd="${WORKDIR}/openshift-install"
      [[ -f "${oc_install_cmd}" ]] \
      || FATAL "OCP Installer is missing. Try to run again with option '--reset-cluster-a/b/c [latest / x.y.z]'"

      TITLE "Destroying OCP cluster ${cluster_name}:"

      timeout 10m ./openshift-install destroy cluster --log-level debug --dir "${ocp_install_dir}" || \
      ( [[ $? -eq 124 ]] && \
        BUG "WARNING: OCP destroy timeout exceeded - loop state while destroying cluster" \
        "Force exist OCP destroy process" \
        "Please submit a new bug for OCP installer (in Bugzilla)"
      ) || :
    fi

    TITLE "Backup previous OCP install-config directory of cluster ${cluster_name}"
    parent_dir=$(dirname -- "$ocp_install_dir")
    base_dir=$(basename -- "$ocp_install_dir")
    backup_and_remove_dir "$ocp_install_dir" "${parent_dir}/_${base_dir}_${DATE_TIME}"

    # Remove existing OCP install-config directory:
    #rm -r "_${ocp_install_dir}/" || echo -e "\n# Old config dir removed."
    TITLE "Deleting all previous ${ocp_install_dir} config directories (older than 1 day):"
    # find -maxdepth 1 -type d -name "_*" -mtime +1 -exec rm -rf {} \;
    delete_old_files_or_dirs "${parent_dir}/_${base_dir}_*" "d" 1
  else
    TITLE "OCP cluster config (metadata.json) was not found in ${ocp_install_dir}. Skipping cluster Destroy."
  fi

  BUG "WARNING: OCP destroy command does not remove the previous DNS record sets from AWS Route53" \
  "Delete previous DNS record sets from AWS Route53" \
  "---"
  # Workaround:

  # set AWS DNS record sets to be deleted
  AWS_DNS_ALIAS1="api.${cluster_name}.${AWS_ZONE_NAME}."
  AWS_DNS_ALIAS2="\052.apps.${cluster_name}.${AWS_ZONE_NAME}."

  TITLE "Deleting AWS DNS record sets from Route53:
  $AWS_DNS_ALIAS1
  $AWS_DNS_ALIAS2
  "

  # curl -LO https://github.com/manosnoam/shift-stack-helpers/raw/master/delete_aws_dns_alias_zones.sh
  # chmod +x delete_aws_dns_alias_zones.sh
  # ./delete_aws_dns_alias_zones.sh "${cluster_name}"
  delete_aws_dns_records "$AWS_ZONE_ID" "$AWS_DNS_ALIAS1" || :
  delete_aws_dns_records "$AWS_ZONE_ID" "$AWS_DNS_ALIAS2" || :

  # Or Manually in https://console.aws.amazon.com/route53/home#hosted-zones:
    # https://console.aws.amazon.com/route53/home#resource-record-sets
    #
    # api.user-cluster-a.devcluster.openshift.com.
    # *.apps.user-cluster-a.devcluster.openshift.com.
    #
    # DO NOT REMOVE OTHER DNSs !!!!
}

# ------------------------------------------

function prepare_ocp_install() {
### Prepare installation files for OCP cluster (according to install-config file) ###
  trap_to_debug_commands;

  local ocp_install_dir="$1"
  local installer_yaml_source="$2"
  local cluster_name="$3"
  # local opt_cred_file="$4"

  PROMPT "Preparing installation files for OCP cluster $cluster_name"

  # cd "${WORKDIR}" || return 1
  cd "${ocp_install_dir}" || return 1
  [[ -f openshift-install ]] || FATAL "OCP Installer is missing. Try to run again with option '--reset-cluster-a/b/c [latest / x.y.z]'"

  # To manually create new OCP install-config.yaml:
  # ./openshift-install create install-config --dir user-cluster-a
  #
  # $ cluster_name=user-cluster-a
  # $ mkdir ${cluster_name}
  # $ cd ${cluster_name}
  # $ ./openshift-install create install-config

    # ? SSH Public Key ~/.ssh/id_rsa.pub
    # ? Platform aws
    # ? Region us-east-1
    # ? Base Domain devcluster.openshift.com
    # ? cluster Name user-cluster-a
    # ? Pull Secret

  mkdir -p "${ocp_install_dir}"
  local installer_yaml_new="${ocp_install_dir}/install-config.yaml"

  echo -e "\n# Using OCP install-config.yaml - make sure to have it in the workspace: ${installer_yaml_source}"
  cp -f "${installer_yaml_source}" "$installer_yaml_new"
  chmod 777 "$installer_yaml_new"

  echo -e "\n# Update cluster name in the new installer yaml: ${installer_yaml_new}"
  [[ -z "$cluster_name" ]] || change_yaml_key_value "$installer_yaml_new" "name" "$cluster_name" "metadata"

  echo -e "\n# Get cluster platform type from OCP installer yaml, and update cluster region"
  local platform
  platform="$(print_ocp_install_platform "${installer_yaml_new}")"

  # Specific requirements for OCP installer on AWS:
  if [[ -n "$AWS_REGION" && "$platform" == aws ]] ; then
    echo -e "\n# Set region for AWS cluster to: $AWS_REGION"
    change_yaml_key_value "$installer_yaml_new" "region" "$AWS_REGION" "aws"

  # Specific requirements for OCP installer on GCP:
  elif [[ -n "$GCP_REGION" && "$platform" == gcp ]] ; then
    echo -e "\n# Set region for GCP cluster to: $GCP_REGION"
    change_yaml_key_value "$installer_yaml_new" "region" "$GCP_REGION" "gcp"

  # Specific requirements for OCP installer on OSP:
  elif [[ "$platform" == osp && -s "$OSP_CRED_YAML" ]] ; then
    local cred_file_new="${ocp_install_dir}/clouds.yaml"

    echo -e "\n# Using clouds.yaml for OSP cluster"
    cp -f "$OSP_CRED_YAML" "$cred_file_new"
    chmod 777 "$cred_file_new"
  fi

  TITLE "OCP installer for platform '$platform' is ready to install $cluster_name"

}

# ------------------------------------------

function create_cluster() {
### Create OCP cluster with OCP installer ###
  trap_to_debug_commands;

  local ocp_install_dir="$1"
  local cluster_name="$2"

  # Get the OCP version to be installed, as created during download_ocp_installer()
  local ocp_target_version
  ocp_target_version="$(< "${ocp_install_dir}/${OCP_VERSION_FILE}")" || :
  
  PROMPT "Creating OCP cluster version [${ocp_target_version}]: $cluster_name"

  if [[ -z "$ocp_target_version" ]] ; then
    FATAL "OCP install directory [$ocp_install_dir] does not exist or missing installation files"
  else
    cd "${ocp_install_dir}" || return 1
  fi

  # Continue previous OCP installation
  if [[ -s "metadata.json" ]] ; then
    TITLE "$ocp_install_dir directory contains previous OCP installer files. Will attempt to continue previous installation"

    local ocp_cmd="./openshift-install wait-for install-complete --log-level=debug" # --dir="$CLUSTER_DIR"
    local ocp_log=".openshift_install.log"

    run_and_tail "$ocp_cmd" "$ocp_log" 100m "Access the OpenShift web-console" \
    || FATAL "OCP installer failed to continue previous installation for $cluster_name"

  # Start new OCP installation
  else
    TITLE "Run OCP installer from scratch using install-config.yaml"

    local ocp_cmd="./openshift-install create cluster --log-level debug"
    local ocp_log=".openshift_install.log"

    run_and_tail "$ocp_cmd" "$ocp_log" 100m "Access the OpenShift web-console" \
    || FATAL "OCP installer failed to create $cluster_name"
  fi

}

# ------------------------------------------

function verify_yaml_for_cluster_deploy() {
### Helper function to check if cluster is required for testing

  local cluster_index="$1" # A / B / C

  TITLE "Checking if Cluster ${cluster_index} is required to be deployed, and verifying its OCP installer yaml"

  declare -n req_reset="RESET_CLUSTER_${cluster_index}"
  declare -n req_create="CREATE_CLUSTER_${cluster_index}"
  declare -n req_destroy="DESTROY_CLUSTER_${cluster_index}"
  declare -n req_clean="CLEAN_CLUSTER_${cluster_index}"
  declare -n req_join="JOIN_CLUSTER_${cluster_index}"
  declare -n cluster_name="CLUSTER_${cluster_index}_NAME"
  declare -n cluster_yaml="CLUSTER_${cluster_index}_YAML"

  if echo "$req_reset $req_create $req_destroy $req_clean $req_join" | grep -w -iP "yes|y" ; then
    echo -e "\n# Cluster '${cluster_name}' is required to be deployed, verifying installer yaml:
    ${cluster_yaml}"

    grep 'Domain:' "$cluster_yaml" || FATAL "Cluster $cluster_index installer yaml is missing or corrupted"

  else
    echo -e "\n# Cluster '${cluster_name}' was not required to be deployed"
  fi

}

# ------------------------------------------

function print_ocp_install_platform() {
  # Print the platform type found in the OCP install yaml, as a 3 letters abbreviation
  trap '' DEBUG # DONT trap_to_debug_commands

  local ocp_install_yaml="$1"
  local platform_name

  platform_name="$(grep -Poz 'platform:\s*(name:\s*)?\K\w+' "$ocp_install_yaml" | awk -F'\0' '{print $1; exit}')" 2>/dev/null || :

  if [[ "$platform_name" == openstack ]] ; then
    # If it's "openstack", it will print "osp" instead
    echo "osp"
  elif [[ "$platform_name" == azure ]] ; then
    # If it's "azure", it will print "osp" instead
    echo "azr"
  else
    # For all the other platforms print just the first 3 letters
    echo "${platform_name:0:3}"
  fi

}

# ------------------------------------------

function check_if_cluster_is_active() {
  # Check if cluster status is active (return exit code 1 if it's not)
  trap_to_debug_commands;

  local kubeconfig_file="$1"

  if [[ -s "${kubeconfig_file}" ]] && \
  ${OC} status --kubeconfig "${kubeconfig_file}" ; then
    return 0
  else
    echo -e "\n# '$kubeconfig_file' does not point to an active cluster"
    return 1
  fi

}

# ------------------------------------------

function update_kubeconfig_default_context() {
  # Backup kubeconfig of a cluster, and update its context name
  trap_to_debug_commands;

  local kubeconfig_file="$1"
  local cluster_name

  if [[ -s "$kubeconfig_file" ]] ; then
    export KUBECONFIG="$kubeconfig_file"
    cluster_name="${2:-$(print_current_cluster_name || :)}"
  else
    cluster_name="Unknown"
  fi

  PROMPT "Updating kubeconfig to the default context on cluster ${cluster_name}"

  [[ -s ${kubeconfig_file} ]] || FATAL "Openshift deployment configuration for cluster '$cluster_name' is missing: ${kubeconfig_file}"

  # Backup/restore KUBECONFIG to/from: ${kubeconfig_file}.bak
  if [[ -s ${kubeconfig_file}.bak ]] ; then
    echo -e "\n# Restore the original KUBECONFIG from: ${kubeconfig_file}.bak"
    cp -f "${kubeconfig_file}.bak" "${kubeconfig_file}"
  else
    echo -e "\n# Backup the new KUBECONFIG into: ${kubeconfig_file}.bak"
    cp -f "${kubeconfig_file}" "${kubeconfig_file}.bak"
  fi

  local admin_user="admin"
  TITLE "Set current context of cluster '$cluster_name' to the first context with '${admin_user}' user"

  local master_context
  master_context=$(${OC} config view -o json | jq -r "[.contexts[] | select(.context.user | test(\"${admin_user}\")).name][0] // empty")

  if [[ -z "$master_context" ]] ; then
    admin_user="master"
    echo -e "\n# Warning: Admin user not found, looking for context of '${admin_user}' user instead"
    master_context=$(${OC} config view -o json | jq -r "[.contexts[] | select(.context.user | test(\"${admin_user}\")).name][0] // empty")
  fi

  echo -e "\n# Switch to the cluster of the '$admin_user' user"
  ${OC} config use-context "$master_context"

  local cur_context
  cur_context="$(${OC} config current-context)"

  local cur_username
  cur_username="$(${OC} config view -o jsonpath="{.contexts[?(@.name == '${cur_context}')].context.user}")"

  local cur_cluster
  cur_cluster="$(${OC} config view -o jsonpath="{.contexts[?(@.name == '${cur_context}')].context.cluster}")"

  BUG "If using managed cluster-id with long name or special characters (from current kubeconfig context),
  Submariner Gateway resource will not be created using Submariner Addon" \
  "Replace all special characters in kubeconfig current context before running subctl deploy" \
  "https://bugzilla.redhat.com/show_bug.cgi?id=2036325"
  # Workaround:
  # Replace anything but letters and numbers with "-"
  local renamed_context="${cluster_name//[^a-zA-Z0-9]/-}"

  if [[ "$cur_context" != "${renamed_context}" ]] ; then

    TITLE "Renaming kubeconfig current-context to '$renamed_context'"

    BUG "E2E will fail if clusters have same name (default is \"admin\")" \
    "Modify KUBECONFIG cluster context name on both clusters to be unique" \
    "https://github.com/submariner-io/submariner/issues/245"

    if ${OC} config get-contexts -o name | grep "${renamed_context}" 2>/dev/null ; then
      echo -e "\n# Rename existing kubeconfig context '${renamed_context}' to: ${renamed_context}_old"
      ${OC} config delete-context "${renamed_context}_old" || :
      ${OC} config rename-context "${renamed_context}" "${renamed_context}_old" || :
    fi

    ${OC} config rename-context "${cur_context}" "${renamed_context}" || :
  fi

  TITLE "Updating kubeconfig current context '$renamed_context' to use:
  Cluster name: $renamed_context
  Auth user: $cur_username
  Namespace: default
  "

  ${OC} config set-context "$renamed_context" --cluster "$renamed_context" --user "${cur_username}" --namespace "default"
  ${OC} config use-context "$renamed_context"

  echo -e "\n# Rename cluster '${cur_cluster}' to '${renamed_context}'"
  sed -E "s/name: ${cur_cluster}\$/name: ${renamed_context}/" -i "${kubeconfig_file}"

  ${OC} config get-contexts

  # ${OC} set env dc/dcname TZ=Asia/Jerusalem

}

# ------------------------------------------

function test_cluster_status() {
  # Verify that OCP cluster is up and healthy
  trap_to_debug_commands;

  local kubeconfig_file="$1"
  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="${2:-$(print_current_cluster_name || :)}"

  PROMPT "Testing status of cluster $cluster_name"

  [[ -f ${kubeconfig_file} ]] || FATAL "Openshift deployment configuration for '$cluster_name' is missing: ${kubeconfig_file}"

  # Get OCP cluster version
  local cluster_version
  cluster_version="$(${OC} version | awk '/Server Version/ { print $3 }' | grep .)" || FATAL "OCP cluster is inaccessible"

  TITLE "Backup Kubeconfig of OCP cluster $cluster_name ${cluster_version:+(OCP Version $cluster_version)}"

  local kubeconfig_copy="${OUTPUT_DIR}/kubconf_${cluster_name}"
  echo -e "\n# Copy '${kubeconfig_file}' to current workspace: ${kubeconfig_copy}"
  cp -f "$kubeconfig_file" "${kubeconfig_copy}" || :

  ${OC} config view
  
  verify_all_nodes_ready_current_cluster

  ${OC} status -n default || FATAL "Openshift cluster is not installed, or using wrong context for '$cluster_name' in kubeconfig: ${kubeconfig_file}"
  ${OC} version
  ${OC} get all -n default
    # NAME                 TYPE           CLUSTER-IP   EXTERNAL-IP                            PORT(S)   AGE
    # service/kubernetes   clusterIP      172.30.0.1   <none>                                 443/TCP   39m
    # service/openshift    ExternalName   <none>       kubernetes.default.svc.cluster.local   <none>    32m
  
}

# ------------------------------------------

function add_elevated_user() {
  # Add new elevated user to OCP cluster
  # Refs: 
  # https://docs.openshift.com/container-platform/latest/authentication/identity_providers/configuring-htpasswd-identity-provider.html
  # https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/

  trap_to_debug_commands;

  local kubeconfig_file="$1"
  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="${2:-$(print_current_cluster_name || :)}"

  PROMPT "Adding elevated user with certificate to cluster ${cluster_name}"

  TITLE "Creating Certificate data '${cluster_name}.csr' and private key for OCP user '$OCP_USR'"

  ${OC} create user ${OCP_USR} || :

  cat > csr.conf <<EOF
  [req]
  req_extensions = v3_req
  distinguished_name = req_distinguished_name
  prompt = no
  [req_distinguished_name]
  CN = system:node:${OCP_USR}
  O = system:nodes
  [ v3_req ]
  basicConstraints = CA:FALSE
  keyUsage = nonRepudiation, digitalSignature, keyEncipherment
  extendedKeyUsage = clientAuth, serverAuth
  subjectAltName = @alt_names
  [alt_names]
  IP = 127.0.0.1
  DNS = localhost
EOF

  openssl req -new -newkey rsa:4096 -nodes -keyout "${cluster_name}.key" -config csr.conf \
  -out "${cluster_name}.csr" -subj "/CN=system:admin" # "/CN=system:node:${OCP_USR};/O=system:nodes"
 
  TITLE "Verify the SSL key '${cluster_name}.key' and CSR data file '${cluster_name}.csr'"

  openssl rsa -in "${cluster_name}.key" -check

  openssl req -text -noout -verify -in "${cluster_name}.csr"

  TITLE "Approving the Certificate Signing Request as '${cluster_name}-access'"

  ${OC} delete csr "${cluster_name}-access" --ignore-not-found

  cat <<EOF | ${OC} create -f -
  apiVersion: certificates.k8s.io/v1
  kind: CertificateSigningRequest
  metadata:
    name: ${cluster_name}-access
  spec:
    groups:
    - system:authenticated
    request: $(base64 "${cluster_name}.csr" | tr -d '\n')
    signerName: kubernetes.io/kube-apiserver-client
    usages:
    - digital signature
    - key encipherment
    - client auth
EOF
    # signerName: kubernetes.io/kubelet-serving
    # - server auth

  ${OC} adm certificate approve "${cluster_name}-access"

  ${OC} wait --timeout=1m --for=condition=approved csr "${cluster_name}-access"

  ${OC} get csr |& highlight 'Approved,Issued'

  local crt_file
  crt_file="${kubeconfig_file}.crt"

  TITLE "Download and validate the new certificate '${crt_file}'"

  # This doesn't always work:
  # ${OC} get csr "${cluster_name}-access" -o jsonpath='{.status.certificate}' | base64 -d > "${crt_file}"

  # Using current SA certificate
  local auth_pod
  auth_pod="$(${OC} get pods -n openshift-authentication --field-selector=status.phase==Running -o name | head -1)"
  ${OC} rsh -n openshift-authentication "${auth_pod}" cat /run/secrets/kubernetes.io/serviceaccount/ca.crt > "${crt_file}"

  openssl x509 -in "${crt_file}" -text -noout

  TITLE "Updating '${OCP_USR}' user credentials and certificate into the kubeconfig"

  ${OC} config set-credentials "${OCP_USR}" --client-certificate="${crt_file}" \
  --client-key="${cluster_name}.key" --embed-certs --kubeconfig="${kubeconfig_file}"

  ${OC} config set-credentials system:admin --client-certificate="${crt_file}" \
  --client-key="${cluster_name}.key" --embed-certs --kubeconfig="${kubeconfig_file}"

  TITLE "Create HTPasswd for OCP user '$OCP_USR'"

  local http_sec_name="http.sec"

  # Update ${OCP_USR}.sec and http.sec - Only if these files are empty or modified more than 1 day ago
  touch -a "${WORKDIR}/${http_sec_name}"
  touch -a "${WORKDIR}/${OCP_USR}.sec"
  if find "${WORKDIR}/${http_sec_name}" "${WORKDIR}/${OCP_USR}.sec" \( -mtime +1 -o -empty \) | grep . ; then
    echo -e "\n# Create random secret for ${OCP_USR}.sec (since secret files are empty or older than 1 day)"
    ( # subshell to hide commands
      openssl rand -base64 12 > "${WORKDIR}/${OCP_USR}.sec"
      local ocp_pwd
      ocp_pwd="$(< "${WORKDIR}/${OCP_USR}.sec")"
      printf "%s:%s\n" "${OCP_USR}" "$(openssl passwd -apr1 "${ocp_pwd}")" > "${WORKDIR}/${http_sec_name}"
    )
  fi

  TITLE "Add the HTPasswd identity provider to cluster ${cluster_name}"

  ${OC} delete secret $http_sec_name -n openshift-config --ignore-not-found || :

  ${OC} create secret generic ${http_sec_name} --from-file=htpasswd="${WORKDIR}/${http_sec_name}" -n openshift-config

  local cluster_auth="${WORKDIR}/cluster_auth.yaml"

  cat <<-EOF > "$cluster_auth"
  apiVersion: config.openshift.io/v1
  kind: OAuth
  metadata:
   name: cluster
  spec:
   identityProviders:
   - name: htpasswd_provider
     mappingMethod: claim
     type: HTPasswd
     htpasswd:
       fileData:
         name: ${http_sec_name}
EOF

  ${OC} apply set-last-applied --create-annotation=true -f "$cluster_auth" || :

  ${OC} apply -f "$cluster_auth"

  ${OC} describe oauth.config.openshift.io/cluster

  TITLE "Adding admin roles to user '${OCP_USR}'"

  ${OC} create clusterrolebinding master-cluster-admin-binding --clusterrole=cluster-admin --user="${OCP_USR}" || :
  ${OC} adm policy add-cluster-role-to-user cluster-admin "${OCP_USR}" --rolebinding-name cluster-admin
  ${OC} adm policy add-scc-to-user privileged "${OCP_USR}"
  
  ${OC} describe clusterrolebindings system:"${OCP_USR}"

  ${OC} describe clusterrole system:"${OCP_USR}"

  local cmd="${OC} get clusterrolebindings --no-headers -o custom-columns='USER:subjects[].name'"
  watch_and_retry "$cmd | grep 'system:${OCP_USR}\$'" 5m || BUG "WARNING: User \"${OCP_USR}\" may not be cluster admin"

  ${OC} get clusterrolebindings

  ocp_login "${OCP_USR}" "$(< "${WORKDIR}/${OCP_USR}.sec")"

  TITLE "Updating kubeconfig Certificate Authority"

  ${OC} get pods -n openshift-authentication
  
  ${OC} config set-cluster "$(${OC} config view -o jsonpath='{.clusters[0].name}')" \
  --certificate-authority="${crt_file}" --kubeconfig="${kubeconfig_file}" --embed-certs \
  --server="$(oc config view -o jsonpath='{.clusters[0].cluster.server}')"
  # --certificate-authority=ingress-ca.crt --kubeconfig="${kubeconfig_file}" --embed-certs

  local cur_context
  cur_context="$(${OC} config current-context)"
  TITLE "Kubeconfig current-context is: $cur_context"

}

# ------------------------------------------

function uninstall_submariner() {
### Uninstall Submariner from the given cluster
  trap_to_debug_commands;

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Uninstalling previous Submariner (Namespaces, OLM, CRDs, Cluster Roles, ServiceExports) from cluster $cluster_name"

  local cleanup_required=NO

  # Since Submariner 0.12 there's a subctl uninstall command
  if check_version_greater_or_equal "${SUBM_VER_TAG}" "0.12" ; then
    subctl uninstall --yes || {

      BUG "subctl uninstall failed deleting resource 'service-discovery'" \
      "Run cleanup the old way" \
      "https://bugzilla.redhat.com/show_bug.cgi?id=2073812"

      cleanup_required=YES
    }

    # TODO: Need to add cloud cleanup, according to cloud provider:
    # subctl cloud cleanup rhos --infra-id osp-nmanos-b2-xyz --region region_name --project-id project_id

  else
    cleanup_required=YES
  fi

  export KUBECONFIG="$kubeconfig_file"

  if [[ "$cleanup_required" == YES ]] ; then

    delete_submariner_namespace_and_crds || :

    delete_submariner_cluster_roles || :

    delete_lighthouse_dns_list || :

    remove_submariner_gateway_labels || :

    remove_submariner_machine_sets || :
  fi

  delete_submariner_test_namespaces || :

  BUG "Low disk-space on OCP cluster that was running for few weeks" \
  "Delete old E2E namespaces" \
  "https://github.com/submariner-io/submariner-website/issues/341
  https://github.com/submariner-io/shipyard/issues/355"

  delete_e2e_namespaces || :

}

# ------------------------------------------

function delete_submariner_namespace_and_crds() {
### Run cleanup of previous Submariner namespace and CRDs ###
  trap_to_debug_commands;

  # force_delete_namespace "${SUBM_NAMESPACE}"

  delete_crds_by_name 'submariner\.io'

  ${OC} delete namespace "${SUBM_NAMESPACE}" --wait || :
  ${OC} wait --for=delete namespace "${SUBM_NAMESPACE}" || :

  # Required if Broker cluster is not a Dataplane cluster as well:
  local broker_namespace
  broker_namespace="$(get_broker_namespace || :)"
  [[ -z "${broker_namespace}" ]] || force_delete_namespace "${broker_namespace}"

}

# ------------------------------------------

function get_broker_namespace() {
### Get correct Broker namespace from the Managed cluster set ###
  # Do not trap_to_debug_commands since using output as return value

  ${OC} get ManagedClusterSet "${ACM_CLUSTER_SET}" \
  -o jsonpath="{.metadata.annotations['cluster\.open-cluster-management\.io/submariner-broker-ns']}" || :

}

# ------------------------------------------

function delete_submariner_cluster_roles() {
### Run cleanup of previous Submariner ClusterRoles and ClusterRoleBindings ###
  trap_to_debug_commands;

  TITLE "Deleting Submariner ClusterRoles and ClusterRoleBindings"

  local roles="submariner-operator submariner-operator-globalnet submariner-lighthouse submariner-networkplugin-syncer"

  ${OC} delete clusterrole,clusterrolebinding "$roles" --ignore-not-found || :

}

# ------------------------------------------

function delete_lighthouse_dns_list() {
### Run cleanup of previous Lighthouse ServiceExport DNS list ###
  trap_to_debug_commands;

  TITLE "Clean Lighthouse ServiceExport DNS list:"

  ${OC} apply -f - <<EOF
  apiVersion: operator.openshift.io/v1
  kind: DNS
  metadata:
    finalizers:
    - dns.operator.openshift.io/dns-controller
    name: default
  spec:
    servers: []
EOF

}

# ------------------------------------------

function remove_submariner_gateway_labels() {
  trap_to_debug_commands;

  TITLE "Remove previous submariner gateway labels from all node in the cluster:"

  ${OC} label --all node submariner.io/gateway- || :
}

# ------------------------------------------

function remove_submariner_machine_sets() {
  trap_to_debug_commands;

  TITLE "Remove previous machineset (if it has a template with submariner gateway label)"

  local subm_machineset
  subm_machineset="$(${OC} get machineset -A -o jsonpath='{.items[?(@.spec.template.spec.metadata.labels.submariner\.io\gateway=="true")].metadata.name}')"
  local ns
  ns="$(${OC} get machineset -A -o jsonpath='{.items[?(@.spec.template.spec.metadata.labels.submariner\.io\gateway=="true")].metadata.namespace}')"

  if [[ -n "$subm_machineset" && -n "$ns" ]] ; then
    ${OC} delete machineset "$subm_machineset" -n "$ns" || :
  fi

  ${OC} get machineset -A -o wide || :
}

# ------------------------------------------

function delete_submariner_test_namespaces() {
### Delete previous Submariner test namespaces from current cluster ###
  trap_to_debug_commands;

  TITLE "Deleting Submariner test namespaces: '$TEST_NS' '$HEADLESS_TEST_NS'"

  for ns in "$TEST_NS" "$HEADLESS_TEST_NS" ; do
    if [[ -n "$ns" ]]; then
      force_delete_namespace "$ns"
      # create_namespace "$ns"
    fi
  done

  TITLE "Unset Submariner test namespaces from kubeconfig current context"
  local cur_context
  cur_context="$(${OC} config current-context)"
  ${OC} config unset "contexts.${cur_context}.namespace"

}

# ------------------------------------------

function delete_e2e_namespaces() {
### Delete previous Submariner E2E namespaces from current cluster ###
  trap_to_debug_commands;

  local e2e_namespaces
  e2e_namespaces="$(${OC} get ns -o=custom-columns=NAME:.metadata.name | grep e2e-tests | cat )"

  if [[ -n "$e2e_namespaces" ]] ; then
    TITLE "Deleting all 'e2e-tests' namespaces: $e2e_namespaces"
    # ${OC} delete --timeout=30s ns $e2e_namespaces

    for ns_name in $e2e_namespaces ; do
      force_delete_namespace "$ns_name"
    done
  else
    echo "No 'e2e-tests' namespaces exist to be deleted"
  fi

}

# ------------------------------------------

# function remove_submariner_images_from_local_registry_with_podman() {
#   trap_to_debug_commands;
#
#   PROMPT "Remove previous Submariner images from local Podman registry"
#
#   [[ -x "$(command -v subctl)" ]] || FATAL "No SubCtl installation found. Try to run again with option '--subctl-version'"
#   # Get SubCtl version (from file $SUBCTL_VERSION_FILE)
#
#   # install_local_podman "${WORKDIR}"
#
#   local VERSION="$(subctl version | awk '{print $3}')"
#
#   for img in \
#     $SUBM_IMG_GATEWAY \
#     $SUBM_IMG_ROUTE \
#     $SUBM_IMG_NETWORK \
#     $SUBM_IMG_LIGHTHOUSE \
#     $SUBM_IMG_COREDNS \
#     $SUBM_IMG_GLOBALNET \
#     $SUBM_IMG_OPERATOR \
#     $SUBM_IMG_BUNDLE \
#     ; do
#       echo -e "\n# Removing Submariner image from local Podman registry: $SUBM_SNAPSHOT_REGISTRY/$SUBM_IMG_PREFIX-$img:$VERSION \n"
#
#       podman image rm -f $SUBM_SNAPSHOT_REGISTRY/$SUBM_IMG_PREFIX-$img:$VERSION # > /dev/null 2>&1
#       podman pull $SUBM_SNAPSHOT_REGISTRY/$SUBM_IMG_PREFIX-$img:$VERSION
#       podman image inspect $SUBM_SNAPSHOT_REGISTRY/$SUBM_IMG_PREFIX-$img:$VERSION # | jq '.[0].Config.Labels'
#   done
# }

# ------------------------------------------

function delete_all_evicted_pods_in_cluster() {
### Delete all evicted (failed) pods from all namespaces in the given cluster
  trap_to_debug_commands

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Delete all evicted pods in cluster $cluster_name"

  delete_all_evicted_pods_in_current_cluster || :

  verify_all_nodes_ready_current_cluster

}

# ------------------------------------------

function delete_old_submariner_images_from_cluster() {
### Delete old Submariner images from the given cluster
  trap_to_debug_commands

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Delete previous Submariner images in cluster $cluster_name"

  for node in $(${OC} get nodes -o name) ; do
    TITLE "Delete Submariner images in $node"
    ${OC} debug "$node" -n default -- chroot /host /bin/bash -c \
    "crictl images | awk '\$1 ~ /submariner|lighthouse/ {print \$3}' | xargs -n1 crictl rmi" || :
  done

  # # Delete images
  # ${OC} get images | grep "${BREW_REGISTRY}" | while read -r line ; do
  #   set -- $(echo $line | awk '{ print $1, $2 }')
  #   local img_sha="$1"
  #   local img_name="$2"
  #
  #   echo -e "\n# Deleting registry image: $(echo $img_name | sed -r 's|.*/([^@]+).*|\1|')"
  #   ${OC} delete image $img_sha --ignore-not-found
  # done
  #
  # # Delete image-stream tags
  # ${OC} get istag -n ${SUBM_NAMESPACE} | awk '{print $1}' | while read -r img_tag ; do
  #   echo -e "\n# Deleting image stream tag: $img_tag"
  #   ${OC} delete istag $img_tag -n ${SUBM_NAMESPACE} --ignore-not-found
  # done

  # # Delete image-stream
  # for img_stream in \
  #   $SUBM_IMG_GATEWAY \
  #   $SUBM_IMG_ROUTE \
  #   $SUBM_IMG_NETWORK \
  #   $SUBM_IMG_LIGHTHOUSE \
  #   $SUBM_IMG_COREDNS \
  #   $SUBM_IMG_GLOBALNET \
  #   $SUBM_IMG_OPERATOR \
  #   $SUBM_IMG_BUNDLE \
  #   ; do
  #   TITLE "Deleting image stream: $img_stream"
  #   ${OC} delete imagestream "${img_stream}" -n "${SUBM_NAMESPACE}" --ignore-not-found || :
  #   # ${OC} tag -d submariner-operator/${img_stream}
  # done

  delete_submariner_image_streams_and_tags_from_current_cluster || :

  prune_all_mirror_images_from_current_cluster_registry || :

}

# ------------------------------------------

function delete_submariner_image_streams_and_tags_from_current_cluster() {
### Delete old image streams and tags of Submariner from the current cluster
  trap_to_debug_commands

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  TITLE "Delete Submariner image streams and tags in cluster ${cluster_name}, namespace '${SUBM_NAMESPACE}'"

  ${OC} delete imagestream --all -n "${SUBM_NAMESPACE}" --wait || :
  ${OC} delete istag --all -n "${SUBM_NAMESPACE}" --wait || :

}

# ------------------------------------------

function prune_all_mirror_images_from_current_cluster_registry() {
# Use OC adm command to delete all local registry images that were imported from $BREW_REGISTRY
  trap_to_debug_commands

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  local registry_url="https://${BREW_REGISTRY}"

  TITLE "Prune all images associated with mirror registry (Brew): ${registry_url} in the registry of cluster ${cluster_name}"

  ocp_login "${OCP_USR}" "$(< "${WORKDIR}/${OCP_USR}.sec")"

  ${OC} adm prune images --registry-url "${registry_url}" --confirm \
  --request-timeout="1m" --insecure-skip-tls-verify --cluster="${cluster_name}"
  # --as="${OCP_USR}" --force-insecure --ignore-invalid-refs --keep-tag-revisions=0 --keep-younger-than=0

  ${OC} rollout restart deployment/image-registry -n openshift-image-registry

}

# ------------------------------------------

function configure_namespace_for_submariner_tests_on_cluster_a() {
  PROMPT "Configure namespace '${TEST_NS:-default}' for running tests on OCP cluster A (public)"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  configure_namespace_for_submariner_tests

}

# ------------------------------------------

function configure_namespace_for_submariner_tests_on_managed_cluster() {
  PROMPT "Configure namespace '${TEST_NS:-default}' for running tests on managed cluster"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_MANAGED}"

  configure_namespace_for_submariner_tests

}

# ------------------------------------------

function configure_namespace_for_submariner_tests() {
  trap_to_debug_commands;

  TITLE "Set the default namespace to '${TEST_NS}' (if TEST_NS parameter was set in variables file)"
  if [[ -n "$TEST_NS" ]] ; then
    TITLE "Create namespace for Submariner tests: ${TEST_NS}"
    create_namespace "${TEST_NS}" "privileged"
  else
    TITLE "Using the 'default' namespace for Submariner tests"
    export TEST_NS=default
  fi

  BUG "On OCP version < 4.4.6 : If running inside different cluster, OC can use wrong project name by default" \
  "Set the default namespace to \"${TEST_NS}\"" \
  "https://bugzilla.redhat.com/show_bug.cgi?id=1826676"
  # Workaround:
  echo -e "\n# Change the default namespace in [${KUBECONFIG}] to: ${TEST_NS:-default}"
  cur_context="$(${OC} config current-context)"
  ${OC} config set "contexts.${cur_context}.namespace" "${TEST_NS:-default}"

}

# ------------------------------------------

function install_netshoot_app_on_cluster_a() {
  PROMPT "Install Netshoot application on OCP cluster A (public)"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"

  [[ -z "$TEST_NS" ]] || create_namespace "${TEST_NS}" "privileged"

  ${OC} delete pod "${NETSHOOT_CLUSTER_A}" --ignore-not-found ${TEST_NS:+-n $TEST_NS} || :

  set_netshoot_image_spec "sleep infinity"
  
  # NETSHOOT_CLUSTER_A=netshoot-cl-a # Already exported in global subm_variables

  ${OC} run "${NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS} --image "${NETSHOOT_IMAGE}" \
  --overrides="${NETSHOOT_IMAGE_SPEC}"

  TITLE "Wait up to 3 minutes for Netshoot pod [${NETSHOOT_CLUSTER_A}] to be ready:"
  ${OC} wait --timeout=3m --for=condition=ready pod -l run="${NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS}
  ${OC} describe pod "${NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS}
}

# ------------------------------------------

function set_netshoot_image_spec() {
  ### Export the $NETSHOOT_IMAGE (Nettest) registry url (downstream or upstream) ###
    trap_to_debug_commands;

    # Command to run in the netshoot container
    local cmd_for_netshoot="$1"

    # Optional param: $2 => Nettest version to find in the registry
    local submariner_version_or_tag="${2:-$SUBM_VER_TAG}"

    # Downloading Nettest from VPN_REGISTRY (downstream) only
    # if using --registry-images and if $submariner_version_or_tag is not devel
    # if check_version_greater_or_equal "${submariner_version_or_tag}" "0.14" && \
    if  [[ ! "$submariner_version_or_tag" =~ devel ]] && \
        [[ "$REGISTRY_IMAGES" =~ ^(y|yes)$ ]] && \
        [[ -n "$SUBM_IMG_NETTEST" ]] ; then

      # Fix the $submariner_version_or_tag value for custom images
      set_subm_version_tag_var "submariner_version_or_tag"

      local netshot_image_url
      netshot_image_url="${VPN_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX}-${SUBM_IMG_NETTEST}:${submariner_version_or_tag}"

      TITLE "Setting \$NETSHOOT_IMAGE security policy and downstream registry url: \n ${netshot_image_url}"

      export "NETSHOOT_IMAGE=${netshot_image_url}"

    else
      TITLE "Using upstream \$NETSHOOT_IMAGE url: $NETSHOOT_IMAGE"
      
    fi

    export NETSHOOT_IMAGE_SPEC='{
      "spec": {
        "securityContext": {
          "runAsNonRoot": true,
          "runAsUser": 2000,
          "runAsGroup": 3000,
          "seccompProfile": {
            "type": "RuntimeDefault"
          }
        },
        "containers": [
          {
            "name": "netshoot",
            "image": "'${NETSHOOT_IMAGE}'",
            "command": [
                "/bin/bash",
                "-c",
                "'${cmd_for_netshoot}'"
              ],
            "securityContext": {
              "allowPrivilegeEscalation": false,
              "capabilities": {
                "drop": ["ALL"]
              }
            }
          }
        ]
      }
    }'

}

# ------------------------------------------

function install_nginx_svc_on_managed_cluster() {
  trap_to_debug_commands;
  local cluster_name

  export KUBECONFIG="${KUBECONF_MANAGED}"
  cluster_name="$(print_current_cluster_name || :)"
  PROMPT "Install Nginx service on managed cluster $cluster_name ${TEST_NS:+ (Namespace $TEST_NS)}"

  TITLE "Creating ${NGINX_CLUSTER_BC}:${NGINX_PORT} in ${TEST_NS}, using ${NGINX_IMAGE}, and disabling it's cluster-ip (with '--cluster-ip=None'):"

  install_nginx_service "${NGINX_CLUSTER_BC}" "${NGINX_IMAGE}" "${TEST_NS}" "--port=${NGINX_PORT}" || :
}

# ------------------------------------------

function test_basic_cluster_connectivity_before_submariner() {
### Pre-test - Demonstrate that the clusters arent connected without Submariner ###
  trap_to_debug_commands;
  local cluster_name

  export KUBECONFIG="${KUBECONF_MANAGED}"
  cluster_name="$(print_current_cluster_name || :)"
  PROMPT "Before Submariner is installed: Verifying IP connectivity on the SAME cluster ($cluster_name)"

  # Trying to connect from cluster A to cluster B/C will fail (after 5 seconds).
  # Its also worth looking at the clusters to see that Submariner is nowhere to be seen.

  echo -e "\n# Get IP of ${NGINX_CLUSTER_BC} on managed cluster $cluster_name ${TEST_NS:+(Namespace: $TEST_NS)} to verify connectivity:\n"

  ${OC} get svc -l app="${NGINX_CLUSTER_BC}" ${TEST_NS:+-n $TEST_NS}
  nginx_IP_cluster_bc=$(${OC} get svc -l app="${NGINX_CLUSTER_BC}" ${TEST_NS:+-n $TEST_NS} | awk 'FNR == 2 {print $3}')
    # nginx_cluster_b_ip: 100.96.43.129

  local netshoot_pod=netshoot-cl-bc-new # A new Netshoot pod on cluster B/C
  TITLE "Install $netshoot_pod on OSP managed cluster, and verify connectivity in the SAME cluster, to ${nginx_IP_cluster_bc}:${NGINX_PORT}"

  [[ -z "$TEST_NS" ]] || create_namespace "${TEST_NS}" "privileged"

  ${OC} delete pod ${netshoot_pod} --ignore-not-found ${TEST_NS:+-n $TEST_NS} || :

  set_netshoot_image_spec "curl --max-time 180 --verbose ${nginx_IP_cluster_bc}:${NGINX_PORT}"

  ${OC} run ${netshoot_pod} --attach=true --restart=Never --pod-running-timeout=3m --request-timeout=3m --rm -i \
  ${TEST_NS:+-n $TEST_NS} --image "${NETSHOOT_IMAGE}" --overrides="${NETSHOOT_IMAGE_SPEC}"
}

# ------------------------------------------

function test_clusters_disconnected_before_submariner() {
### Pre-test - Demonstrate that the clusters arent connected without Submariner ###
  PROMPT "Before Submariner is installed:
  Verifying that Netshoot pod on OCP cluster A (public), cannot reach Nginx service on managed cluster"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_MANAGED}"

  # Trying to connect from cluster A to cluster B/C, will fails (after 5 seconds).
  # Its also worth looking at the clusters to see that Submariner is nowhere to be seen.

  # nginx_IP_cluster_bc=$(${OC} get svc -l app=${NGINX_CLUSTER_BC} ${TEST_NS:+-n $TEST_NS} | awk 'FNR == 2 {print $3}')
  ${OC} get svc -l app="${NGINX_CLUSTER_BC}" ${TEST_NS:+-n $TEST_NS} | awk 'FNR == 2 {print $3}' > "$TEMP_FILE"
  nginx_IP_cluster_bc="$(< "$TEMP_FILE")"
    # nginx_cluster_bc_ip: 100.96.43.129

  export KUBECONFIG="${KUBECONF_HUB}"
  # ${OC} get pods -l run=${NETSHOOT_CLUSTER_A} ${TEST_NS:+-n $TEST_NS} --field-selector status.phase=Running | awk 'FNR == 2 {print $1}' > "$TEMP_FILE"
  # netshoot_pod_cluster_a="$(< $TEMP_FILE)"
  netshoot_pod_cluster_a="$(get_running_pod_by_label "run=${NETSHOOT_CLUSTER_A}" "$TEST_NS" )"

  msg="# Negative Test - Clusters should NOT be able to connect without Submariner."

  ${OC} exec "$netshoot_pod_cluster_a" ${TEST_NS:+-n $TEST_NS} -- \
  curl --output /dev/null --max-time 20 --verbose "${nginx_IP_cluster_bc}:${NGINX_PORT}" \
  |& (! highlight "command terminated with exit code" && FAILURE "$msg") || echo -e "$msg"
    # command terminated with exit code 28
}

# ------------------------------------------

function download_and_install_subctl() {
  ### Download SubCtl - Submariner installer - Latest RC release ###
    PROMPT "Testing \"getsubctl.sh\" to download and use SubCtl version $SUBM_VER_TAG"

    local subctl_version="${1:-$SUBM_VER_TAG}"

    download_subctl_by_tag "$subctl_version"

}

# ------------------------------------------

function set_versions_variables() {
# Set $SUBM_VER_TAG and $ACM_VER_TAG variables with the correct version (vX.Y.Z), branch name, or tag
  # trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"

  set_subm_version_tag_var "SUBM_VER_TAG" || :

  set_acm_version_tag_var "ACM_VER_TAG" || :

}

# ------------------------------------------

function set_subm_version_tag_var() {
# Update the variable value of $SUBM_VER_TAG (or the $1 input var name)
  trap_to_debug_commands;

  # Get variable name (default is "SUBM_VER_TAG")
  local tag_var_name="${1:-SUBM_VER_TAG}"

  # Set subm_version_tag as the actual value of tag_var_name
  local subm_version_tag="${!tag_var_name}" || :

  if [[ -z "${subm_version_tag}" ]] ; then
    TITLE "Submariner version to use was not defined. Checking if existing Submariner version is already installed"

    subm_version_tag="$(${OC} get submariner -A -o jsonpath='{.items[0].spec.version}')" || \
    FAILURE "Submariner version to use was not found. Try to run again with option '--subctl-version x.y.z'"
  fi

  TITLE "Retrieve correct tag for Submariner version \$${tag_var_name} : $subm_version_tag"

  if [[ "$subm_version_tag" =~ latest|devel ]]; then
    subm_version_tag=$(get_submariner_branch_tag)
  elif [[ "$subm_version_tag" =~ ^[0-9] ]]; then
    echo -e "\n# Version ${subm_version_tag} is considered as 'v${subm_version_tag}' tag"
    subm_version_tag=v${subm_version_tag}
  fi

  # export REGISTRY_TAG_MATCH='[0-9]+\.[0-9]+' # Regex for required image tag (X.Y.Z ==> X.Y)
  # echo -e "\n# REGISTRY_TAG_MATCH variable was set to extract from '$subm_version_tag' the regex match: $REGISTRY_TAG_MATCH"
  # subm_version_tag=v$(echo $subm_version_tag | grep -Po "$REGISTRY_TAG_MATCH")
  # echo -e "\n# New \$${tag_var_name} for registry images: $subm_version_tag"

  # Reevaluate $tag_var_name value
  local eval_cmd="export ${tag_var_name}=${subm_version_tag}"
  eval "$eval_cmd"
}

# ------------------------------------------

function set_acm_version_tag_var() {
# Update the variable value of $ACM_VER_TAG (or the $1 input var name)
  trap_to_debug_commands;

  # Get variable name (default is "ACM_VER_TAG")
  local tag_var_name="${1:-ACM_VER_TAG}"

  # Set acm_version_tag as the actual value of tag_var_name
  local acm_version_tag="${!tag_var_name}" || :

  if [[ -z "${acm_version_tag}" ]] ; then
    TITLE "ACM version to use was not defined. Checking if existing ACM version is already installed"

    acm_version_tag="$(${OC} get multiclusterhub -A -o jsonpath='{.items[0].status.currentVersion}')" || \
    FAILURE "ACM version to use was not found. Try to run again with option '--acm-version x.y.z'"
  fi

  TITLE "Retrieve correct tag for ACM version \$${tag_var_name} : $acm_version_tag"

  # Reevaluate $tag_var_name value
  local eval_cmd="export ${tag_var_name}=${acm_version_tag}"
  eval "$eval_cmd"
}

# ------------------------------------------

function download_subctl_by_tag() {
  ### Download SubCtl - Submariner installer ###
    trap_to_debug_commands;

    # Optional param: $1 => SubCtl version by tag to download
    # If not specifying a tag - it will download latest version released (not latest subctl-devel)
    local submariner_version_or_tag="${1:-v[0-9]}"

    cd "${WORKDIR}" || return 1

    # Downloading SubCtl from VPN_REGISTRY (downstream) only
    # if using --registry-images and if $submariner_version_or_tag is not devel
    if [[ ! "$submariner_version_or_tag" =~ devel ]] && \
        [[ "$REGISTRY_IMAGES" =~ ^(y|yes)$ ]] && \
        [[ -n "$SUBM_IMG_SUBCTL" ]] ; then

      TITLE "Backup previous subctl archive (if exists)"
      local subctl_xz="subctl-${submariner_version_or_tag}-linux-amd64.tar.xz"
      [[ ! -e "$subctl_xz" ]] || mv -f "${subctl_xz}" "${subctl_xz}".bak

      TITLE "Downloading SubCtl from [${VPN_REGISTRY}]"

      # Fix the $submariner_version_or_tag value for custom images
      set_subm_version_tag_var "submariner_version_or_tag"

      echo -e "\n# Since Submariner 0.12 the image prefix should not include 'tech-preview'"
      local subctl_image_url

      if check_version_greater_or_equal "${submariner_version_or_tag}" "0.12" ; then
        subctl_image_url="${VPN_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX}-${SUBM_IMG_SUBCTL}:${submariner_version_or_tag}"
      else
        subctl_image_url="${VPN_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}-${SUBM_IMG_SUBCTL}:${submariner_version_or_tag}"
        # e.g. subctl_image_url="registry-proxy.engineering.redhat.com/rh-osbs/rhacm2-tech-preview-subctl-rhel8:0.9"
      fi

      echo -e "\n# Check if $subctl_xz exists in $subctl_image_url"
      ${OC} image extract "$subctl_image_url" --path=/dist/subctl*:./ --dry-run \
      |& highlight "$subctl_xz" || BUG "SubCtl binary with tag '$submariner_version_or_tag' was not found in $subctl_image_url"

      ${OC} image extract "$subctl_image_url" --path=/dist/subctl-*-linux-amd64.tar.xz:./ --confirm

      echo -e "\n# Getting last downloaded subctl archive filename"
      subctl_xz="$(ls -1 -tc subctl-*-linux-amd64.tar.xz | head -1 )" || :
      ls -l "${subctl_xz}" || FATAL "subctl archive was not downloaded"

      echo -e "\n# SubCtl binary will be extracted from [${subctl_xz}]"
      tar -xvf "${subctl_xz}" --strip-components 1 --wildcards --no-anchored  "subctl*"

      echo -e "\n# Rename last extracted file to subctl"
      local extracted_file
      extracted_file="$(ls -1 -tc subctl* | head -1)"
      [[ -f "$extracted_file" ]] || FATAL "subctl binary was not found in ${subctl_xz}"
      ls -l "${extracted_file}"

      mv "$extracted_file" subctl
      chmod +x subctl

      echo -e "\n# Install subctl into ${GOBIN}:"
      mkdir -p "$GOBIN"
      # cp -f ./subctl $GOBIN/
      /usr/bin/install ./subctl "$GOBIN/subctl" || :

      echo -e "\n# Install subctl into user HOME bin:"
      # cp -f ./subctl ~/.local/bin/
      /usr/bin/install ./subctl ~/.local/bin/subctl || :

    else
      # Downloading SubCtl from Github (upstream)

      local repo_tag
      repo_tag=$(get_submariner_branch_tag "${submariner_version_or_tag}")

      TITLE "Downloading SubCtl '${repo_tag}' with getsubctl.sh from: https://get.submariner.io/"

      # curl https://get.submariner.io/ | VERSION=${submariner_version_or_tag} bash -x
      BUG "getsubctl.sh fails on an unexpected argument, since the local 'install' is not the default" \
      "set 'PATH=/usr/bin:$PATH' for the execution of 'getsubctl.sh'" \
      "https://github.com/submariner-io/submariner-operator/issues/473"
      # Workaround:
      PATH="/usr/bin:$PATH" type -P install

      #curl https://get.submariner.io/ | VERSION=${submariner_version_or_tag} PATH="/usr/bin:$PATH" bash -x
      BUG "getsubctl.sh sometimes fails on error 403 (rate limit exceeded)" \
      "If it has failed - Set 'getsubctl_status=FAILED' in order to download with wget instead" \
      "https://github.com/submariner-io/submariner-operator/issues/526"
      # Workaround:
      curl https://get.submariner.io/ | VERSION="${repo_tag}" PATH="/usr/bin:$PATH" bash -x || getsubctl_status=FAILED

      if [[ "$getsubctl_status" == FAILED ]] ; then

        TITLE "Download subctl directly, since 'getsubctl.sh' (https://get.submariner.io) failed"
        # For example, download: https://github.com/submariner-io/submariner-operator/releases/tag/subctl-release-0.8

        local subctl_releases_url
        subctl_releases_url="https://github.com/submariner-io/submariner-operator"
        releases_url="${subctl_releases_url}/releases"
        TITLE "Downloading SubCtl from upstream repository tag: ${releases_url}/tag/${repo_tag}"

        local file_path
        file_path="$(curl "${releases_url}/tag/${repo_tag}" | grep -Eoh 'download\/.*\/subctl-.*-linux-amd64[^"]+' -m 1)"

        download_file "${releases_url}/${file_path}"

        file_name=$(basename -- "$file_path")
        tar -xvf "${file_name}" --strip-components 1 --wildcards --no-anchored  "subctl*"

        # Rename last extracted file to subctl
        extracted_file="$(ls -1 -tc subctl* | head -1)"

        [[ ! -e "$extracted_file" ]] || mv "$extracted_file" subctl
        chmod +x subctl

        echo -e "\n# Install subctl into ${GOBIN}:"
        mkdir -p "$GOBIN"
        # cp -f ./subctl $GOBIN/
        /usr/bin/install ./subctl "$GOBIN/subctl"

        echo -e "\n# Install subctl into user HOME bin:"
        # cp -f ./subctl ~/.local/bin/
        /usr/bin/install ./subctl ~/.local/bin/subctl
      fi

    fi

    echo -e "\n# Copy subctl from user HOME bin into ${GOBIN}:"
    mkdir -p "$GOBIN"
    # cp -f ./subctl $GOBIN/
    /usr/bin/install "$HOME/.local/bin/subctl" "$GOBIN/subctl"

    echo -e "\n# Add user HOME bin to system PATH:"
    export PATH="$HOME/.local/bin:$PATH"

    echo -e "\n# Store SubCtl version in $SUBCTL_VERSION_FILE"
    subctl version | awk '{print $3}' > "$SUBCTL_VERSION_FILE"

}

# ------------------------------------------

function get_submariner_branch_tag() {
  ### Print the tag of latest subctl version released ###
  # Do not echo more info, since the output is the returned value

  local subctl_tag_to_search=
  subctl_tag_to_search="${1:-v[0-9]}" # any tag that starts with v{NUMBER}
  local subctl_repo_url
  subctl_repo_url="https://github.com/submariner-io/submariner-operator"

  get_latest_repository_version_by_tag "$subctl_repo_url" "$subctl_tag_to_search"
}

# ------------------------------------------

function test_subctl_command() {
  trap_to_debug_commands;
  local subctl_version
  subctl_version="$(subctl version | awk '{print $3}' )" || :

  PROMPT "Verifying Submariner CLI tool ${subctl_version:+ ($subctl_version)}"

  [[ -x "$(command -v subctl)" ]] || FATAL "No SubCtl installation found. Try to run again with option '--subctl-version'"
  subctl version

  subctl --help

  if [[ ! "$SUBM_VER_TAG" =~ latest|devel ]] && [[ ! $subctl_version =~ $SUBM_VER_TAG ]] ; then

    local msg="Subctl tool version '${subctl_version}' is not as requested: $SUBM_VER_TAG"

    BUG "$msg" \
    "Bug was reported, no workaround is required" \
    "https://issues.redhat.com/browse/ACM-2132"

    FAILURE "$msg"
  fi

}

# ------------------------------------------

function set_join_parameters_for_cluster_a() {
  PROMPT "Set parameters of SubCtl Join command for OCP cluster A (public)"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  create_subctl_join_file "${SUBCTL_JOIN_CLUSTER_A_FILE}"
}

# ------------------------------------------

function set_join_parameters_for_cluster_b() {
  PROMPT "Set parameters of SubCtl Join command for OSP cluster B (on-prem)"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  create_subctl_join_file "${SUBCTL_JOIN_CLUSTER_B_FILE}"
}

# ------------------------------------------

function set_join_parameters_for_cluster_c() {
  PROMPT "Set parameters of SubCtl Join command for cluster C"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  create_subctl_join_file "${SUBCTL_JOIN_CLUSTER_C_FILE}"
}

# ------------------------------------------

function create_subctl_join_file() {
# Join Submariner member - of current cluster kubeconfig
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"
  local join_cmd_file="$1"

  TITLE "Adding Broker file and IPSec ports to subctl join command on cluster ${cluster_name}"

  JOIN_CMD="subctl join \
  ./${SUBM_BROKER_INFO} ${SUBM_CABLE_DRIVER:+--cable-driver $SUBM_CABLE_DRIVER} \
  --ikeport ${IPSEC_IKE_PORT} --nattport ${IPSEC_NATT_PORT}"

  TITLE "Adding '--health-check' to subctl join command (to enable Gateway health check)"

  JOIN_CMD="${JOIN_CMD} --health-check"

  local pod_debug_flag="--pod-debug"
  # For SubCtl <= 0.8 : '--enable-pod-debugging' is expected as the debug flag for the join command"
  [[ $(subctl version | grep --invert-match "v0.8") ]] || pod_debug_flag="--enable-pod-debugging"

  TITLE "Adding '${pod_debug_flag}' and '--ipsec-debug' to subctl join command (for tractability)"
  JOIN_CMD="${JOIN_CMD} ${pod_debug_flag} --ipsec-debug"

  BUG "SubCtl fails to join cluster, since it cannot auto-generate a valid cluster id" \
  "Add '--clusterid <ID>' to $join_cmd_file" \
  "https://bugzilla.redhat.com/show_bug.cgi?id=1972703"
  # Workaround
  ${OC} config view
  local cluster_id
  cluster_id=$(${OC} config current-context)

  BUG "SubCtl join of a long clusterid - No IPsec connections will be loaded later" \
  "Add '--clusterid <SHORT ID>' (e.g. of cluster id of the admin context) to $join_cmd_file" \
  "https://bugzilla.redhat.com/show_bug.cgi?id=1988797"
  # Workaround
  cluster_id=$(${OC} config view -o jsonpath='{.contexts[?(@.context.user == "admin")].context.cluster}' | awk '{print $1}')

  echo -e "\n# Write the join parameters into the join command file: $join_cmd_file"
  # Replace anything but letters and numbers with "-"
  JOIN_CMD="${JOIN_CMD} --clusterid ${cluster_id//[^a-zA-Z0-9]/-}"

  echo "$JOIN_CMD" > "$join_cmd_file"

}

# ------------------------------------------

function append_custom_images_to_join_cmd_cluster_a() {
# Append custom images to the join cmd file, for cluster A
  PROMPT "Append custom images to the join command of cluster A"
  trap_to_debug_commands;

  append_custom_images_to_join_cmd_file "${SUBCTL_JOIN_CLUSTER_A_FILE}"
}

# ------------------------------------------

function append_custom_images_to_join_cmd_cluster_b() {
# Append custom images to the join cmd file, for cluster B
  PROMPT "Append custom images to the join command of cluster B"
  trap_to_debug_commands;

  append_custom_images_to_join_cmd_file "${SUBCTL_JOIN_CLUSTER_B_FILE}"
}

# ------------------------------------------

function append_custom_images_to_join_cmd_cluster_c() {
# Append custom images to the join cmd file, for cluster C
  PROMPT "Append custom images to the join command of cluster C"
  trap_to_debug_commands;

  append_custom_images_to_join_cmd_file "${SUBCTL_JOIN_CLUSTER_C_FILE}"
}

# ------------------------------------------

function append_custom_images_to_join_cmd_file() {
# Join Submariner member - of current cluster kubeconfig
  trap_to_debug_commands;

  local join_cmd_file="$1"
  echo -e "\n# Read subctl join command from file: $join_cmd_file"
  local JOIN_CMD
  JOIN_CMD="$(< "$join_cmd_file")"

  [[ -x "$(command -v subctl)" ]] || FATAL "No SubCtl installation found. Try to run again with option '--subctl-version'"
  local submariner_version_or_tag
  submariner_version_or_tag="$(subctl version | awk '{print $3}')"

  BUG "Overriding images with wrong keys should fail first in join command" \
  "No workaround" \
  "https://github.com/submariner-io/submariner-operator/issues/1018"

  echo -e "\n# Since Submariner 0.12 the image prefix should not include 'tech-preview'"
  local submariner_operator_image

  if check_version_greater_or_equal "${submariner_version_or_tag}" "0.12" ; then
    submariner_operator_image="${OFFICIAL_REGISTRY}/${REGISTRY_IMAGE_PREFIX}-${SUBM_IMG_OPERATOR}:${submariner_version_or_tag}"
  else
    submariner_operator_image="${OFFICIAL_REGISTRY}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}-${SUBM_IMG_OPERATOR}:${submariner_version_or_tag}"
  fi

  TITLE "Append \"--image-override\" to subctl join command with a custom image:
  ${submariner_operator_image}"

  JOIN_CMD="${JOIN_CMD} --image-override submariner-operator=${submariner_operator_image}"

  echo -e "\n# Write into the join command file [${join_cmd_file}]: \n${JOIN_CMD}"
  echo "$JOIN_CMD" > "$join_cmd_file"

}

# ------------------------------------------

function install_broker_via_subctl_on_cluster_a() {
### Installing Submariner Broker on OCP cluster A (public) ###
  echo -e "\n# TODO: Should test broker deployment also on different Public cluster (C), rather than on Public cluster A."
  echo -e "\n# TODO: Call kubeconfig of broker cluster"
  trap_to_debug_commands;

  local DEPLOY_CMD="subctl deploy-broker"

  if [[ "$GLOBALNET" =~ ^(y|yes)$ ]]; then
    echo -e "\n# TODO: Move to a separate function"
    PROMPT "Adding GlobalNet to Submariner Deploy command"

    BUG "Running subctl with GlobalNet can fail if glabalnet_cidr address is already assigned" \
    "Define a new and unique globalnet-cidr for this cluster" \
    "https://github.com/submariner-io/submariner/issues/544"

    # DEPLOY_CMD="${DEPLOY_CMD} --globalnet --globalnet-cidr 169.254.0.0/19"
    DEPLOY_CMD="${DEPLOY_CMD} --globalnet"
  fi

  PROMPT "Deploying Submariner Broker on OCP cluster A (public)"
  # Deploys Submariner CRDs, creates the SA for the broker, the role and role bindings

  export KUBECONFIG="${KUBECONF_HUB}"
  DEPLOY_CMD="${DEPLOY_CMD} --kubecontext $(${OC} config current-context)"

  cd "${WORKDIR}" || return 1
  #cd $GOPATH/src/github.com/submariner-io/submariner-operator

  TITLE "Remove previous ${SUBM_BROKER_INFO} (if exists)"
  [[ ! -e "${SUBM_BROKER_INFO}" ]] || rm "${SUBM_BROKER_INFO}"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"
  TITLE "Executing SubCtl Deploy command on $cluster_name: \n# ${DEPLOY_CMD}"

  BUG "For Submariner 0.9+ operator image should be accessible before broker deploy" \
  "Run broker deployment after uploading custom images to the cluster registry" \
  "https://github.com/submariner-io/submariner-website/issues/483"

  $DEPLOY_CMD
}

# ------------------------------------------

function install_broker_via_api_on_cluster() {
  ### Create the Submariner Broker via API - Only on the first cluster (usually the Hub) ###

  trap_to_debug_commands;

  local kubeconfig_file="$1"
  local submariner_version_or_tag="${2:-$SUBM_VER_TAG}"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Create the Submariner Broker via API on cluster ${cluster_name}"

  # Since Submariner 0.12 it is required to create Broker CRD
  if check_version_greater_or_equal "$submariner_version_or_tag" "0.12" ; then

    local broker_namespace
    broker_namespace="$(get_broker_namespace || :)"

    [[ -n "${broker_namespace}" ]] || FATAL "Failed to retreive the managed cluster-set namespace on cluster ${cluster_name}"

    local enable_globalnet="true"
    [[ "$GLOBALNET" =~ ^(y|yes)$ ]] || enable_globalnet="false"

    TITLE "Creating the Submariner Broker in the cluster-set namespace '${broker_namespace}' with Globalnet=${enable_globalnet}"

    # Submariner Broker is created once per cluster-set.
    # Using a different name then 'submariner-broker' is not supported.
    # Ref: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html-single/add-ons/index#submariner-globalnet

    cat <<EOF | ${OC} apply -f -
    apiVersion: submariner.io/v1alpha1
    kind: Broker
    metadata:
      name: submariner-broker
      namespace: ${broker_namespace}
    spec:
      globalnetEnabled: ${enable_globalnet}
EOF

  else
    echo -e "\n# Submariner version $submariner_version_or_tag is less than 0.12 - Broker creation is not supported via API"
  fi

}

# ------------------------------------------

function test_broker_before_join() {
  PROMPT "Verify Submariner resources on the Broker cluster"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"

  # Now looking at Broker cluster, it should show that CRDs, but no pods in namespace
  ${OC} get crds | grep 'submariner.io'

  ${OC} describe crds \
  clusters.submariner.io \
  endpoints.submariner.io \
  serviceimports.multicluster.x-k8s.io || FAILURE "Expected to find CRD 'serviceimports.multicluster.x-k8s.io'"

  # serviceexports.lighthouse.submariner.io \
  # servicediscoveries.submariner.io \
  # submariners.submariner.io \
  # gateways.submariner.io \

  local regex="submariner-operator"
  # For SubCtl <= 0.8 : "No resources found" is expected on the broker after deploy command
  [[ $(subctl version | grep --invert-match "v0.8") ]] || regex="No resources found"

  if [[ ! "$SKIP_OCP_SETUP" =~ ^(y|yes)$ ]]; then
    ${OC} get pods -n "${SUBM_NAMESPACE}" --show-labels |& highlight "$regex" \
     || FATAL "Submariner Broker which was created with $(subctl version) deploy command (before join) \
      should have \"$regex\" in the Broker namespace '${SUBM_NAMESPACE}'"
  fi
}

# ------------------------------------------

function open_firewall_ports_on_cluster_a() {
  PROMPT "Open firewall ports for the gateway node on OCP cluster A"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  open_firewall_ports_on_aws_gateway_nodes "$CLUSTER_A_DIR"
}

# ------------------------------------------

function open_firewall_ports_on_cluster_c() {
  PROMPT "Open firewall ports for the gateway node on OCP cluster C"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  open_firewall_ports_on_aws_gateway_nodes "$CLUSTER_C_DIR"
}

# ------------------------------------------

function open_firewall_ports_on_aws_gateway_nodes() {
### Open firewall ports for the gateway node with terraform (prep_for_subm.sh) on AWS cluster ###
  # Old readme: https://github.com/submariner-io/submariner/tree/devel/tools/openshift/ocp-ipi-aws
  echo -e "\n# TODO: subctl cloud prepare as: https://submariner.io/getting-started/quickstart/openshift/aws/#prepare-aws-clusters-for-submariner"
  trap_to_debug_commands;

  TITLE "Using \"prep_for_subm.sh\" - to add External IP and open ports on AWS cluster nodes for Submariner gateway"
  command -v terraform || FATAL "Terraform is required in order to run 'prep_for_subm.sh'"

  local ocp_install_dir="$1"

  local git_user="submariner-io"
  local git_project="submariner"
  local commit_or_branch="release-0.8"
  local github_dir="tools/openshift/ocp-ipi-aws"
  local target_path="${ocp_install_dir}/${github_dir}"
  local terraform_script="prep_for_subm.sh"

  mkdir -p "${git_project}_scripts"
  cd "${git_project}_scripts" || return 1

  download_github_file_or_dir "$git_user" "$git_project" "$commit_or_branch" "${github_dir}"

  BUG "'${terraform_script}' ignores local yamls and always download from devel branch" \
  "Copy '${github_dir}' directory (including '${terraform_script}') into OCP install dir" \
  "https://github.com/submariner-io/submariner/issues/880"
  # Workaround:

  TITLE "Copy '${github_dir}' directory (including '${terraform_script}') to ${target_path}"
  mkdir -p "${target_path}"
  cp -rf "${github_dir}"/* "${target_path}"
  cd "${target_path}/" || return 1

  # Fix bug in terraform version
  sed -r 's/0\.12\.12/0\.12\.29/g' -i versions.tf || :

  # Workaround for Terraform provider permission denied
  local terraform_plugins_dir="./.terraform/plugins/linux_amd64"
  if [[ -d "${terraform_plugins_dir}" ]] && [[ "$(ls -A "$terraform_plugins_dir")" ]] ; then
    chmod -R a+x $terraform_plugins_dir/* || :
  fi

  # Fix bug of using non-existing kubeconfig conext "admin"
  sed -e 's/--context=admin //g' -i "${terraform_script}"

  BUG "'prep_for_subm.sh' downloads remote 'ocp-ipi-aws', even if local 'ocp-ipi-aws' already exists" \
  "Modify 'prep_for_subm.sh' so it will download all 'ocp-ipi-aws/*' and do not change directory" \
  "----"
  # Workaround:
  sed 's/.*submariner_prep.*/# \0/' -i "${terraform_script}"

  BUG "Using the same IPSEC port numbers multiple times in one project, may be blocked on firewall" \
  "Make sure to use different IPSEC_NATT_PORT and IPSEC_IKE_PORT across clusters on same project" \
  "https://github.com/submariner-io/submariner-operator/issues/1047"
  # Workaround:
  # Do not use the same IPSEC port numbers multiple times in one project
  # Add in global submariner_variables
  # export IPSEC_NATT_PORT=${IPSEC_NATT_PORT:-4501}
  # export IPSEC_IKE_PORT=${IPSEC_IKE_PORT:-501}

  export GW_INSTANCE_TYPE=${GW_INSTANCE_TYPE:-m4.xlarge}

  TITLE "Running '${terraform_script} ${ocp_install_dir} -auto-approve' script to apply Terraform 'ec2-resources.tf'"
  # Use variables: -var region=eu-west-2 -var region=eu-west-1 or with: -var-file=newvariable.tf
  # bash -x ...
  ./${terraform_script} "${ocp_install_dir}" -auto-approve |& highlight "Apply complete| already exists" \
  || FATAL "./${terraform_script} did not complete successfully"

  # Apply complete! Resources: 5 added, 0 changed, 0 destroyed.
  # OR
  # Security group rule already exists.
  #

}

# ------------------------------------------

function open_firewall_ports_on_openstack_cluster_b() {
  PROMPT "Open firewall ports for the gateway node on OSP cluster B"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  open_firewall_ports_on_osp_gateway_nodes "$CLUSTER_B_DIR"
}

# ------------------------------------------

function open_firewall_ports_on_osp_gateway_nodes() {
### Open OSP firewall ports on the gateway node with terraform (configure_osp.sh) ###
  # Readme: https://github.com/sridhargaddam/configure-osp-for-subm
  trap_to_debug_commands;

  TITLE "Using \"configure_osp.sh\" - to open firewall ports on all nodes in OSP cluster (on-prem)"
  command -v terraform || FATAL "Terraform is required in order to run 'configure_osp.sh'"

  local ocp_install_dir="$1"
  local git_user="redhat-openshift"
  local git_project="configure-osp-for-subm"
  local commit_or_branch="main"
  local github_dir="osp-scripts"
  local target_path="${ocp_install_dir}/${github_dir}"
  local terraform_script="configure_osp.sh"

  mkdir -p "${git_project}_scripts"
  cd "${git_project}_scripts" || return 1

  # Temporary, until merged to upstream
  # download_github_file_or_dir "$git_user" "$git_project" "$commit_or_branch" "${github_dir}"
  download_github_file_or_dir "$git_user" "$git_project" "$commit_or_branch" # "${github_dir}"

  # echo -e "\n# Copy '${github_dir}' directory (including '${terraform_script}') to ${target_path}"
  # mkdir -p "${target_path}"
  # cp -rf "${github_dir}"/* "${target_path}"
  # cd "${target_path}/"

  TITLE "Copy '${git_project}_scripts' directory (including '${terraform_script}') to ${target_path}_scripts"
  mkdir -p "${target_path}_scripts"
  cp -rf * "${target_path}_scripts"
  cd "${target_path}_scripts/" || return 1
  ### Temporary end

  # Fix bug in terraform version
  sed -r 's/0\.12\.12/0\.12\.29/g' -i versions.tf || :

  # Fix bug in terraform provider permission denied
  chmod -R a+x ./.terraform/plugins/linux_amd64/* || :

  # export IPSEC_NATT_PORT=${IPSEC_NATT_PORT:-4501}
  # export IPSEC_IKE_PORT=${IPSEC_IKE_PORT:-501}

  TITLE "Running '${terraform_script} ${ocp_install_dir} -auto-approve' script to apply open OSP required ports:"

  chmod a+x ./${terraform_script}
  # Use variables: -var region=eu-west-2 -var region=eu-west-1 or with: -var-file=newvariable.tf
  # bash -x ...
  ./${terraform_script} "${ocp_install_dir}" -auto-approve |& highlight "Apply complete| already exists" \
  || FAILURE "./${terraform_script} did not complete successfully"

  # Apply complete! Resources: 5 added, 0 changed, 0 destroyed.
  # OR
  # Security group rule already exists.
  #

}

# ------------------------------------------

function label_gateway_on_broker_nodes_with_external_ip() {
### Label a Gateway node on OCP cluster A (public) ###
  PROMPT "Adding Gateway label to all worker nodes with an External-IP on OCP cluster A (public)"
  trap_to_debug_commands;

  BUG "If one of the gateway nodes does not have External-IP, submariner will fail to connect later" \
  "Make sure one node with External-IP has a gateway label" \
  "https://github.com/submariner-io/submariner-operator/issues/253"

  export KUBECONFIG="${KUBECONF_HUB}"
  echo -e "\n# TODO: Check that the Gateway label was created with prep_for_subm.sh on OCP cluster A (public) ?"
  gateway_label_all_nodes_external_ip
}

# ------------------------------------------

function label_first_gateway_cluster_b() {
### Label a Gateway node on cluster B ###
  PROMPT "Adding Gateway label to the first worker node on cluster B"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  gateway_label_first_worker_node
}

# ------------------------------------------

function label_first_gateway_cluster_c() {
### Label a Gateway node on cluster C ###
  PROMPT "Adding Gateway label to the first worker node on cluster C"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"

  BUG "Having 2 nodes with GW label, can cause connection failure" \
  "Skip labeling another node with Submariner gateway" \
  "TODO: Report bug"
  # Workaround: Skip:
  # gateway_label_first_worker_node
}

# ------------------------------------------

function gateway_label_first_worker_node() {
### Adding submariner gateway label to the first worker node ###
  trap_to_debug_commands;

  # gw_node1=$(${OC} get nodes -l node-role.kubernetes.io/worker | awk 'FNR == 2 {print $1}')
  ${OC} get nodes -l node-role.kubernetes.io/worker | awk 'FNR == 2 {print $1}' > "$TEMP_FILE"
  gw_node1="$(< "$TEMP_FILE")"

  [[ -n "$gw_node1" ]] || FATAL "Failed to list worker nodes in current cluster"

  TITLE "Adding submariner gateway labels to first worker node: $gw_node1"
    # gw_node1: user-cl1-bbmkg-worker-8mx4k

  echo -e "\n# TODO: Run only If there's no Gateway label already"
  ${OC} label node "$gw_node1" "submariner.io/gateway=true" --overwrite
    # node/user-cl1-bbmkg-worker-8mx4k labeled

  # ${OC} get nodes -l "submariner.io/gateway=true" |& highlight "Ready"
      # NAME                          STATUS   ROLES    AGE     VERSION
      # ip-10-0-89-164.ec2.internal   Ready    worker   5h14m   v1.14.6+c07e432da
  wait_for_all_nodes_ready

  echo -e "\n# Show Submariner Gateway Nodes: \n"
  ${OC} describe nodes -l submariner.io/gateway=true

}

# ------------------------------------------

function gateway_label_all_nodes_external_ip() {
### Adding submariner gateway label to all worker nodes with an External-IP ###
  trap_to_debug_commands;

  local external_ips
  external_ips="$(mktemp)_external_ips"

  ${OC} wait --timeout=3m --for=condition=ready nodes -l node-role.kubernetes.io/worker

  watch_and_retry "get_worker_nodes_with_external_ip" 5m || external_ips=NONE

  ${OC} get nodes -o wide

  if [[ "$external_ips" == NONE ]] ; then
    failed_machines=$(${OC} get Machine -A -o jsonpath='{.items[?(@.status.phase!="Running")].metadata.name}')

    FATAL "EXTERNAL-IP was not created yet. Please check if \"prep_for_subm.sh\" script had errors.
    ${failed_machines:+ Failed Machines: \n$(${OC} get Machine -A -o wide)}"
  fi

  local gw_nodes
  gw_nodes=$(get_worker_nodes_with_external_ip)

  TITLE "Adding submariner gateway label to all worker nodes with an External-IP: $gw_nodes"
    # gw_nodes: user-cl1-bbmkg-worker-8mx4k

  for node in $gw_nodes; do
    echo -e "\n# TODO: Run only If there's no Gateway label already"
    ${OC} label node "$node" "submariner.io/gateway=true" --overwrite
      # node/user-cl1-bbmkg-worker-8mx4k labeled
  done

  #${OC} get nodes -l "submariner.io/gateway=true" |& highlight "Ready"
    # NAME                          STATUS   ROLES    AGE     VERSION
    # ip-10-0-89-164.ec2.internal   Ready    worker   5h14m   v1.14.6+c07e432da
  wait_for_all_nodes_ready

  echo -e "\n# Show Submariner Gateway Nodes: \n"
  ${OC} describe nodes -l submariner.io/gateway=true
}

# ------------------------------------------

function configure_ocp_garbage_collection_and_images_prune() {
### function to set garbage collection on all cluster nodes
  trap_to_debug_commands;

  local kubeconfig_file="$1"
  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="${2:-$(print_current_cluster_name || :)}"

  PROMPT "Configure Garbage Collection and Registry Images Prune on cluster ${cluster_name}"

  ocp_login "${OCP_USR}" "$(< "${WORKDIR}/${OCP_USR}.sec")"

  TITLE "Setting garbage collection on all OCP cluster nodes"

  cat <<EOF | ${OC} apply -f -
  apiVersion: machineconfiguration.openshift.io/v1
  kind: KubeletConfig
  metadata:
    name: garbage-collector-kubeconfig
  spec:
    machineConfigPoolSelector:
      matchLabels:
        custom-kubelet: small-pods
    kubeletConfig:
      evictionSoft:
        memory.available: "500Mi"
        nodefs.available: "10%"
        nodefs.inodesFree: "5%"
        imagefs.available: "15%"
        imagefs.inodesFree: "10%"
      evictionSoftGracePeriod:
        memory.available: "1m30s"
        nodefs.available: "1m30s"
        nodefs.inodesFree: "1m30s"
        imagefs.available: "1m30s"
        imagefs.inodesFree: "1m30s"
      evictionHard:
        memory.available: "200Mi"
        nodefs.available: "5%"
        nodefs.inodesFree: "4%"
        imagefs.available: "10%"
        imagefs.inodesFree: "5%"
      evictionPressureTransitionPeriod: 0s
      imageMinimumGCAge: 5m
      imageGCHighThresholdPercent: 80
      imageGCLowThresholdPercent: 75
EOF

  TITLE "Setting ContainerRuntimeConfig limits on all OCP cluster nodes"

  cat <<EOF | ${OC} apply -f -
  apiVersion: machineconfiguration.openshift.io/v1
  kind: ContainerRuntimeConfig
  metadata:
   name: overlay-size
  spec:
   machineConfigPoolSelector:
     matchLabels:
       custom-crio: overlay-size
   containerRuntimeConfig:
     pidsLimit: 2048
     logLevel: debug
     overlaySize: 8G
     log_size_max: 52428800
EOF

  TITLE "Enable Image Pruner policy - to delete unused images from registry (daily at midnight)"

  cat <<EOF | ${OC} apply -f -
  kind: List
  apiVersion: v1
  items:
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: pruner
      namespace: openshift-image-registry
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: openshift-image-registry-pruner
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:image-pruner
    subjects:
    - kind: ServiceAccount
      name: pruner
      namespace: openshift-image-registry
  - apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: image-pruner
      namespace: openshift-image-registry
    spec:
      schedule: "0 0 * * *"
      concurrencyPolicy: Forbid
      successfulJobsHistoryLimit: 1
      failedJobsHistoryLimit: 3
      jobTemplate:
        spec:
          template:
            spec:
              restartPolicy: OnFailure
              containers:
              - image: "quay.io/openshift/origin-cli:4.1"
                resources:
                  requests:
                    cpu: 1
                    memory: 1Gi
                terminationMessagePolicy: FallbackToLogsOnError
                command:
                - oc
                args:
                - adm
                - prune
                - images
                - --certificate-authority=/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
                - --keep-tag-revisions=5
                - --keep-younger-than=96h
                - --confirm=true
                name: image-pruner
              serviceAccountName: pruner
EOF

  ${OC} patch imagepruner.imageregistry/cluster --patch '{"spec":{"suspend":false}}' --type=merge
    # imagepruner.imageregistry.operator.openshift.io/cluster patched

  ${OC} wait imagepruner --timeout=10s --for=condition=available cluster
    # imagepruner.imageregistry.operator.openshift.io/cluster condition met

  ${OC} describe imagepruner.imageregistry.operator.openshift.io

  TITLE "List all images in all pods:"
  ${OC} get pods -A -o jsonpath="{..imageID}" |tr -s '[[:space:]]' '\n' | sort | uniq -c | awk '{print $2}'
}

# ------------------------------------------

function configure_custom_registry_in_cluster() {
  trap_to_debug_commands;

  local kubeconfig_file="$1"
  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="${2:-$(print_current_cluster_name || :)}"

  PROMPT "Using custom Registry for ACM and Submariner images on cluster ${cluster_name}"

  [[ -n "${cluster_name}" ]] || FATAL "Cluster is inaccessible"

  ocp_login "${OCP_USR}" "$(< "${WORKDIR}/${OCP_USR}.sec")"
  
  configure_cluster_custom_registry_secrets

  configure_cluster_custom_registry_mirror

}

# ------------------------------------------

function configure_cluster_custom_registry_secrets() {
### Configure access to external docker registry
  trap '' DEBUG # DONT trap_to_debug_commands

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  TITLE "Configure OCP registry local secret on cluster $cluster_name"

  wait_for_all_machines_ready || :
  wait_for_all_nodes_ready || :

  ( # subshell to hide commands
    ocp_token=$(${OC} whoami -t)

    local ocp_registry_url
    ocp_registry_url=$(${OC} registry info --internal)

    create_docker_registry_secret "$ocp_registry_url" "$OCP_USR" "$ocp_token" "$SUBM_NAMESPACE"

    # Do not ${OC} logout - it will cause authentication error pulling images during join command
  )

}

# ------------------------------------------

function configure_cluster_custom_registry_mirror() {
### Configure a mirror server on the cluster registry
  trap '' DEBUG # DONT trap_to_debug_commands

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  TITLE "Wait for Cluster Operators, Authentication and Kube API Server readiness on cluster $cluster_name"

  ${OC} wait --timeout=5m --for=condition=Available clusteroperators authentication kube-apiserver || :
  ${OC} wait --timeout=5m --for='condition=Progressing=False' clusteroperators authentication kube-apiserver || :
  ${OC} wait --timeout=5m --for='condition=Degraded=False' clusteroperators authentication kube-apiserver || :

  ${OC} get clusteroperators authentication kube-apiserver

  TITLE "Add OCP secrets to access custom registry '${BREW_REGISTRY}' on cluster $cluster_name"

  echo -e "\n# Adding access to ${BREW_REGISTRY} in ${SUBM_NAMESPACE}"
  ( # subshell to hide commands
    create_docker_registry_secret "$BREW_REGISTRY" "$REGISTRY_USR" "$REGISTRY_PWD" "$SUBM_NAMESPACE"
  )

  echo -e "\n# Adding access to ${BREW_REGISTRY} in ${ACM_NAMESPACE}"
  ( # subshell to hide commands
    create_docker_registry_secret "$BREW_REGISTRY" "$REGISTRY_USR" "$REGISTRY_PWD" "$ACM_NAMESPACE"
  )

  echo -e "\n# Adding access to ${BREW_REGISTRY} in ${MCE_NAMESPACE}"
  ( # subshell to hide commands
    create_docker_registry_secret "$BREW_REGISTRY" "$REGISTRY_USR" "$REGISTRY_PWD" "$MCE_NAMESPACE"
  )

  TITLE "Add OCP Registry mirrors to OCP cluster '$cluster_name' nodes using MachineConfig"

  local ocp_registry_url
  ocp_registry_url=$(${OC} registry info --internal)
  local local_registry_path="${ocp_registry_url}/${SUBM_NAMESPACE}"

  BUG "Using image tags for ImageContentSourcePolicy resource is not supported" \
  "Create MachineConfig (instead of ImageContentSourcePolicy) to configure registry mirrors on all nodes" \
  "https://issues.redhat.com/browse/RFE-1608"
  # Workaround: deprecating creation of ImageContentSourcePolicy in favor of MachineConfig
  # add_image_content_source_policy
  add_acm_registry_mirror_to_ocp_node "master" "${local_registry_path}" || :
  add_acm_registry_mirror_to_ocp_node "worker" "${local_registry_path}" || :

  wait_for_all_machines_ready || :
  wait_for_all_nodes_ready || :

  TITLE "Show OCP Registry (machine-config encoded) on master nodes of cluster $cluster_name:"
  ${OC} get mc 99-master-submariner-registries -o json | jq -r '.spec.config.storage.files[0].contents.source' | awk -F ',' '{print $2}' || :

  TITLE "Show OCP Registry (machine-config encoded) on worker nodes of cluster $cluster_name:"
  ${OC} get mc 99-worker-submariner-registries -o json | jq -r '.spec.config.storage.files[0].contents.source' | awk -F ',' '{print $2}' || :

}

# ------------------------------------------

function add_image_content_source_policy() {
### Helper function to add OCP registry mirror for ACM and Submariner on all master or all worker nodes
  trap_to_debug_commands

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  TITLE "Configuring custom registry mirrors with ImageContentSourcePolicy on cluster $cluster_name"

  local img_policy
  img_policy="$(mktemp)_ImageContentSourcePolicy.yaml"

  cat <<-EOF > "$img_policy"
  apiVersion: operator.openshift.io/v1alpha1
  kind: ImageContentSourcePolicy
  metadata:
    name: brew-registry
  spec:
    repositoryDigestMirrors:
    - mirrors:
      - ${BREW_REGISTRY}
      source: ${OFFICIAL_REGISTRY}
    - mirrors:
      - ${BREW_REGISTRY}
      source: ${STAGING_REGISTRY}
    - mirrors:
      - ${BREW_REGISTRY}
      source: ${VPN_REGISTRY}
EOF

  ${OC} apply --dry-run='server' -f "$img_policy" | highlight "unchanged" \
  || ${OC} apply -f "$img_policy"

  ${OC} get ImageContentSourcePolicy -o yaml

}

# ------------------------------------------

function add_acm_registry_mirror_to_ocp_node() {
### Helper function to add OCP registry mirror for ACM and Submariner on all master or all worker nodes
  trap_to_debug_commands

  # set registry variables
  local node_type="$1" # master or worker
  # local registry_url="$2"
  local local_registry_path="$2"

  reg_values="
  node_type = $node_type
  local_registry_path = $local_registry_path"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  if [[ -z "$local_registry_path" ]] || [[ ! "$node_type" =~ ^(master|worker)$ ]]; then
    FATAL "Openshift Registry values are missing: $reg_values"
  else
    TITLE "Adding Submariner registry mirrors in all OCP '${node_type}' nodes of cluster $cluster_name:
    * ${OFFICIAL_REGISTRY}/${REGISTRY_IMAGE_PREFIX} -->
          - ${local_registry_path}
          - ${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX}

    * ${STAGING_REGISTRY}/${REGISTRY_IMAGE_PREFIX} -->
          - ${local_registry_path}
          - ${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX}

    * ${VPN_REGISTRY} -->
          - ${BREW_REGISTRY}

    * ${OFFICIAL_REGISTRY}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW} -->
          - ${local_registry_path}
          - ${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}

    * ${STAGING_REGISTRY}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW} -->
          - ${local_registry_path}
          - ${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}

    * ${CATALOG_REGISTRY}/${CATALOG_IMAGE_PREFIX}/${CATALOG_IMAGE_IMPORT_PATH} -->
          - ${OFFICIAL_REGISTRY}/${CATALOG_IMAGE_PREFIX}/${CATALOG_IMAGE_IMPORT_PATH}

    * ${OFFICIAL_REGISTRY}/${QUAY_IMAGE_MCE_PREFIX} -->
          - ${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${QUAY_IMAGE_MCE_PREFIX}
          - ${QUAY_REGISTRY}/${QUAY_IMAGE_IMPORT_PATH}/${QUAY_IMAGE_MCE_PREFIX}
    "
  fi

  config_source=$(cat <<EOF | raw_to_url_encode
  [[registry]]
    prefix = ""
    location = "${OFFICIAL_REGISTRY}/${REGISTRY_IMAGE_PREFIX}"
    mirror-by-digest-only = false
    insecure = false
    blocked = false

    [[registry.mirror]]
      location = "${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX}"
      insecure = false

    [[registry.mirror]]
      location = "${local_registry_path}"
      insecure = false

  [[registry]]
    prefix = ""
    location = "${STAGING_REGISTRY}/${REGISTRY_IMAGE_PREFIX}"
    mirror-by-digest-only = false
    insecure = false
    blocked = false

    [[registry.mirror]]
      location = "${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX}"
      insecure = false

    [[registry.mirror]]
      location = "${local_registry_path}"
      insecure = false

  [[registry]]
    prefix = ""
    location = "${VPN_REGISTRY}"
    mirror-by-digest-only = false
    insecure = false
    blocked = false

    [[registry.mirror]]
      location = "${BREW_REGISTRY}"
      insecure = false

  [[registry]]
    prefix = ""
    location = "${OFFICIAL_REGISTRY}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}"
    mirror-by-digest-only = false
    insecure = false
    blocked = false

    [[registry.mirror]]
      location = "${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}"
      insecure = false

    [[registry.mirror]]
      location = "${local_registry_path}"
      insecure = false

  [[registry]]
    prefix = ""
    location = "${STAGING_REGISTRY}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}"
    mirror-by-digest-only = false
    insecure = false
    blocked = false

    [[registry.mirror]]
      location = "${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}"
      insecure = false

    [[registry.mirror]]
      location = "${local_registry_path}"
      insecure = false

  [[registry]]
    prefix = ""
    location = "${CATALOG_REGISTRY}/${CATALOG_IMAGE_PREFIX}/${CATALOG_IMAGE_IMPORT_PATH}"
    mirror-by-digest-only = true
    insecure = false
    blocked = false

    [[registry.mirror]]
      location = "${OFFICIAL_REGISTRY}/${CATALOG_IMAGE_PREFIX}/${CATALOG_IMAGE_IMPORT_PATH}"
      insecure = false

  [[registry]]
    prefix = ""
    location = "${OFFICIAL_REGISTRY}/${QUAY_IMAGE_MCE_PREFIX}"
    mirror-by-digest-only = false
    insecure = false
    blocked = false

    [[registry.mirror]]
      location = "${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${QUAY_IMAGE_MCE_PREFIX}"
      insecure = false

    [[registry.mirror]]
      location = "${QUAY_REGISTRY}/${QUAY_IMAGE_IMPORT_PATH}/${QUAY_IMAGE_MCE_PREFIX}"
      insecure = false
EOF

  )

  TITLE "Enabling auto-reboot of ${node_type} when changing Machine Config Pool:"
  ${OC} patch --type=merge --patch='{"spec":{"paused":false}}' "machineconfigpool/${node_type}"

  local ocp_version
  ocp_version=$(${OC} version | awk '/Server Version/ { print $3 }')
  echo -e "\n# Checking API ignition version for OCP version: $ocp_version"

  ignition_version=$(${OC} extract -n openshift-machine-api secret/worker-user-data --keys=userData --to=- | grep -oP '(?s)(?<=version":")[0-9\.]+(?=")')

  TITLE "Updating Registry in ${node_type} Machine configuration of cluster '$cluster_name', via OCP API Ignition version: $ignition_version"

  local nodes_conf
  nodes_conf="$(mktemp)_${node_type}.yaml"

  cat <<-EOF > "$nodes_conf"
  apiVersion: machineconfiguration.openshift.io/v1
  kind: MachineConfig
  metadata:
    labels:
      machineconfiguration.openshift.io/role: ${node_type}
    name: 99-${node_type}-submariner-registries
  spec:
    config:
      ignition:
        version: ${ignition_version}
      storage:
        files:
        - contents:
            source: data:text/plain,${config_source}
          filesystem: root
          mode: 0420
          path: /etc/containers/registries.conf.d/submariner-registries.conf
EOF

  ${OC} apply --dry-run='server' -f "$nodes_conf" | highlight "unchanged" \
  || ${OC} apply -f "$nodes_conf"

}

# ------------------------------------------

function upload_submariner_images_to_cluster_registry() {
# Upload custom images to the registry of the requested OCP cluster
  trap_to_debug_commands;

  local kubeconfig_file="$1"

  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Upload custom Submariner ${SUBM_VER_TAG} images to the registry of cluster $cluster_name"

  local image_path_prefix
  if check_version_greater_or_equal "${SUBM_VER_TAG}" "0.12" ; then
    image_path_prefix="${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX}-"
  else
    image_path_prefix="${BREW_REGISTRY}/${REGISTRY_IMAGE_IMPORT_PATH}/${REGISTRY_IMAGE_PREFIX_TECH_PREVIEW}-"
  fi

  TITLE "Overriding submariner images with custom images from mirror registry (Brew):
  Source registry path: ${image_path_prefix}
  Images version tag: ${SUBM_VER_TAG}
  "

  create_namespace "$SUBM_NAMESPACE"

  local import_image_output
  import_image_output="$(mktemp)_import_image"

  for img in \
    $SUBM_IMG_GATEWAY \
    $SUBM_IMG_ROUTE \
    $SUBM_IMG_NETWORK \
    $SUBM_IMG_LIGHTHOUSE \
    $SUBM_IMG_COREDNS \
    $SUBM_IMG_GLOBALNET \
    $SUBM_IMG_OPERATOR \
    $SUBM_IMG_BUNDLE \
    ; do
      local img_source="${image_path_prefix}${img}:${SUBM_VER_TAG}"
      TITLE "Importing image '${img}:${SUBM_VER_TAG}' from a mirror OCP registry: \n ${img_source}"

      local cmd="${OC} import-image -n ${SUBM_NAMESPACE} ${img}:${SUBM_VER_TAG} --from=${img_source} --confirm"

      # Wait (silently) for import image to complete
      watch_and_retry "$cmd &> $import_image_output" 3m || :
      
      echo -e "\n# Verify that the imported image '${img}' has correct version tag: ${SUBM_VER_TAG} \n\n"

      highlight "Image Name:\s+${img}:${SUBM_VER_TAG}" "$import_image_output" # e.g. Image Name:	submariner-route-agent-rhel8:v0.13.0
      grep "version=${SUBM_VER_TAG}" "$import_image_output" # e.g. version=v0.13.0
      
  done

}

# ------------------------------------------

function run_subctl_join_on_cluster_a() {
# Join Submariner member - OCP cluster A (public)
  PROMPT "Joining cluster A to Submariner Broker"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  run_subctl_join_cmd_from_file "${SUBCTL_JOIN_CLUSTER_A_FILE}"
}

# ------------------------------------------

function run_subctl_join_on_cluster_b() {
# Join Submariner member - OSP cluster B (on-prem)
  PROMPT "Joining cluster B to Submariner Broker"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  run_subctl_join_cmd_from_file "${SUBCTL_JOIN_CLUSTER_B_FILE}"

}

# ------------------------------------------

function run_subctl_join_on_cluster_c() {
# Join Submariner member - cluster C (on-prem)
  PROMPT "Joining cluster C to Submariner Broker"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  run_subctl_join_cmd_from_file "${SUBCTL_JOIN_CLUSTER_C_FILE}"

}

# ------------------------------------------

function run_subctl_join_cmd_from_file() {
# Join Submariner member - of current cluster kubeconfig
  trap_to_debug_commands;

  echo -e "\n# Read subctl join command from file: $1"
  local JOIN_CMD
  JOIN_CMD="$(< "$1")"

  cd "${WORKDIR}" || return 1

  # Process:
  #
  # 1) Loads the broker-info.subm file.
  # 2) Asks the user for any missing information.
  # 3) Deploys the operator.
  # 4) Finds the CIDRs.
  # 5) Creates the Submariner CR (for the operator).
  # 6) Adds the CR to the cluster.
  #
  # Note: You dont need to specify the CIDRs - subctl determines them on its own.
  # The user can specify them if necessary, and subctl will use that information
  # (warning the user if it doesnt match what it determined).

  # export KUBECONFIG="${KUBECONFIG}:${KUBECONF_HUB}"
  ${OC} config view

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"
  TITLE "Executing SubCtl Join command on $cluster_name: \n# ${JOIN_CMD}"

  $JOIN_CMD

}

# ------------------------------------------

function test_submariner_resources_cluster_a() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  test_submariner_resources_status
}

# ------------------------------------------

function test_submariner_resources_cluster_b() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  test_submariner_resources_status
}

# ------------------------------------------

function test_submariner_resources_cluster_c() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  test_submariner_resources_status
}

# ------------------------------------------

function test_submariner_resources_status() {
# Check submariner-gateway on the Operator pod
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"
  local submariner_status=UP

  PROMPT "Testing that Submariner CRDs and resources were created on cluster ${cluster_name}"

  ${OC} get catalogsource -n "${SUBM_NAMESPACE}" --ignore-not-found

  ${OC} get crds | grep submariners || submariner_status=DOWN
      # ...
      # submariners.submariner.io                                   2019-11-28T14:09:56Z

  ${OC} get namespace "${SUBM_NAMESPACE}" -o json  || submariner_status=DOWN

  ${OC} get Submariner -n "${SUBM_NAMESPACE}" -o yaml || submariner_status=DOWN

  ${OC} get all -n "${SUBM_NAMESPACE}" --show-labels || submariner_status=DOWN

  ${OC} get pods -n "${SUBM_NAMESPACE}" | grep "${SUBM_OPERATOR}" \
  |& (! highlight "Error|CrashLoopBackOff|ErrImagePull|No resources found") || submariner_status=DOWN

  if [[ "$submariner_status" == DOWN ]] ; then
    FATAL "Submariner installation failure occurred on $cluster_name.
    Resources/CRDs were not installed, or Submariner pods have crashed."
  fi

}

# ------------------------------------------

function test_public_ip_on_gateway_node() {
# Testing that Submariner Gateway node received public (external) IP
  PROMPT "Testing that Submariner Gateway node received public (external) IP on the Broker cluster A"
  trap_to_debug_commands;

  # Should be run on the Broker cluster
  export KUBECONFIG="${KUBECONF_HUB}"

  local public_ip
  public_ip=$(get_external_ips_of_worker_nodes)
  TITLE "Before VM reboot - Gateway public (external) IP should be: $public_ip"

  BUG "When upgrading Submariner 0.8 to 0.9 - Lighthouse replica-set may fail to start" \
  "No workaround" \
  "https://bugzilla.redhat.com/show_bug.cgi?id=1951587"

  verify_gateway_public_ip "$public_ip"

}

# ------------------------------------------

function test_disaster_recovery_of_gateway_nodes() {
# Check that submariner tunnel works if broker nodes External-IPs (on gateways) is changed
  PROMPT "Testing Disaster Recovery: Reboot Submariner Gateway VM on the Broker cluster A, to verify re-allocation of public (external) IP"
  trap_to_debug_commands;

  aws --version || FATAL "AWS-CLI is missing. Try to run again with option '--config-aws-cli'"

  # Should be run on the Broker cluster
  export KUBECONFIG="${KUBECONF_HUB}"

  local ocp_infra_id
  ocp_infra_id="$(${OC} get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster)"
  # e.g. nmanos-aws-devcluster-dzzxk

  TITLE "Get all AWS running VMs, that were assigned as 'submariner-gw' in OCP cluster $CLUSTER_A_NAME (InfraID '${ocp_infra_id}')"

  local cmd="aws ec2 describe-instances --filters \
  Name=tag:Name,Values=${ocp_infra_id}-submariner-gw-* \
  Name=instance-state-name,Values=running \
  --output text --query Reservations[*].Instances[*].InstanceId \
  | sed -r 's/\s+//g'"

  watch_and_retry "$cmd > '$TEMP_FILE'" 5m || aws_reboot=FAILED

  local gateway_aws_instance_ids
  gateway_aws_instance_ids=$(< "$TEMP_FILE")

  [[ -n "$gateway_aws_instance_ids" ]] || \
  FATAL "No running VM instances of '${ocp_infra_id}-submariner-gw' in OCP cluster $CLUSTER_A_NAME. \n\
  subctl cloud-prerpare (via Submariner Addon) might have failed labeling a Gateway node"

  echo -e "\n# Stopping all AWS VMs of 'submariner-gw' in OCP cluster $CLUSTER_A_NAME: [${gateway_aws_instance_ids}]"
  aws "${DEBUG_FLAG}" ec2 stop-instances --force --instance-ids "$gateway_aws_instance_ids" || :
  cmd="aws ec2 describe-instances --instance-ids $gateway_aws_instance_ids --output text &> '$TEMP_FILE'"
  watch_and_retry "$cmd ; grep 'STATE' $TEMP_FILE" 10m "stopped" || :

  cat "$TEMP_FILE"

  echo -e "\n# Starting all AWS VMs of 'submariner-gw' in OCP cluster $CLUSTER_A_NAME: [$gateway_aws_instance_ids]"
  aws "${DEBUG_FLAG}" ec2 start-instances --instance-ids "$gateway_aws_instance_ids" || :
  cmd="aws ec2 describe-instances --instance-ids $gateway_aws_instance_ids --output text &> '$TEMP_FILE'"
  watch_and_retry "$cmd ; grep 'STATE' $TEMP_FILE" 10m "running" || aws_reboot=FAILED

  cat "$TEMP_FILE"

  if [[ "$aws_reboot" == FAILED ]] ; then
      FATAL "AWS-CLI reboot VMs of 'submariner-gw' in OCP cluster $CLUSTER_A_NAME has failed"
  fi

}

# ------------------------------------------

function test_renewal_of_gateway_and_public_ip() {
# Testing that Submariner Gateway was re-created with new public IP
  PROMPT "Testing that Submariner Gateway was re-created with new public IP on the Broker cluster A"
  trap_to_debug_commands;

  # Should be run on the Broker cluster
  export KUBECONFIG="${KUBECONF_HUB}"

  TITLE "Watching Submariner Gateway pod - It should create new Gateway:"

  local active_gateway_pod
  export_variable_name_of_active_gateway_pod "active_gateway_pod"

  local regex="All controllers stopped or exited"
  # Watch submariner-gateway pod logs for 200 (10 X 20) seconds
  watch_pod_logs "$active_gateway_pod" "${SUBM_NAMESPACE}" "$regex" 10 || :

  local public_ip
  public_ip=$(get_external_ips_of_worker_nodes)
  echo -e "\n\n# The new Gateway public (external) IP should be: $public_ip \n"
  verify_gateway_public_ip "$public_ip"

}

# ------------------------------------------

function export_variable_name_of_active_gateway_pod() {
# Set the variable value for the active gateway pod
  # trap_to_debug_commands;

  # Get variable name
  local var_name="${1}"

  # Optional: Timeout to wait for active gateway pod (3 minutes by default)
  local wait_for_gw="${2:-3m}"

  # Optional: Do not print detailed output (silent echo)
  local silent="${3:-no}"

  local gateways_output
  gateways_output="$(mktemp)_gateways"

  [[ "$silent" =~ ^(y|yes)$ ]] || \
  TITLE "Wait for Submariner active gateway, and set it into variable '${var_name}'"

  [[ -x "$(command -v subctl)" ]] || FATAL "No SubCtl installation found. Try to run again with option '--subctl-version'"

  # Wait (silently) for an active gateway node
  watch_and_retry "subctl show gateways &> $gateways_output ; grep 'active' $gateways_output" "$wait_for_gw" || :

  [[ "$silent" =~ ^(y|yes)$ ]] || \
  TITLE "Show Submariner Gateway nodes, and get the active one"

  [[ "$silent" =~ ^(y|yes)$ ]] || \
  highlight "active" "$gateways_output"

  local active_gateway_node
  active_gateway_node="$(awk '/active/ {print $1}' "$gateways_output")"

  # Define label for the search of a gateway pod
  local gw_label='app=submariner-gateway'
  # For SubCtl <= 0.8 : 'app=submariner-engine' is expected as the Gateway pod label
  [[ $(subctl version | grep --invert-match "v0.8") ]] || gw_label="app=submariner-engine"

  [[ "$silent" =~ ^(y|yes)$ ]] || \
  TITLE "Find Submariner Gateway pod that runs on the active node: $active_gateway_node"
  ${OC} get pod -n "${SUBM_NAMESPACE}" -l $gw_label -o wide > "$gateways_output"

  [[ "$silent" =~ ^(y|yes)$ ]] || \
  cat "$gateways_output"

  local gw_id
  gw_id="$(grep "$active_gateway_node" "$gateways_output" | cut -d ' ' -f 1)"

  [[ "$silent" =~ ^(y|yes)$ ]] || \
  highlight "${gw_id}" "$gateways_output"

  [[ "$silent" =~ ^(y|yes)$ ]] || \
  echo -e "\n# Eval and export the variable '${var_name}=${gw_id}'"

  local eval_cmd="export ${var_name}=${gw_id}"
  eval "$eval_cmd"
}

# ------------------------------------------

function verify_gateway_public_ip() {
# sub-function for test_disaster_recovery_of_gateway_nodes functions
  trap_to_debug_commands;

  local public_ip="$1"

  # Show worker node EXTERNAL-IP
  ${OC} get nodes -l node-role.kubernetes.io/worker -o wide |& highlight "EXTERNAL-IP"

  # Show Submariner Gateway public_ip
  cmd="${OC} describe Gateway -n ${SUBM_NAMESPACE} | grep -C 12 'Local Endpoint:'"
  local regex="public_ip:\s*${public_ip}"
  # Attempt cmd for 3 minutes (grepping for 'Local Endpoint:' and print 12 lines afterwards), looking for Public IP
  watch_and_retry "$cmd" 3m "$regex"

}

# ------------------------------------------

function test_cable_driver_cluster_a() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  test_submariner_cable_driver
}

# ------------------------------------------

function test_cable_driver_cluster_b() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  test_submariner_cable_driver
}

# ------------------------------------------

function test_cable_driver_cluster_c() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  test_submariner_cable_driver
}

# ------------------------------------------

function test_submariner_cable_driver() {
# Check submariner cable driver
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Testing Cable-Driver ${SUBM_CABLE_DRIVER:+\"$SUBM_CABLE_DRIVER\" }on ${cluster_name}"

  export_variable_name_of_active_gateway_pod "active_gateway_pod"

  local regex="(cable.* started|Status:connected)"
  # Watch submariner-gateway pod logs for 200 (10 X 20) seconds
  watch_pod_logs "$active_gateway_pod" "${SUBM_NAMESPACE}" "$regex" 10

}

# ------------------------------------------

function test_ha_status_cluster_a() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  test_ha_status
}

# ------------------------------------------

function test_ha_status_cluster_b() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  test_ha_status
}

# ------------------------------------------

function test_ha_status_cluster_c() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  test_ha_status
}

# ------------------------------------------

function test_ha_status() {
# Check submariner HA status
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"
  local submariner_status=UP

  PROMPT "Check HA status of Submariner and Gateway resources on ${cluster_name}"

  ${OC} describe configmaps -n openshift-dns || submariner_status=DOWN

  ${OC} get clusters -n "${SUBM_NAMESPACE}" -o wide || submariner_status=DOWN

  echo -e "\n# TODO: Need to get current cluster ID"
  #${OC} describe cluster "${cluster_id}" -n ${SUBM_NAMESPACE} || submariner_status=DOWN

  local cmd="${OC} describe Gateway -n ${SUBM_NAMESPACE} &> '$TEMP_FILE'"

  TITLE "Checking 'Gateway' resource status"
  local regex="Ha Status:\s*active"
  # Attempt cmd for 3 minutes (grepping for 'Connections:' and print 30 lines afterwards), looking for HA active
  watch_and_retry "$cmd ; grep -E '$regex' $TEMP_FILE" 3m || :
  highlight "$regex" "$TEMP_FILE" || submariner_status=DOWN

  TITLE "Checking 'Submariner' resource status"
  local regex="Status:\s*connect"
  # Attempt cmd for 3 minutes (grepping for 'Connections:' and print 30 lines afterwards), looking for Status connected
  watch_and_retry "$cmd ; grep -E '$regex' $TEMP_FILE" 3m || :
  # ! highlight "Status:\s*connected" "$TEMP_FILE" || submariner_status=DOWN
  ! highlight "Status Failure\s*\w+" "$TEMP_FILE" || submariner_status=DOWN

  if [[ "$submariner_status" == DOWN ]] ; then
    FATAL "Submariner HA failure occurred."
  fi

}

# ------------------------------------------

function test_submariner_connection_cluster_a() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  test_submariner_connection_established
}

# ------------------------------------------

function test_submariner_connection_cluster_b() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  test_submariner_connection_established
}

# ------------------------------------------

function test_submariner_connection_cluster_c() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  test_submariner_connection_established
}

# ------------------------------------------

function test_submariner_connection_established() {
# Check submariner cable driver
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Check Submariner Gateway established connection on ${cluster_name}"

  export_variable_name_of_active_gateway_pod "active_gateway_pod"

  TITLE "Tailing logs in Submariner-Gateway pod [$active_gateway_pod] to verify connection between clusters"
  # ${OC} logs $active_gateway_pod -n ${SUBM_NAMESPACE} | grep "received packet" -C 2 || submariner_status=DOWN

  local regex="(Successfully installed Endpoint cable .* remote IP|Status:connected|CableName:.*[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)"
  # Watch submariner-gateway pod logs for 400 (20 X 20) seconds
  watch_pod_logs "$active_gateway_pod" "${SUBM_NAMESPACE}" "$regex" 20 || submariner_status=DOWN

  ${OC} describe pod "$active_gateway_pod" -n "${SUBM_NAMESPACE}" || submariner_status=DOWN

  [[ "$submariner_status" != DOWN ]] || FATAL "Submariner clusters are not connected."
}

# ------------------------------------------

function test_ipsec_status_cluster_a() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  test_ipsec_status
}

# ------------------------------------------

function test_ipsec_status_cluster_b() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  test_ipsec_status
}

# ------------------------------------------

function test_ipsec_status_cluster_c() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  test_ipsec_status
}

# ------------------------------------------

function test_ipsec_status() {
# Check submariner cable driver
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Testing IPSec Status of the Active Gateway in ${cluster_name}"

  export_variable_name_of_active_gateway_pod "active_gateway_pod"

  : > "$TEMP_FILE"

  TITLE "Verify IPSec status on the active Gateway pod [${active_gateway_pod}]:"
  ${OC} exec "$active_gateway_pod" -n "${SUBM_NAMESPACE}" -- bash -c "ipsec status" |& tee -a "$TEMP_FILE" || :

  local loaded_con
  loaded_con="$(grep "Total IPsec connections:" "$TEMP_FILE" | grep -Po "loaded \K([0-9]+)" | tail -1)"
  local active_con
  active_con="$(grep "Total IPsec connections:" "$TEMP_FILE" | grep -Po "active \K([0-9]+)" | tail -1)"

  if [[ "$active_con" != "$loaded_con" ]] ; then
    FATAL "IPSec tunnel error: $loaded_con Loaded connections, but only $active_con Active"
  fi

}

# ------------------------------------------

function test_globalnet_status_cluster_a() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  test_globalnet_status
}

# ------------------------------------------

function test_globalnet_status_cluster_b() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  test_globalnet_status
}

# ------------------------------------------

function test_globalnet_status_cluster_c() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  test_globalnet_status
}

# ------------------------------------------

function test_globalnet_status() {
  # Check Globalnet controller pod status
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Testing GlobalNet controller, Global IPs and Endpoints status on ${cluster_name}"

  # globalnet_pod=$(${OC} get pod -n ${SUBM_NAMESPACE} -l app=submariner-globalnet -o jsonpath="{.items[0].metadata.name}")
  # [[ -n "$globalnet_pod" ]] || globalnet_status=DOWN
  globalnet_pod="$(get_running_pod_by_label 'app=submariner-globalnet' "$SUBM_NAMESPACE" )"


  TITLE "Tailing logs in GlobalNet pod [$globalnet_pod] to verify that Global IPs were allocated to cluster services"

  local regex="(Allocating globalIp|Starting submariner-globalnet)"
  # Watch globalnet pod logs for 200 (10 X 20) seconds
  watch_pod_logs "$globalnet_pod" "${SUBM_NAMESPACE}" "$regex" 10 || globalnet_status=DOWN

  TITLE "Tailing logs in GlobalNet pod [$globalnet_pod], to see if Endpoints were removed (due to Submariner Gateway restarts)"

  regex="remove endpoint"
  (! watch_pod_logs "$globalnet_pod" "${SUBM_NAMESPACE}" "$regex" 1 "1s") || globalnet_status=DOWN

  [[ "$globalnet_status" != DOWN ]] || FATAL "GlobalNet pod error on ${SUBM_NAMESPACE} namespace, or globalIp / Endpoints failure occurred."
}

# ------------------------------------------

function export_nginx_default_namespace_managed_cluster() {
  PROMPT "Create ServiceExport for $NGINX_CLUSTER_BC on managed cluster, without specifying Namespace"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_MANAGED}"

  configure_namespace_for_submariner_tests

  local current_namespace
  current_namespace="$(${OC} config view --minify -o jsonpath='{..namespace}')"

  TITLE "KUBECONFIG current context is set to the namespace '${current_namespace}', which should include Ngnix service '$NGINX_CLUSTER_BC'"

  ${OC} config get-contexts

  ${OC} get svc -o wide

  ${OC} describe svc "$NGINX_CLUSTER_BC"

  TITLE "The ServiceExport should be created in the current Namespace '${current_namespace}', if exporting service without specifying a Namespace:"

  # export_service_in_lighthouse "$NGINX_CLUSTER_BC"

  BUG "Subctl export service uses submariner-operator namespace instead of the current context namespace" \
  "Explicitly set target namespace or use --kubeconfig" \
  "https://bugzilla.redhat.com/show_bug.cgi?id=2064344"
  # Workaound:
  export_service_in_lighthouse "$NGINX_CLUSTER_BC" "${current_namespace}"

}

# ------------------------------------------

function export_nginx_headless_namespace_managed_cluster() {
  PROMPT "Create ServiceExport for the HEADLESS $NGINX_CLUSTER_BC on managed cluster, in the Namespace '$HEADLESS_TEST_NS'"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_MANAGED}"

  echo -e "\n# The ServiceExport should be created on the default Namespace, as configured in KUBECONFIG:
  \n# $KUBECONFIG : ${HEADLESS_TEST_NS}"

  export_service_in_lighthouse "$NGINX_CLUSTER_BC" "$HEADLESS_TEST_NS"
}

# ------------------------------------------

function export_service_in_lighthouse() {
  trap_to_debug_commands;
  local svc_name="$1"

  # Optional args:
  local namespace="$2"

  subctl export service -h

  TITLE "Exporting the following service $svc_name :"

  BUG "Subctl export fails if same service name is already exported" \
  "Delete the Service Export object corresponding to that Service, before creating it" \
  "https://bugzilla.redhat.com/show_bug.cgi?id=2066085"
  # Workaound:
  ${OC} delete serviceexport "${svc_name}" ${namespace:+-n $namespace} --ignore-not-found

  subctl export service "${svc_name}" --kubeconfig "${KUBECONFIG}" ${namespace:+-n $namespace}

  ${OC} describe svc "${svc_name}" ${namespace:+-n $namespace}

  TITLE "Wait up to 3 minutes for $svc_name to successfully sync to the broker:"

  ${OC} wait --timeout=3m --for=condition=Valid serviceexport "${svc_name}" ${namespace:+-n $namespace}

  BUG "kubectl get serviceexport with '-o wide' does not show more info" \
  "Use 'describe serviceexport' instead" \
  "https://github.com/submariner-io/submariner/issues/739"
  # Workaround:
  local cmd="${OC} describe serviceexport $svc_name ${namespace:+-n $namespace}"

  # BUG:
  # local regex='Status:\s+True'
  local regex='Message:.*successfully synced'
  watch_and_retry "$cmd" 3m "$regex"

  TITLE "Verify that $svc_name ServiceExport status is Valid:"
  ${OC} get serviceexport "${svc_name}" ${namespace:+-n $namespace}
  ${OC} get serviceexport "$svc_name" ${namespace:+-n $namespace} -o jsonpath='{.status.conditions[?(@.status=="True")].type}' | grep "Valid"

  echo -e "\n# Show all exported services:"
  ${OC} get serviceexport -A -o wide

  # Only if GlobalNet is enabled - there should be global IPs allocated after the service export
  if [[ "$GLOBALNET" =~ ^(y|yes)$ ]] ; then

    TITLE "Show GlobalNet Ingress IPs:"
    ${OC} get globalingressips -A -o wide || :
    ${OC} describe globalingressips ${namespace:+-n $namespace} || :
  fi

}

# ------------------------------------------

function test_lighthouse_status_cluster_a() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  test_lighthouse_status
}

# ------------------------------------------

function test_lighthouse_status_cluster_b() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  test_lighthouse_status
}

# ------------------------------------------

function test_lighthouse_status_cluster_c() {
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  test_lighthouse_status
}

# ------------------------------------------

function test_lighthouse_status() {
  # Check Lighthouse (the pod for service-discovery) status
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Testing Lighthouse agent status on ${cluster_name}"

  # lighthouse_pod=$(${OC} get pod -n ${SUBM_NAMESPACE} -l app=submariner-lighthouse-agent -o jsonpath="{.items[0].metadata.name}")
  # [[ -n "$lighthouse_pod" ]] || FATAL "Lighthouse pod was not created on ${SUBM_NAMESPACE} namespace."
  lighthouse_pod="$(get_running_pod_by_label 'app=submariner-lighthouse-agent' "$SUBM_NAMESPACE" )"

  TITLE "Tailing logs in Lighthouse pod [$lighthouse_pod] to verify Service-Discovery sync with Broker"
  local regex="agent .* started"
  # Watch lighthouse pod logs for 100 (5 X 20) seconds
  watch_pod_logs "$lighthouse_pod" "${SUBM_NAMESPACE}" "$regex" 5 || FAILURE "Lighthouse status is not as expected"

  echo -e "\n# TODO: Can also test app=submariner-lighthouse-coredns  for the lighthouse DNS status"
}


# ------------------------------------------

# Test for Submariner < 0.12 with old GlobalNet V1 (deprecated)
function test_global_ip_created_for_svc_or_pod() {
  # Check that the Service or Pod was annotated with GlobalNet IP
  # Set external variable GLOBAL_IP if there's a GlobalNet IP
  trap_to_debug_commands

  obj_type="$1" # "svc" for Service, "pod" for Pod
  obj_id="$2" # Object name or id
  namespace="$3" # Optional : namespace

  cmd="${OC} describe $obj_type $obj_id ${namespace:+-n $namespace}"

  globalnet_tag='submariner.io\/globalIp'
  ipv4_regex='[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+'
  watch_and_retry "$cmd | grep '$globalnet_tag'" 3m "$ipv4_regex"

  $cmd | highlight "$globalnet_tag" || \
  BUG "GlobalNet annotation and IP was not set on $obj_type : $obj_id ${namespace:+(namespace : $namespace)}"

  # Set the external variable $GLOBAL_IP with the GlobalNet IP
  # GLOBAL_IP=$($cmd | grep -E "$globalnet_tag" | awk '{print $NF}')
  GLOBAL_IP=$($cmd | grep "$globalnet_tag" | grep -Eoh "$ipv4_regex")
  export GLOBAL_IP
}

# ------------------------------------------

function test_clusters_connected_by_service_ip() {
  PROMPT "After Submariner is installed:
  Identify Netshoot pod on cluster A, and Nginx service on managed cluster"
  trap_to_debug_commands;

  export KUBECONFIG="${KUBECONF_HUB}"
  # ${OC} get pods -l run=${NETSHOOT_CLUSTER_A} ${TEST_NS:+-n $TEST_NS} --field-selector status.phase=Running | awk 'FNR == 2 {print $1}' > "$TEMP_FILE"
  # netshoot_pod_cluster_a="$(< $TEMP_FILE)"
  netshoot_pod_cluster_a="$(get_running_pod_by_label "run=${NETSHOOT_CLUSTER_A}" "$TEST_NS" )"

  echo -e "\n# NETSHOOT_CLUSTER_A: $netshoot_pod_cluster_a"
    # netshoot-785ffd8c8-zv7td

  export KUBECONFIG="${KUBECONF_MANAGED}"
  echo "${OC} get svc -l app=${NGINX_CLUSTER_BC} ${TEST_NS:+-n $TEST_NS} | awk 'FNR == 2 {print $3}')"
  # nginx_IP_cluster_bc=$(${OC} get svc -l app=${NGINX_CLUSTER_BC} ${TEST_NS:+-n $TEST_NS} | awk 'FNR == 2 {print $3}')
  ${OC} get svc -l app="${NGINX_CLUSTER_BC}" ${TEST_NS:+-n $TEST_NS} | awk 'FNR == 2 {print $3}' > "$TEMP_FILE"
  nginx_IP_cluster_bc="$(< "$TEMP_FILE")"
  TITLE "Nginx service on cluster B/C, will be identified by its IP (without DNS from service-discovery): ${nginx_IP_cluster_bc}:${NGINX_PORT}"
    # nginx_IP_cluster_bc: 100.96.43.129

  export KUBECONFIG="${KUBECONF_HUB}"

  local curl_cmd
  curl_cmd=(${OC:+$OC} exec "${netshoot_pod_cluster_a}" ${TEST_NS:+-n $TEST_NS} -- curl --output /dev/null --max-time 30 --verbose "${nginx_IP_cluster_bc}:${NGINX_PORT}")
  
  if [[ ! "$GLOBALNET" =~ ^(y|yes)$ ]] ; then
    PROMPT "Testing connection without GlobalNet: From Netshoot on OCP cluster A (public), to Nginx service IP on managed cluster"

    echo -e "\n# Running: ${curl_cmd[*]}"

    if ! "${curl_cmd[@]}" ; then
      FAILURE "Submariner connection failure${SUBM_CABLE_DRIVER:+ (Cable-driver=$SUBM_CABLE_DRIVER)}.
      \n Did you install clusters with overlapping CIDRs ?"
    fi

      # *   Trying 100.96.72.226:8080...
      # * TCP_NODELAY set
      # * Connected to 100.96.72.226 (100.96.72.226) port 8080 (#0)
      # > HEAD / HTTP/1.1
      # > Host: 100.96.72.226
      # > User-Agent: curl/7.65.1
      # > Accept: */*
      # >
      # * Mark bundle as not supporting multiuse
      # < HTTP/1.1 200 OK
      # < Server: nginx/1.17.6
      # < Date: Thu, 05 Dec 2019 20:54:17 GMT
      # < Content-Type: text/html
      # < Content-Length: 612
      # < Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT
      # < Connection: keep-alive
      # < ETag: "5dd406e1-264"
      # < Accept-Ranges: bytes
      # <
      # * Connection #0 to host 100.96.72.226 left intact

  else
    PROMPT "Testing GlobalNet: There should be NO-connectivity if clusters A and B have Overlapping CIDRs"

    msg="# Negative Test - Clusters have Overlapping CIDRs:
    \n# Nginx internal IP (${nginx_IP_cluster_bc}:${NGINX_PORT}) on cluster B/C, should NOT be reachable outside cluster, if using GlobalNet."

    "${curl_cmd[@]}" |& (! highlight "Failed to connect|Connection timed out" && FAILURE "$msg") || echo -e "$msg"
  fi
}

# ------------------------------------------

# Test for Submariner < 0.12 with old GlobalNet V1 (deprecated)
function test_clusters_connected_overlapping_cidrs_globalnet_v1() {
### Run Connectivity tests between the On-Premise and Public clusters ###
# To validate that now Submariner made the connection possible!
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"
  PROMPT "Testing GlobalNet annotation - Nginx service on managed cluster $cluster_name should get a GlobalNet IP"

  export KUBECONFIG="${KUBECONF_MANAGED}"

  # Should fail if NGINX_CLUSTER_BC was not annotated with GlobalNet IP
  GLOBAL_IP=""
  test_global_ip_created_for_svc_or_pod svc "$NGINX_CLUSTER_BC" $TEST_NS || :
  [[ -n "$GLOBAL_IP" ]] || FAILURE "GlobalNet error on Nginx service (${NGINX_CLUSTER_BC}${TEST_NS:+.$TEST_NS})"
  nginx_global_ip="$GLOBAL_IP"

  PROMPT "Testing GlobalNet annotation - Netshoot pod on OCP cluster A (public) should get a GlobalNet IP"
  export KUBECONFIG="${KUBECONF_HUB}"
  # netshoot_pod_cluster_a=$(${OC} get pods -l run=${NETSHOOT_CLUSTER_A} ${TEST_NS:+-n $TEST_NS} \
  # --field-selector status.phase=Running | awk 'FNR == 2 {print $1}')
  netshoot_pod_cluster_a="$(get_running_pod_by_label "run=${NETSHOOT_CLUSTER_A}" "$TEST_NS" )"

  # Should fail if netshoot_pod_cluster_a was not annotated with GlobalNet IP
  GLOBAL_IP=""
  test_global_ip_created_for_svc_or_pod pod "$netshoot_pod_cluster_a" $TEST_NS || :
  [[ -n "$GLOBAL_IP" ]] || FAILURE "GlobalNet error on Netshoot Pod (${netshoot_pod_cluster_a}${TEST_NS:+ in $TEST_NS})"
  netshoot_global_ip="$GLOBAL_IP"

  echo -e "\n# TODO: Ping to the netshoot_global_ip"


  PROMPT "Testing GlobalNet connectivity - From Netshoot pod ${netshoot_pod_cluster_a} (IP ${netshoot_global_ip}) on cluster A
  To Nginx service on cluster B/C, by its Global IP: $nginx_global_ip:${NGINX_PORT}"

  export KUBECONFIG="${KUBECONF_HUB}"
  ${OC} exec "${netshoot_pod_cluster_a}" ${TEST_NS:+-n $TEST_NS} \
  -- curl --output /dev/null --max-time 30 --verbose "${nginx_global_ip}:${NGINX_PORT}"

  echo -e "\n# TODO: validate annotation of globalIp in the node"
}

# ------------------------------------------

function test_clusters_connected_full_domain_name() {
### Nginx service on cluster B/C, will be identified by its Domain Name ###
# This is to test service-discovery (Lighthouse) of NON-headless $NGINX_CLUSTER_BC service, on the default namespace

  trap_to_debug_commands;

  # Set FQDN on clusterset.local when using Service-Discovery (lighthouse)
  local nginx_cl_b_dns="${NGINX_CLUSTER_BC}${TEST_NS:+.$TEST_NS}.svc.${MULTI_CLUSTER_DOMAIN}"

  PROMPT "Testing Service-Discovery: From Netshoot pod on cluster A${TEST_NS:+ (Namespace $TEST_NS)}
  To the default Nginx service on cluster B/C${TEST_NS:+ (Namespace ${TEST_NS:-default})}, by DNS hostname: $nginx_cl_b_dns"

  export KUBECONFIG="${KUBECONF_HUB}"

  TITLE "Try to ping ${NGINX_CLUSTER_BC} until getting expected FQDN: $nginx_cl_b_dns (and IP)"
  echo -e "\n# TODO: Validate both GlobalIP and svc.${MULTI_CLUSTER_DOMAIN} with   ${OC} get all"
      # NAME                 TYPE           CLUSTER-IP   EXTERNAL-IP                            PORT(S)   AGE
      # service/kubernetes   clusterIP      172.30.0.1   <none>                                 443/TCP   39m
      # service/openshift    ExternalName   <none>       kubernetes.default.svc.clusterset.local   <none>    32m

  cmd="${OC} exec ${NETSHOOT_CLUSTER_A} ${TEST_NS:+-n $TEST_NS} -- ping -c 1 $nginx_cl_b_dns"
  local regex="PING ${nginx_cl_b_dns}"
  watch_and_retry "$cmd" 3m "$regex"
    # PING netshoot-cl-a-new.test-submariner-new.svc.clusterset.local (169.254.59.89)

  BUG "LibreSwan may fail here, but subctl may not warn on sub-connection problem" \
   "No workaround yet, it is caused by LibreSwan bug 1081" \
  "https://github.com/submariner-io/submariner/issues/1080"
  # No workaround yet

  TITLE "Try to CURL from ${NETSHOOT_CLUSTER_A} to ${nginx_cl_b_dns}:${NGINX_PORT} :"
  ${OC} exec "${NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS} -- /bin/bash -c "curl --max-time 30 --verbose ${nginx_cl_b_dns}:${NGINX_PORT}"

  echo -e "\n# TODO: Test connectivity with https://github.com/tsliwowicz/go-wrk"

}

# ------------------------------------------

function test_clusters_cannot_connect_short_service_name() {
### Negative test for nginx_cl_b_short_dns FQDN ###

  trap_to_debug_commands;

  local nginx_cl_b_short_dns="${NGINX_CLUSTER_BC}${TEST_NS:+.$TEST_NS}"

  PROMPT "Testing Service-Discovery:
  There should be NO DNS resolution from cluster A to the local Nginx address on cluster B/C: $nginx_cl_b_short_dns (FQDN without \"clusterset\")"

  export KUBECONFIG="${KUBECONF_HUB}"

  msg="# Negative Test - ${nginx_cl_b_short_dns}:${NGINX_PORT} should not be reachable (FQDN without \"clusterset\")."

  ${OC} exec "${NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS} \
  -- /bin/bash -c "curl --max-time 30 --verbose ${nginx_cl_b_short_dns}:${NGINX_PORT}" \
  |& (! highlight "command terminated with exit code" && FATAL "$msg") || echo -e "$msg"
    # command terminated with exit code 28
}

# ------------------------------------------

function install_new_netshoot_cluster_a() {
### Install $NEW_NETSHOOT_CLUSTER_A on the $TEST_NS namespace ###

  trap_to_debug_commands;
  PROMPT "Install NEW Netshoot pod on OCP cluster A${TEST_NS:+ (Namespace $TEST_NS)}"
  export KUBECONFIG="${KUBECONF_HUB}" # Can also use --context ${CLUSTER_A_NAME} on all further oc commands

  [[ -z "$TEST_NS" ]] || create_namespace "$TEST_NS" "privileged"

  ${OC} delete pod "${NEW_NETSHOOT_CLUSTER_A}" --ignore-not-found ${TEST_NS:+-n $TEST_NS} || :

  set_netshoot_image_spec "sleep 5m"
  
  ${OC} run "${NEW_NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS} --image "${NETSHOOT_IMAGE}" \
  --pod-running-timeout=5m --restart=Never --overrides="${NETSHOOT_IMAGE_SPEC}"

  TITLE "Wait up to 3 minutes for NEW Netshoot pod [${NEW_NETSHOOT_CLUSTER_A}] to be ready:"
  ${OC} wait --timeout=3m --for=condition=ready pod -l run="${NEW_NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS}
  ${OC} describe pod "${NEW_NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS}
}

# ------------------------------------------

# Test for Submariner < 0.12 with old GlobalNet V1 (deprecated)
function test_new_netshoot_ip_cluster_a_globalnet_v1() {
### Check that $NEW_NETSHOOT_CLUSTER_A on the $TEST_NS is annotated with GlobalNet IP ###

  trap_to_debug_commands;
  PROMPT "Testing GlobalNet annotation - NEW Netshoot pod on OCP cluster A (public) should get a GlobalNet IP"
  export KUBECONFIG="${KUBECONF_HUB}"

  # netshoot_pod=$(${OC} get pods -l run=${NEW_NETSHOOT_CLUSTER_A} ${TEST_NS:+-n $TEST_NS} \
  # --field-selector status.phase=Running | awk 'FNR == 2 {print $1}')
  # get_running_pod_by_label "run=${NEW_NETSHOOT_CLUSTER_A}" "${TEST_NS}"

  # Should fail if NEW_NETSHOOT_CLUSTER_A was not annotated with GlobalNet IP
  GLOBAL_IP=""
  test_global_ip_created_for_svc_or_pod pod "$NEW_NETSHOOT_CLUSTER_A" $TEST_NS || :
  [[ -n "$GLOBAL_IP" ]] || FAILURE "GlobalNet error on NEW Netshoot Pod (${NEW_NETSHOOT_CLUSTER_A}${TEST_NS:+ in $TEST_NS})"
}

# ------------------------------------------

function install_nginx_headless_namespace_managed_cluster() {
### Install $NGINX_CLUSTER_BC on the $HEADLESS_TEST_NS namespace ###
  trap_to_debug_commands;

  PROMPT "Install HEADLESS Nginx service on managed cluster${HEADLESS_TEST_NS:+ (Namespace $HEADLESS_TEST_NS)}"
  export KUBECONFIG="${KUBECONF_MANAGED}"

  TITLE "Creating ${NGINX_CLUSTER_BC}:${NGINX_PORT} in ${HEADLESS_TEST_NS}, using ${NGINX_IMAGE}, and disabling it's cluster-ip (with '--cluster-ip=None'):"

  install_nginx_service "${NGINX_CLUSTER_BC}" "${NGINX_IMAGE}" "${HEADLESS_TEST_NS}" "--port=${NGINX_PORT} --cluster-ip=None" || :
}

# ------------------------------------------

# Test for Submariner < 0.12 with old GlobalNet V1 (deprecated)
function test_nginx_headless_ip_globalnet_v1() {
### Check that $NGINX_CLUSTER_BC on the $HEADLESS_TEST_NS is annotated with GlobalNet IP ###
  trap_to_debug_commands;

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"
  PROMPT "Testing GlobalNet annotation - The HEADLESS Nginx service on managed cluster $cluster_name should get a GlobalNet IP"

  if [[ "$GLOBALNET" =~ ^(y|yes)$ ]] ; then
    BUG "HEADLESS Service is not supported with GlobalNet" \
     "No workaround yet - Skip the whole test" \
    "https://github.com/submariner-io/lighthouse/issues/273"
    # No workaround yet
    FAILURE "Mark this test as failed, but continue"
  fi

  export KUBECONFIG="${KUBECONF_MANAGED}"

  # Should fail if NGINX_CLUSTER_BC was not annotated with GlobalNet IP
  GLOBAL_IP=""
  test_global_ip_created_for_svc_or_pod svc "$NGINX_CLUSTER_BC" "$HEADLESS_TEST_NS" || :
  [[ -n "$GLOBAL_IP" ]] || FAILURE "GlobalNet error on the HEADLESS Nginx service (${NGINX_CLUSTER_BC}${HEADLESS_TEST_NS:+.$HEADLESS_TEST_NS})"

  echo -e "\n# TODO: Ping to the new_nginx_global_ip"
  # new_nginx_global_ip="$GLOBAL_IP"
}

# ------------------------------------------

function test_clusters_connected_headless_service_on_new_namespace() {
### Nginx service on cluster B/C, will be identified by its Domain Name (with service-discovery) ###

  trap_to_debug_commands;

  # Set FQDN on clusterset.local when using Service-Discovery (lighthouse)
  local nginx_headless_cl_b_dns="${NGINX_CLUSTER_BC}${HEADLESS_TEST_NS:+.$HEADLESS_TEST_NS}.svc.${MULTI_CLUSTER_DOMAIN}"

  PROMPT "Testing Service-Discovery: From NEW Netshoot pod on cluster A${TEST_NS:+ (Namespace $TEST_NS)}
  To the HEADLESS Nginx service on cluster B/C${HEADLESS_TEST_NS:+ (Namespace $HEADLESS_TEST_NS)}, by DNS hostname: $nginx_headless_cl_b_dns"

  if [[ "$GLOBALNET" =~ ^(y|yes)$ ]] ; then

    BUG "HEADLESS Service is not supported with GlobalNet" \
     "No workaround yet - Skip the whole test" \
    "https://github.com/submariner-io/lighthouse/issues/273"
    # No workaround yet - skipping test
    return

  else

    export KUBECONFIG="${KUBECONF_HUB}"

    TITLE "Try to ping HEADLESS ${NGINX_CLUSTER_BC} until getting expected FQDN: $nginx_headless_cl_b_dns (and IP)"
    echo -e "\n# TODO: Validate both GlobalIP and svc.${MULTI_CLUSTER_DOMAIN} with   ${OC} get all"
        # NAME                 TYPE           CLUSTER-IP   EXTERNAL-IP                            PORT(S)   AGE
        # service/kubernetes   clusterIP      172.30.0.1   <none>                                 443/TCP   39m
        # service/openshift    ExternalName   <none>       kubernetes.default.svc.clusterset.local   <none>    32m

    BUG "It may fail resolving Headless service host, that was previously exported (when redeploying Submariner)" \
    "No workaround yet" \
    "https://github.com/submariner-io/submariner/issues/872"

    cmd="${OC} exec ${NEW_NETSHOOT_CLUSTER_A} ${TEST_NS:+-n $TEST_NS} -- ping -c 1 $nginx_headless_cl_b_dns"
    local regex="PING ${nginx_headless_cl_b_dns}"
    watch_and_retry "$cmd" 3m "$regex"
      # PING netshoot-cl-a-new.test-submariner-new.svc.clusterset.local (169.254.59.89)

    TITLE "Try to CURL from ${NEW_NETSHOOT_CLUSTER_A} to ${nginx_headless_cl_b_dns}:${NGINX_PORT} :"
    ${OC} exec "${NEW_NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS} -- /bin/bash -c "curl --max-time 30 --verbose ${nginx_headless_cl_b_dns}:${NGINX_PORT}"

    echo -e "\n# TODO: Test connectivity with https://github.com/tsliwowicz/go-wrk"

  fi

}

# ------------------------------------------

function test_clusters_cannot_connect_headless_short_service_name() {
### Negative test for HEADLESS nginx_cl_b_short_dns FQDN ###

  trap_to_debug_commands;

  local nginx_cl_b_short_dns="${NGINX_CLUSTER_BC}${HEADLESS_TEST_NS:+.$HEADLESS_TEST_NS}"

  PROMPT "Testing Service-Discovery:
  There should be NO DNS resolution from cluster A to the local Nginx address on cluster B/C: $nginx_cl_b_short_dns (FQDN without \"clusterset\")"

  export KUBECONFIG="${KUBECONF_HUB}"

  msg="# Negative Test - ${nginx_cl_b_short_dns}:${NGINX_PORT} should not be reachable (FQDN without \"clusterset\")."

  ${OC} exec "${NETSHOOT_CLUSTER_A}" ${TEST_NS:+-n $TEST_NS} \
  -- /bin/bash -c "curl --max-time 30 --verbose ${nginx_cl_b_short_dns}:${NGINX_PORT}" \
  |& (! highlight "command terminated with exit code" && FATAL "$msg") || echo -e "$msg"
    # command terminated with exit code 28

}

# ------------------------------------------

function test_subctl_show_on_merged_kubeconfigs() {
### Test subctl show command on merged kubeconfig ###
  PROMPT "Testing SubCtl show on merged kubeconfig of multiple clusters"
  trap_to_debug_commands;

  local subctl_info

  export_merged_kubeconfigs

  echo -e "\n# Store SubCtl version in $SUBM_IMAGES_VERSION_FILE"
  subctl show versions > "$SUBM_IMAGES_VERSION_FILE" || subctl_info=ERROR

  highlight "submariner.*${SUBM_VER_TAG}" "$SUBM_IMAGES_VERSION_FILE" || subctl_info=ERROR

  subctl show networks || subctl_info=ERROR

  subctl show endpoints || subctl_info=ERROR

  subctl show connections || subctl_info=ERROR

  subctl show gateways || subctl_info=ERROR

  if [[ "$subctl_info" == ERROR ]] ; then
    BUG "subctl diagnose firewall metrics does not work on merged kubeconfig" \
    "Ignore 'subctl diagnose firewall metrics' output" \
    "https://bugzilla.redhat.com/show_bug.cgi?id=2013711"
    
    FAILURE "SubCtl show failed when using merged kubeconfig"
  fi

}

# ------------------------------------------

function export_merged_kubeconfigs() {
### Helper function to export all active clusters kubeconfig at once (merged) ###
  trap_to_debug_commands;

  TITLE "Exporting all active clusters kubeconfig at once (merged)"

  local merged_kubeconfigs="${KUBECONF_HUB}"
  # local active_context_names="${CLUSTER_A_NAME}"

  if [[ -s "$KUBECONF_CLUSTER_B" ]] ; then
    echo -e "\n# Appending ${CLUSTER_B_NAME} context to \"${merged_kubeconfigs}\""
    merged_kubeconfigs="${merged_kubeconfigs}:${KUBECONF_CLUSTER_B}"
    # active_context_names="${active_context_names}|${CLUSTER_B_NAME}"
  fi

  if [[ -s "$KUBECONF_CLUSTER_C" ]] ; then
    echo -e "\n# Appending ${CLUSTER_C_NAME} context to \"${merged_kubeconfigs}\""
    merged_kubeconfigs="${merged_kubeconfigs}:${KUBECONF_CLUSTER_C}"
    # active_context_names="${active_context_names}|${CLUSTER_C_NAME}"
  fi

  export KUBECONFIG="${merged_kubeconfigs}"
  ${OC} config get-contexts

  # echo -e "\n# Deleting all contexts except \"${active_context_names}\" from current kubeconfig:"
  # local context_changed
  #
  # ${OC} config get-contexts -o name | grep -E --invert-match "^(${active_context_names})\$" \
  # | while read -r context_name ; do
  #   echo -e "\n# Deleting kubeconfig context: $context_name"
  #   ${OC} config delete-context "${context_name}" || :
  #   context_changed=YES
  # done
  #
  # [[ -z "$context_changed" ]] || ${OC} config get-contexts
  #   #   CURRENT   NAME                  CLUSTER               AUTHINFO      NAMESPACE
  #   #   *         nmanos-cluster-a      nmanos-cluster-a      admin         default
  #   #             nmanos-cluster-c      nmanos-cluster-c      admin         default

  echo -e "\n# Current OC user: $(${OC} whoami || : )"

}

# ------------------------------------------

function test_submariner_packages() {
### Run Submariner Unit tests (mock) ###
  PROMPT "Testing Submariner Packages (Unit-Tests) with GO"
  trap_to_debug_commands;

  local junit_output_file="$PKG_JUNIT_XML"

  local project_path="$GOPATH/src/github.com/submariner-io/submariner"
  cd "$project_path" || return 1
  pwd

  export GO111MODULE="on"
  # export CGO_ENABLED=1 # required for go test -race
  go env
  # TODO: Change execution to be run by Gingo binary (from GOBIN)
  # ginkgo version
  # ginkgo help ginkgo

  local test_dir="./pkg/..."
  local msg="# Running unit-tests with GO in project: \n# $project_path/$test_dir
  \n# Ginkgo test parameters: ${test_params[*]}"

  echo -e "$msg \n# Output will be printed both to stdout and to $E2E_LOG file. \n"
  echo -e "$msg" >> "$E2E_LOG"

  local junit_params
  if [[ "$CREATE_JUNIT_XML" =~ ^(y|yes)$ ]]; then
    msg="# Junit report file will be created: \n# $junit_output_file \n"
    echo -e "$msg"
    echo -e "$msg" >> "$E2E_LOG"

    if go test -v $test_dir -ginkgo.dryRun -ginkgo.help | highlight 'junit-report string' ; then
      # For Ginkgo version 2
      junit_params=("-junit-report" "$junit_output_file")
    else
      # For Ginkgo version 1
      junit_params=("-ginkgo.reportFile" "$junit_output_file")
    fi
  fi

  go test -v -cover $test_dir \
  -ginkgo.v -ginkgo.trace -ginkgo.reportPassed "${junit_params[@]}"  \
  | tee -a "$E2E_LOG"

}

# ------------------------------------------

function test_submariner_e2e_with_go() {
# Run E2E Tests of Submariner:
  PROMPT "Testing Submariner End-to-End tests with GO"
  trap_to_debug_commands;

  test_project_e2e_with_go \
  "$GOPATH/src/github.com/submariner-io/submariner" \
  "$E2E_JUNIT_XML"

}

# ------------------------------------------

function test_lighthouse_e2e_with_go() {
# Run E2E Tests of Lighthouse with Ginkgo
  PROMPT "Testing Lighthouse End-to-End tests with GO"
  trap_to_debug_commands;

  local test_globalnet

  if [[ "$GLOBALNET" =~ ^(y|yes)$ ]] ; then
    test_globalnet="--globalnet"
  fi

  test_project_e2e_with_go \
  "$GOPATH/src/github.com/submariner-io/lighthouse" \
  "$LIGHTHOUSE_JUNIT_XML" \
  "$test_globalnet"

}

# ------------------------------------------

function test_project_e2e_with_go() {
# Helper function to run E2E Tests of Submariner repo with Ginkgo
  trap_to_debug_commands;
  local project_path="$1"
  local junit_output_file="$2"
  declare -a "test_params=($3)"

  cd "$project_path" || return 1
  pwd

  TITLE "Set E2E context for the active clusters"
  export KUBECONFIG="${KUBECONF_HUB}"
  local e2e_dp_context

  e2e_dp_context=("--dp-context" "$(${OC} config current-context)")

  if [[ -s "$KUBECONF_CLUSTER_B" ]] ; then
    echo -e "\n# Appending \"${CLUSTER_B_NAME}\" to current E2E context (${e2e_dp_context[*]})"
    export KUBECONFIG="${KUBECONF_CLUSTER_B}"
    e2e_dp_context+=("--dp-context" "$(${OC} config current-context)")
  fi

  if [[ -s "$KUBECONF_CLUSTER_C" ]] ; then
    echo -e "\n# Appending \"${CLUSTER_C_NAME}\" to current E2E context (${e2e_dp_context[*]})"
    export KUBECONFIG="${KUBECONF_CLUSTER_C}"
    e2e_dp_context+=("--dp-context" "$(${OC} config current-context)")
  fi

  echo -e "\n# E2E context: ${e2e_dp_context[*]}"

  ### Set E2E $test_params and $junit_params" ###

  test_params+=("${e2e_dp_context[@]}"
  "--submariner-namespace" "${SUBM_NAMESPACE}"
  "--connection-timeout" 30 "--connection-attempts" 3)

  local test_dir="./test/e2e"
  local msg="# Running End-to-End tests with GO in project: \n# $project_path/$test_dir
  \n# Ginkgo test parameters: ${test_params[*]}"

  echo -e "$msg \n# Output will be printed both to stdout and to $E2E_LOG file. \n"
  echo -e "$msg" >> "$E2E_LOG"

  local junit_params
  if [[ "$CREATE_JUNIT_XML" =~ ^(y|yes)$ ]]; then
    msg="# Junit report file will be created: \n# $junit_output_file \n"
    echo -e "$msg"
    echo -e "$msg" >> "$E2E_LOG"

    if go test -v $test_dir -ginkgo.dryRun -ginkgo.help | highlight 'junit-report string' ; then
      # For Ginkgo version 2
      junit_params=("-junit-report" "$junit_output_file")
    else
      # For Ginkgo version 1
      junit_params=("-ginkgo.reportFile" "$junit_output_file")
    fi
  fi

  local go_cmd="go test -v $test_dir
  -timeout 120m
  -ginkgo.v -ginkgo.trace
  -ginkgo.randomizeAllSpecs
  -ginkgo.noColor
  -ginkgo.reportPassed
  ${junit_params[*]}
  -ginkgo.skip \[redundancy\]
  -args ${test_params[*]}
  "

  TITLE "Running E2E tests with Ginkgo: \n$go_cmd"

  export_merged_kubeconfigs

  export GO111MODULE="on"
  go env
  # TODO: Change execution to be run by Gingo binary (from GOBIN)
  # ginkgo version
  # ginkgo help ginkgo

  $go_cmd | tee -a "$E2E_LOG"

}

# ------------------------------------------

function test_subctl_diagnose_on_merged_kubeconfigs() {
### Test subctl diagnose command on merged kubeconfig ###
  PROMPT "Testing SubCtl diagnose on merged kubeconfig of multiple clusters"
  trap_to_debug_commands;

  local subctl_diagnose

  export_merged_kubeconfigs

  # For SubCtl > 0.8 : Run subctl diagnose:
  if [[ $(subctl version | grep --invert-match "v0.8") ]] ; then

    subctl diagnose deployment || : # Temporarily ignore error

    subctl diagnose connections || subctl_diagnose=ERROR

    subctl diagnose k8s-version || subctl_diagnose=ERROR

    subctl diagnose kube-proxy-mode ${TEST_NS:+--namespace $TEST_NS} || subctl_diagnose=ERROR

    subctl diagnose cni || subctl_diagnose=ERROR

    subctl diagnose firewall intra-cluster --validation-timeout 120 || subctl_diagnose=ERROR

    subctl diagnose firewall inter-cluster "${KUBECONF_HUB}" "${KUBECONF_MANAGED}" --validation-timeout 120 --verbose || subctl_diagnose=ERROR

    BUG "subctl diagnose firewall metrics does not work on merged kubeconfig" \
    "Ignore 'subctl diagnose firewall metrics' output" \
    "https://bugzilla.redhat.com/show_bug.cgi?id=2013711"
    # workaround:
    export KUBECONFIG="${KUBECONF_HUB}"

    subctl diagnose firewall metrics --validation-timeout 120 --verbose || :

    if [[ "$subctl_diagnose" == ERROR ]] ; then
      FAILURE "SubCtl diagnose had some failed checks, please investigate"
    fi

  else
    TITLE "Subctl diagnose command is not supported in $(subctl version)"
  fi

}

# ------------------------------------------

function test_subctl_benchmarks() {
  PROMPT "Testing subctl benchmark: latency and throughput tests"
  trap_to_debug_commands;

  subctl benchmark latency "${KUBECONF_HUB}" "${KUBECONF_MANAGED}" --verbose || benchmark_status=ERROR

  subctl benchmark throughput "${KUBECONF_HUB}" "${KUBECONF_MANAGED}" --verbose || benchmark_status=ERROR

  if [[ "$benchmark_status" == ERROR ]] ; then
    FAILURE "Submariner benchmark tests have ended with failures. \n\
    Possible bug: https://bugzilla.redhat.com/show_bug.cgi?id=1971246"
  fi

}

# ------------------------------------------

function build_submariner_repos() {
### Building latest Submariner code and tests ###
  PROMPT "Building submariner-io repositories for the E2E and unit-tests"
  trap_to_debug_commands;

  verify_golang || FATAL "No Golang compiler found. Try to run again with option '--config-golang'"

  local submariner_version_or_tag="$1"

  TITLE "Retrieve correct branch to pull for Submariner version '$submariner_version_or_tag'"

  if [[ -z "$submariner_version_or_tag" ]] ; then
    echo -e "\n# No Submariner version was specified, retrieving installed version with subctl:"
    subctl show versions > "$SUBM_IMAGES_VERSION_FILE"
    submariner_version_or_tag="$(grep -m 1 "submariner" "$SUBM_IMAGES_VERSION_FILE" | awk '{print $3}')"
  fi

  if [[ "$submariner_version_or_tag" =~ latest|devel ]]; then
    echo -e "\n# Find the latest branch (devel) in Submariner repository:"
    submariner_version_or_tag="$(get_submariner_branch_tag)"
  else
    echo -e "\n# Find branch name that includes '${submariner_version_or_tag}' in Submariner repository:"
    submariner_version_or_tag=$(get_submariner_branch_tag "${submariner_version_or_tag}")
  fi

  build_go_repo "https://github.com/submariner-io/submariner" "$submariner_version_or_tag"

  build_go_repo "https://github.com/submariner-io/lighthouse" "$submariner_version_or_tag"
}

# ------------------------------------------

function build_operator_latest() {  # [DEPRECATED]
### Building latest Submariner-Operator code and SubCtl tool ###
  PROMPT "Building latest Submariner-Operator code and SubCtl tool"
  trap_to_debug_commands;

  verify_golang || FATAL "No Golang compiler found. Try to run again with option '--config-golang'"

  # Install Docker
  # install_local_docker "${WORKDIR}"

  # Delete old submariner-operator directory
  #rm -rf $GOPATH/src/github.com/submariner-io/submariner-operator

  # Download Submariner Operator with go
  # export PATH=$PATH:$GOROOT/bin
  GO111MODULE="off" go get -v github.com/submariner-io/submariner-operator/... || echo -e "\n# GO Get Submariner Operator finished"

  # Pull latest changes and build:
  cd "$GOPATH/src/github.com/submariner-io/submariner-operator" || return 1
  ls

  # go get -v -u -t ./...
  git_reset_local_repo

  TITLE "Build SubCtl tool and install it in $GOBIN/"

  BUG "GO111MODULE=on go install" \
  "make bin/subctl # BUT Will fail if Docker is not pre-installed" \
  "https://github.com/submariner-io/submariner-operator/issues/319"
  # export GO111MODULE=on
  # GO111MODULE=on go mod vendor
  # GO111MODULE=on go install # Compile binary and moves it to $GOBIN

  GO111MODULE="on" go mod vendor
  ./scripts/generate-embeddedyamls

  # ./scripts/build-subctl
  BUG "./scripts/build-subctl failed since it runs git outside repo directory" \
  "Precede with DAPPER_SOURCE = submariner-operator path" \
  "https://github.com/submariner-io/submariner-operator/issues/390"
  # workaround:
  DAPPER_SOURCE="$(git rev-parse --show-toplevel)"
  export DAPPER_SOURCE

  BUG "./scripts/build fails for missing library file" \
  "Use SCRIPTS_DIR from Shipyard" \
  "https://github.com/submariner-io/submariner/issues/576"
  # workaround:
  wget -O - https://github.com/submariner-io/shipyard/archive/devel.tar.gz | tar xz --strip=2 "shipyard-devel/scripts/shared"
  export SCRIPTS_DIR=${PWD}/shared

  BUG "Building subctl: compile.sh fails on bad substitution of flags" \
  "NO Workaround yet" \
  "https://github.com/submariner-io/submariner-operator/issues/403"

  ./scripts/build-subctl
    # ...
    # Building subctl version dev for linux/amd64
    # ...

  ls -l ./bin/subctl
  mkdir -p "$GOBIN"
  # cp -f ./bin/subctl $GOBIN/
  /usr/bin/install ./bin/subctl "$GOBIN/subctl"

  # Create symbolic link /usr/local/bin/subctl :
  #sudo ln -sf $GOPATH/src/github.com/submariner-io/submariner-operator/bin/subctl /usr/local/bin/subctl
  #cp -f ./bin/subctl ~/.local/bin

}

# ------------------------------------------

function test_submariner_e2e_with_subctl() {
# Run E2E Tests of Submariner:
  PROMPT "Testing Submariner End-to-End tests with SubCtl command"
  trap_to_debug_commands;

  [[ -x "$(command -v subctl)" ]] || FATAL "No SubCtl installation found. Try to run again with option '--subctl-version'"
  subctl version

  TITLE "Set SubCtl E2E context for the active clusters"
  export KUBECONFIG="${KUBECONF_HUB}"
  local e2e_subctl_context
  e2e_subctl_context="$(${OC} config current-context)"

  if [[ -s "$KUBECONF_CLUSTER_B" ]] ; then
    echo -e "\n# Appending \"${CLUSTER_B_NAME}\" to current E2E context (${e2e_subctl_context})"
    export KUBECONFIG="${KUBECONF_CLUSTER_B}"
    e2e_subctl_context="${e2e_subctl_context},$(${OC} config current-context)"
  fi

  if [[ -s "$KUBECONF_CLUSTER_C" ]] ; then
    echo -e "\n# Appending \"${CLUSTER_C_NAME}\" to current E2E context (${e2e_subctl_context})"
    export KUBECONFIG="${KUBECONF_CLUSTER_C}"
    e2e_subctl_context="${e2e_subctl_context},$(${OC} config current-context)"
  fi

  BUG "No SubCtl option to set -ginkgo.reportFile" \
  "No workaround yet..." \
  "https://github.com/submariner-io/submariner-operator/issues/509"

  TITLE "SubCtl E2E output will be printed both to stdout and to the file $E2E_LOG"

  export_merged_kubeconfigs

  # For SubCtl > 0.8:
  if [[ $(subctl version | grep --invert-match "v0.8") ]] ; then
    subctl verify --only service-discovery,connectivity --verbose --kubecontexts "${e2e_subctl_context}" | tee -a "$E2E_LOG"
  else
    # For SubCtl <= 0.8:
    # subctl verify --disruptive-tests --verbose ${KUBECONF_HUB} ${KUBECONF_CLUSTER_B} ${KUBECONF_CLUSTER_C} | tee -a "$E2E_LOG"
    subctl verify --only service-discovery,connectivity --verbose "${KUBECONF_HUB}" "${KUBECONF_CLUSTER_B}" "${KUBECONF_CLUSTER_C}" | tee -a "$E2E_LOG"
  fi

}

# ------------------------------------------

function upload_junit_xml_to_polarion() {
  trap_to_debug_commands;
  local junit_file="$1"
  local polarion_include_skipped="$2" # Optional: If "true" - Display Junit skipped test as "Waiting" in Polarion (i.e. test not run yet)
  echo -e "\n### Uploading test results to Polarion from Junit file: $junit_file ###\n"

  create_polarion_testcases_doc_from_junit "https://$POLARION_SERVER/polarion" "$POLARION_AUTH" "$junit_file" \
  "$POLARION_PROJECT_ID" "$POLARION_TEAM_NAME" "$POLARION_USR" "$POLARION_COMPONENT_ID" "$POLARION_TESTCASES_DOC" "$POLARION_TESTPLAN_ID"

  create_polarion_testrun_result_from_junit "https://$POLARION_SERVER/polarion" "$POLARION_AUTH" \
  "$junit_file" "$POLARION_PROJECT_ID" "$POLARION_TEAM_NAME" "$POLARION_TESTRUN_TEMPLATE" "$POLARION_TESTPLAN_ID" "$polarion_include_skipped"

}

# ------------------------------------------

function create_all_test_results_in_polarion() {
  PROMPT "Upload all test results to Polarion"
  trap_to_debug_commands;

  # Temp file to store Polarion output
  local polarion_testrun_import_log
  polarion_testrun_import_log="$(mktemp)_polarion_import_log"
  local polarion_rc=0

  # Upload SYSTEM tests to Polarion
  TITLE "Upload Junit results of SYSTEM (Shell) tests to Polarion (including skipped tests)"

  # Redirect output to stdout and to $polarion_testrun_import_log, in order to get polarion testrun url into report
  upload_junit_xml_to_polarion "$SHELL_JUNIT_XML" "true" |& tee "$polarion_testrun_import_log" || polarion_rc=1

  # Add Polarion link to the HTML report
  add_polarion_testrun_url_to_report_headlines "$polarion_testrun_import_log" "$SHELL_JUNIT_XML"


  # Upload Ginkgo E2E tests to Polarion
  if [[ (! "$SKIP_TESTS" =~ ((e2e|all)(,|$))+) && -s "$E2E_JUNIT_XML" ]] ; then

    TITLE "Upload Junit results of Submariner E2E (Ginkgo) tests to Polarion:"

    # Redirecting with TEE to stdout and to $polarion_testrun_import_log, in order to get polarion testrun url into report
    upload_junit_xml_to_polarion "$E2E_JUNIT_XML" |& tee "$polarion_testrun_import_log" || polarion_rc=1

    # Add Polarion link to the HTML report
    add_polarion_testrun_url_to_report_headlines "$polarion_testrun_import_log" "$E2E_JUNIT_XML"

    TITLE "Upload Junit results of Lighthouse E2E (Ginkgo) tests to Polarion:"

    # Redirecting with TEE to stdout and to $polarion_testrun_import_log, in order to get polarion testrun url into report
    upload_junit_xml_to_polarion "$LIGHTHOUSE_JUNIT_XML" |& tee "$polarion_testrun_import_log" || polarion_rc=1

    # Add Polarion link to the HTML report
    add_polarion_testrun_url_to_report_headlines "$polarion_testrun_import_log" "$LIGHTHOUSE_JUNIT_XML"

  fi

  # Upload UNIT tests to Polarion (skipping, not really required)

  # if [[ (! "$SKIP_TESTS" =~ ((pkg|all)(,|$))+) && -s "$PKG_JUNIT_XML" ]] ; then
  #   echo -e "\n# Upload Junit results of PKG (Ginkgo) unit-tests to Polarion:"
  #   sed -r 's/(<\/?)(passed>)/\1system-out>/g' -i "$PKG_JUNIT_XML" || :
  #
  #   upload_junit_xml_to_polarion "$PKG_JUNIT_XML" || polarion_rc=1
  # fi

  return $polarion_rc
}


# ------------------------------------------

function add_polarion_testrun_url_to_report_headlines() {
# Helper function to search polarion testrun url in the testrun import output, in order to add later to the HTML report
  trap_to_debug_commands;

  local polarion_testrun_import_log="$1"
  local polarion_test_run_file="$2"

  TITLE "Add new Polarion Test run results to the Html report headlines: "
  local polarion_testrun_result_page
  polarion_testrun_result_page="$(grep -Poz '(?s)Test suite.*\n.*Polarion results published[^\n]*' "$polarion_testrun_import_log" \
  | sed -z 's/\.\n.* to:/:\n/' )" || :

  local polarion_testrun_name
  polarion_testrun_name="$(basename "${polarion_test_run_file%.*}")" # Get file name without path and extension
  polarion_testrun_name="${polarion_testrun_name//junit}" # Remove all "junit" from file name
  polarion_testrun_name="${polarion_testrun_name//_/ }" # Replace all _ with spaces

  if [[ -n "$polarion_testrun_result_page" ]] ; then
    # echo "$polarion_testrun_result_page" | sed -r 's/(https:[^ ]*)/\1\&tab=records/g' >> "$POLARION_RESULTS" || :
    echo "$polarion_testrun_result_page" >> "$POLARION_RESULTS" || :
    # echo -e " (${polarion_testrun_name}) \n" >> "$POLARION_RESULTS" || :
  else
    echo -e "\n# Error reading Polarion Test results link for ${polarion_testrun_name}: \n ${polarion_testrun_result_page}" 1>&2
  fi

}

# ------------------------------------------

function env_teardown() {
  # Run tests and environment functions at the end (call with trap exit)

  # Get test exit status (from file $TEST_STATUS_FILE)
  EXIT_STATUS="$([[ ! -s "$TEST_STATUS_FILE" ]] || cat "$TEST_STATUS_FILE")"

  TITLE "Showing product versions on script teardown (only if EXIT_STATUS [${EXIT_STATUS}] is already set)"

  if [[ -n "$EXIT_STATUS" ]] ; then

    ${JUNIT_CMD_IGNORE_STATUS} test_products_versions_cluster_a || :

    [[ ! -s "$KUBECONF_CLUSTER_B" ]] || ${JUNIT_CMD_IGNORE_STATUS} test_products_versions_cluster_b || :

    [[ ! -s "$KUBECONF_CLUSTER_C" ]] || ${JUNIT_CMD_IGNORE_STATUS} test_products_versions_cluster_c || :

  fi

}

# ------------------------------------------

function test_products_versions_cluster_a() {
  PROMPT "Show products versions on cluster A"

  export KUBECONFIG="${KUBECONF_HUB}"
  test_products_versions

  save_cluster_info_to_file
}

# ------------------------------------------

function test_products_versions_cluster_b() {
  PROMPT "Show products versions on cluster B"

  export KUBECONFIG="${KUBECONF_CLUSTER_B}"
  test_products_versions

  save_cluster_info_to_file
}

# ------------------------------------------

function test_products_versions_cluster_c() {
  PROMPT "Show products versions on cluster C"

  export KUBECONFIG="${KUBECONF_CLUSTER_C}"
  test_products_versions

  save_cluster_info_to_file
}

# ------------------------------------------

function test_products_versions() {
# Show OCP clusters versions, and Submariner version
  trap '' DEBUG # DONT trap_to_debug_commands

  ${OC} status || { FAILURE "OCP cluster is inaccessible" ; return ; }

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"
  local cluster_info_output="${OUTPUT_DIR}/${cluster_name}.info"

  local ocp_cloud
  ocp_cloud="$(print_current_cluster_cloud || :)"

  local cluster_version
  cluster_version="$(${OC} version | awk '/Server Version/ { print $3 }' | grep .)"

  # Get ACM version if ACM hub is install on current cluster
  local acm_current_version
  acm_current_version="$(${OC} get MultiClusterHub -n "${ACM_NAMESPACE}" multiclusterhub -o jsonpath='{.status.currentVersion}')" || :

  TITLE "OCP cluster ${cluster_name} information"
  echo -e "\n# Cloud platform: ${ocp_cloud}"
  echo -e "\n# OCP version: ${cluster_version}"
  [[ -z "$acm_current_version" ]] || echo -e "\n# ACM version: ${acm_current_version}"

  ${OC} adm release info

  echo -e "\n### Submariner components ###\n"

  subctl version || :

  if subctl show versions ; then

    # Show Libreswan (cable driver) version in the active gateway pod
    export_variable_name_of_active_gateway_pod "active_gateway_pod" "10s" "yes" || :

    if [[ -n "$active_gateway_pod" ]] ; then
      echo -e "\n### Linux version on the running Gateway pod: $active_gateway_pod ###"
      ${OC} exec "$active_gateway_pod" -n "${SUBM_NAMESPACE}" -- bash -c "cat /etc/os-release" | awk -F\" '/PRETTY_NAME/ {print $2}' || :
      echo -e "\n\n"

      echo -e "\n### LibreSwan version on the running Gateway pod: $active_gateway_pod ###"
      ${OC} exec "$active_gateway_pod" -n "${SUBM_NAMESPACE}" -- bash -c "rpm -qa libreswan" || :
      echo -e "\n\n"
    fi

  fi

  # Show Submariner and ACM CSVs (Cluster service versions)
  print_csvs_in_namespace "$SUBM_NAMESPACE"

  # Show Submariner images info of running pods
  print_images_info_of_namespace_pods "${SUBM_NAMESPACE}"

  # Show Submariner image-stream tags
  print_image_tags_info "${SUBM_NAMESPACE}"

  # # Show BREW_REGISTRY images
  # ${OC} get images | grep "${BREW_REGISTRY}" |\
  # grep "$SUBM_IMG_GATEWAY|\
  #     $SUBM_IMG_ROUTE|\
  #     $SUBM_IMG_NETWORK|\
  #     $SUBM_IMG_LIGHTHOUSE|\
  #     $SUBM_IMG_COREDNS|\
  #     $SUBM_IMG_GLOBALNET|\
  #     $SUBM_IMG_OPERATOR|\
  #     $SUBM_IMG_BUNDLE" |\
  # while read -r line ; do
  #   set -- $(echo $line | awk '{ print $1, $2 }')
  #   local img_id="$1"
  #   local img_name="$2"
  #
  #   echo -e "\n### Local registry image: $(echo $img_name | sed -r 's|.*/([^@]+).*|\1|') ###"
  #   print_image_info "$img_id"
  # done

  # Show CSVs (Cluster service versions) of ACM and MCE
  print_csvs_in_namespace "$ACM_NAMESPACE"
  print_csvs_in_namespace "$MCE_NAMESPACE"

  # Show ACM and MCE images info in the running pods
  print_images_info_of_namespace_pods "${ACM_NAMESPACE}"
  print_images_info_of_namespace_pods "${MCE_NAMESPACE}"

  echo -e "\n# Current OC user: $(${OC} whoami || : )"
  echo -e "\n# Current Kubeconfig contexts:"
  ${OC} config get-contexts

  TITLE "Cluster routes on ${cluster_name}"
  ${OC} get routes -A || :

}

# ------------------------------------------

function save_cluster_info_to_file() {
# Save important OCP cluster and Submariner information to local files
  trap '' DEBUG # DONT trap_to_debug_commands

  ${OC} status || { FAILURE "OCP cluster is inaccessible" ; return ; }

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  local ocp_cloud
  ocp_cloud="$(print_current_cluster_cloud || :)"

  local cluster_version
  cluster_version="$(${OC} version | awk '/Server Version/ { print $3 }' | grep .)"

  # Print OCP cluster info into file local file "<cluster name>.info"
  local cluster_info_output="${OUTPUT_DIR}/${cluster_name}.info"

  local cluster_info="${ocp_cloud} cluster : OCP ${cluster_version}"
  echo "${cluster_info}" > "${cluster_info_output}" || :

  # Print all cluster routes into file "<cluster name>.info"
  ${OC} get routes -A | awk '$2 ~ /console/ {print $1 " : " $3}' >> "${cluster_info_output}" || :

  # Just for the first managed cluster - print Submariner images url into $PRODUCT_IMAGES file
  if [[ ! -s "$PRODUCT_IMAGES" ]] ; then
    print_images_info_of_namespace_pods "${SUBM_NAMESPACE}" | grep -Po "url=\K.*" >> "$PRODUCT_IMAGES"
    print_images_info_of_namespace_pods "${ACM_NAMESPACE}" | grep -Po "url=\K.*" >> "$PRODUCT_IMAGES"
  fi

}

# ------------------------------------------

function gather_submariner_info() {
  # print submariner pods descriptions and logs
  # Ref: https://github.com/submariner-io/shipyard/blob/devel/scripts/shared/post_mortem.sh

  local log_file="${1:-subm_pods.log}"
  local cluster_name
  local cluster_context

  (
    PROMPT "Collecting system information due to test failure" "$RED"
    trap_to_debug_commands;

    df -h
    free -h

    TITLE "Submariner information (using subctl show and diagnose)"

    # export_merged_kubeconfigs
    BUG "Some OC/SubCtl versions cannot use merged kubeconfig" \
    "Do not run export_merged_kubeconfigs before subctl show and subctl diagnose, and use old kubeconfig (.bak) with a single context"
    # Workaround:
    export KUBECONFIG="${KUBECONF_HUB}.bak"

    subctl show all || :

    subctl diagnose all || :

    # Gather logs of cluster A
    print_resources_and_pod_logs "${KUBECONF_HUB}" || :

    # Gather logs of cluster B
    if [[ -s "$KUBECONF_CLUSTER_B" ]] ; then
      print_resources_and_pod_logs "${KUBECONF_CLUSTER_B}" || :
    fi

    # Gather logs of cluster C
    if [[ -s "$KUBECONF_CLUSTER_C" ]] ; then
      print_resources_and_pod_logs "${KUBECONF_CLUSTER_C}" || :
    fi

  ) |& tee -a "$log_file"

}

# ------------------------------------------

function print_resources_and_pod_logs() {
  trap_to_debug_commands;

  local kubeconfig_file="$1"
  export KUBECONFIG="$kubeconfig_file"

  local cluster_name
  cluster_name="$(print_current_cluster_name || :)"

  PROMPT "Gather logs of OCP, ACM and Submariner in ${cluster_name}"

  ${OC} status || { FAILURE "OCP cluster is inaccessible" ; return ; }

  ${OC} version || :

  ${OC} config get-contexts

  TITLE "Openshift Nodes and Operators on ${cluster_name}"

  ${OC} get nodes -o wide || :

  ${OC} describe node | grep Degraded -C 2 || :

  TITLE "Cluster Operators on ${cluster_name}"

  ${OC} get clusteroperators || :

  TITLE "Unready Pods (if any) on ${cluster_name}"

  ${OC} get pod -A |  grep -Ev '([0-9]+)/\1' | grep -v 'Completed' | grep -E '[0-9]+/[0-9]+' || :

  TITLE "Submariner gather logs in cluster $cluster_name"

  # cluster_context="$cluster_name"
  BUG "subctl gather does not support multiple kubeconfig contexts"
  # Workaround:
  # Use old kubeconfig (.bak) and "admin" context
  export KUBECONFIG="${kubeconfig_file}.bak"
  cluster_context="admin"

  subctl gather --dir submariner-gather_"${cluster_name}" --kubecontexts ${cluster_context} || :

  TITLE "ACM resources in ${cluster_name}"

  ${OC} get all -n "${ACM_NAMESPACE}" -o wide || :

  TITLE "Submariner resources in ${cluster_name}"

  ${OC} get all -n "${SUBM_NAMESPACE}" -o wide || :

  local broker_namespace
  broker_namespace="$(get_broker_namespace || :)"
  [[ -z "${broker_namespace}" ]] || \
  ${OC} get all -n "${broker_namespace}" -o wide || :

  TITLE "Submariner Gateway info on ${cluster_name}"

  ${OC} get nodes --selector=submariner.io/gateway=true --show-labels || :

  ${OC} describe Submariner -n "${SUBM_NAMESPACE}" || :
  # ${OC} get Submariner -o yaml -n ${SUBM_NAMESPACE} || :

  ${OC} describe Gateway -n "${SUBM_NAMESPACE}" || :

  TITLE "Submariner Roles and Broker info on ${cluster_name}"

  ${OC} get roles -A | grep "submariner" || :

  [[ -z "${broker_namespace}" ]] || \
  ${OC} describe role submariner-k8s-broker-cluster -n "${broker_namespace}" || :

  TITLE "Submariner Deployments, Daemon and Replica sets on ${cluster_name}"

  ${OC} describe deployments -n "${SUBM_NAMESPACE}" || :
  #  ${OC} get deployments -o yaml -n ${SUBM_NAMESPACE} || :

  ${OC} describe daemonsets -n "${SUBM_NAMESPACE}" || :

  ${OC} describe replicasets -n "${SUBM_NAMESPACE}" || :

  TITLE "Openshift configurations on ${cluster_name}"

  ${OC} get all -n openshift-machine-config-operator -o wide || :

  ${OC} describe configmaps -n openshift-dns || :

  echo -e "\n# TODO: Loop on each cluster: ${OC} describe cluster ${cluster_name} -n ${SUBM_NAMESPACE}"

  # for pod in $(${OC} get pods -A \
  # -l 'name in (submariner-operator,submariner-gateway,submariner-globalnet,kube-proxy)' \
  # -o jsonpath='{.items[0].metadata.namespace} {.items[0].metadata.name}' ; do
  #     echo "######################: Logs for Pod $pod :######################"
  #     ${OC}  -n $ns describe pod $name
  #     ${OC}  -n $namespace logs $pod
  # done

  TITLE "Openshift Machines on ${cluster_name}"

  ${OC} get machineconfigpool || :

  ${OC} get Machine -A | awk '{
    if (NR>1) {
      namespace = $1
      machine = $2
      printf ("\n###################### Machine: %s (Namespece: %s) ######################\n", machine, namespace )
      cmd = "oc describe Machine " machine " -n " namespace
      printf ("\n$ %s\n\n", cmd)
      system("oc describe Machine "$2" -n "$1)
    }
  }'

  TITLE "All Submariner logs on ${cluster_name}"

  print_pod_logs_in_namespace "$SUBM_NAMESPACE" "name=submariner-operator"

  local gw_label='app=submariner-gateway'
  # For SubCtl <= 0.8 : 'app=submariner-engine' is expected as the Gateway pod label
  [[ $(subctl version | grep --invert-match "v0.8") ]] || gw_label="app=submariner-engine"

  print_pod_logs_in_namespace "$SUBM_NAMESPACE" $gw_label

  print_pod_logs_in_namespace "$SUBM_NAMESPACE" "control-plane=submariner-operator"

  print_pod_logs_in_namespace "$SUBM_NAMESPACE" "app=submariner-addon"

  print_pod_logs_in_namespace "$SUBM_NAMESPACE" "app=submariner-globalnet"

  print_pod_logs_in_namespace "$SUBM_NAMESPACE" "app=submariner-lighthouse-agent"

  print_pod_logs_in_namespace "$SUBM_NAMESPACE" "app=submariner-lighthouse-coredns"

  print_pod_logs_in_namespace "$SUBM_NAMESPACE" "app=submariner-routeagent"

  print_pod_logs_in_namespace "kube-system" "k8s-app=kube-proxy"

  print_pod_logs_in_namespace "kube-system" "component=kube-controller-manager"

  echo -e "\n############################## End of Submariner logs collection on ${cluster_name} ##############################\n"

  TITLE "All Openshift events on ${cluster_name}"

  ${OC} get events -A --sort-by='.metadata.creationTimestamp' \
  -o custom-columns=FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message || :

}

# ------------------------------------------


###################################################################
#         Internal usage functions to debug this script           #
###################################################################


function debug_test_polarion() {
  # Internal debugging function for Polarion

  trap_to_debug_commands;
  PROMPT "DEBUG Polarion setup"

  # Set Polarion access if $UPLOAD_TO_POLARION = yes/y
  if [[ "$UPLOAD_TO_POLARION" =~ ^(y|yes)$ ]] ; then
    TITLE "Set Polarion access for the user [$POLARION_USR]"
    ( # subshell to hide commands
      local polauth
      polauth=$(echo "${POLARION_USR}:${POLARION_PWD}" | base64 --wrap 0)
      echo "--header \"Authorization: Basic ${polauth}\"" > "$POLARION_AUTH"
    )
  fi

  echo 1 > "$TEST_STATUS_FILE"
}

# ------------------------------------------

function debug_test_pass() {
  # Internal debugging function to test juint with arguments and special characters

  trap_to_debug_commands;
  PROMPT "PASS test for DEBUG"

  local arg1="$1"
  local arg2="$2"

  if [[ "$arg1" != "$arg2" ]] ; then
    BUG "A dummy bug" \
     "A workaround" \
    "A link"
  fi

  local msg="
    & (ampersand) <br>
    < (lower) <br>
    > (greater) <br>
     (single quotes) <br>
    \" (double quotes) <br>
    "

  TITLE "PRINT TEST: \n $msg"

}

# ------------------------------------------

function debug_test_fail() {
  # Internal debugging function to mark junit test as failed, without exiting script on error

  trap_to_debug_commands;
  PROMPT "FAIL test for DEBUG"
  echo "Should not get here if calling after a bad exit code (e.g. FAILURE or FATAL)"
  # find ${CLUSTER_A_DIR} -name "*.log" -print0 | xargs -0 cat

  local TEST=1
  if [[ -n "$TEST" ]] ; then
    TITLE "Test FAILURE() function, that should not break whole script, but just this test"
    FAILURE "MARK TEST FAILURE, BUT CONTINUE"
  fi

  echo "It should NOT print this"

}

# ------------------------------------------

function debug_test_fatal() {
  # Internal debugging function to test FATAL error (tests should not continue afterwards)

  trap_to_debug_commands;
  PROMPT "FATAL test for DEBUG"
  FATAL "Terminating script here"
}

# ------------------------------------------
